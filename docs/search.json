[
  {
    "objectID": "user_guide/index.html",
    "href": "user_guide/index.html",
    "title": "User Guide",
    "section": "",
    "text": "This is the user guide for neuralyzer",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "user_guide/image_processing/contrast.html",
    "href": "user_guide/image_processing/contrast.html",
    "title": "Contrast and Brightness Adjustment",
    "section": "",
    "text": "These two filters are implemented with a single linear transform taking parameters “alpha” (\\(\\alpha\\)) and “beta” (\\(\\beta\\)). Pixel values are multiplied by \\(\\alpha\\) to adjust contrast, then have \\(\\beta\\) added to adjust brightness: \\[\ng(x) = \\alpha f(x) + \\beta\n\\] In effect the magnitude of \\(\\alpha\\) corresponds to the amount of contrast and the magnitude of \\(\\beta\\) corresponds to the amount of brightness.",
    "crumbs": [
      "Image Processing",
      "Contrast and Brightness Adjustment"
    ]
  },
  {
    "objectID": "user_guide/image_processing/bilateral_filter.html",
    "href": "user_guide/image_processing/bilateral_filter.html",
    "title": "Bilateral Filter",
    "section": "",
    "text": "https://docs.opencv.org/4.x/js_filtering_bilateralFilter.html\nhttps://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga9d7064d478c95d60003cf839430737ed\nhttps://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html",
    "crumbs": [
      "Image Processing",
      "Bilateral Filter"
    ]
  },
  {
    "objectID": "user_guide/image_processing/bilateral_filter.html#references",
    "href": "user_guide/image_processing/bilateral_filter.html#references",
    "title": "Bilateral Filter",
    "section": "",
    "text": "https://docs.opencv.org/4.x/js_filtering_bilateralFilter.html\nhttps://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga9d7064d478c95d60003cf839430737ed\nhttps://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html",
    "crumbs": [
      "Image Processing",
      "Bilateral Filter"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/overview.html",
    "href": "user_guide/data_transformations/overview.html",
    "title": "Data Transformations",
    "section": "",
    "text": "Data transformations are operations we perform on one kind of data that result in another kind of data.\nTransformations supported by Neuralyzer are listed below:",
    "crumbs": [
      "Data Transformations",
      "Data Transformations"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/overview.html#transformations",
    "href": "user_guide/data_transformations/overview.html#transformations",
    "title": "Data Transformations",
    "section": "Transformations",
    "text": "Transformations\n\nAnalog Time Series\n\nEvent Detection by Threshold Crossing\nGenerate events from a time series by comparing signal to some threshold value.\nThreshold Value:\nThe numerical level the signal must cross to be considered an event. The signal will be compared directly against this value based on the selected direction.\nThreshold Direction:\nHow the signal must cross the Threshold Value to trigger an event:\n\nPositive (Rising): Detects an event only when the signal value increases and crosses above the Threshold Value.\nNegative (Falling): Detects an event only when the signal value decreases and crosses below the Threshold Value.\nAbsolute (Magnitude): Detects an event when the absolute value of the signal (abs(signal)) rises above the Threshold Value. This detects crossings away from zero on either the positive or negative side.\n\nLockout Time (Samples):\nThe minimum number of samples that must pass after an event is detected before another threshold crossing can trigger a new event. Enter 0 (or less) to disable the lockout, allowing consecutive samples to trigger events if they cross the threshold. This helps prevent multiple detections from a single noisy crossing.\n\n\nFind Peaks\nAttempts to find peaks in analog time series (local maxima or minima).\n\n\nInstantaneous Amplitude Calculation\nDetermine the instantaneous amplitude of an oscillatory time series. uses Hilbert Transform.\n\n\nInstantaneous Phase Calculation\nDetermine the instantaneous phase of an oscillatory time series. Uses Hilbert Transform.\n\n\nInterval Detection\nDetermine an interval range where an angle signal is above (or below) a threshold value. Returns the beginning and end of the above/below threshold time period.\nThreshold Value:\nDescription: Enter the numerical level the signal must cross to be considered an event. The signal will be compared directly against this value based on the selected direction. The interval will be defined from the first to last sample that are above or below the Threshold Value.\nThreshold Direction:\nDescription: Select how the signal must cross the Threshold Value to trigger an interval:\n\nPositive (Rising): Detects an interval only when the signal value increases and remains above the Threshold Value.\nNegative (Falling): Detects an event only when the signal value decreases and remains below the Threshold Value.\nAbsolute (Magnitude): Detects an event when the absolute value of the signal (abs(signal)) rises above the Threshold Value. This detects crossings away from zero on either the positive or negative side. Once the absolute value is smaller than the abs(signal), the interval has ended.\n\nLockout Time (Samples):\nDescription: Specify the minimum number of samples that must pass after an interval has ended before another threshold crossing can trigger a new event. Enter 0 (or less) to disable the lockout.\nMinimum Duration (Samples):\nDescription: Specify the minimum number of samples that must be in an interval to be included. Threshold crossings with durations shorter than Minimum Duration will not be included in result.\n\n\n\nMasks\n\nArea\nTotal area of the mask in a frame is calculated.",
    "crumbs": [
      "Data Transformations",
      "Data Transformations"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html",
    "href": "user_guide/data_transformations/analog_filter.html",
    "title": "Analog Filter",
    "section": "",
    "text": "The Analog Filter transform applies a digital filter to an AnalogTimeSeries. This can be used to remove noise, isolate specific frequency bands, or perform other signal processing tasks."
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#purpose",
    "href": "user_guide/data_transformations/analog_filter.html#purpose",
    "title": "Analog Filter",
    "section": "",
    "text": "The Analog Filter transform applies a digital filter to an AnalogTimeSeries. This can be used to remove noise, isolate specific frequency bands, or perform other signal processing tasks."
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#parameters",
    "href": "user_guide/data_transformations/analog_filter.html#parameters",
    "title": "Analog Filter",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilter_type\nstring\nThe type of filter to apply. One of “lowpass”, “highpass”, “bandpass”, “bandstop”.\n“lowpass”\n\n\ncutoff_frequency\ndouble\nThe cutoff frequency for lowpass and highpass filters, or the lower cutoff frequency for bandpass and bandstop filters.\n10.0\n\n\ncutoff_frequency2\ndouble\nThe upper cutoff frequency for bandpass and bandstop filters.\n0.0\n\n\norder\ninteger\nThe order of the filter. Must be between 1 and 8.\n4\n\n\nripple\ndouble\nThe ripple for Chebyshev filters (in dB). Not used for Butterworth filters.\n0.0\n\n\nzero_phase\nboolean\nIf true, applies the filter forward and backward to eliminate phase distortion.\nfalse"
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#json-schema",
    "href": "user_guide/data_transformations/analog_filter.html#json-schema",
    "title": "Analog Filter",
    "section": "JSON Schema",
    "text": "JSON Schema\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"transform\": {\n      \"type\": \"string\",\n      \"const\": \"Analog Filter\"\n    },\n    \"input\": {\n      \"type\": \"string\"\n    },\n    \"output\": {\n      \"type\": \"string\"\n    },\n    \"params\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"filter_type\": {\n          \"type\": \"string\",\n          \"enum\": [\"lowpass\", \"highpass\", \"bandpass\", \"bandstop\"]\n        },\n        \"cutoff_frequency\": {\n          \"type\": \"number\"\n        },\n        \"cutoff_frequency2\": {\n          \"type\": \"number\"\n        },\n        \"order\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"maximum\": 8\n        },\n        \"ripple\": {\n          \"type\": \"number\"\n        },\n        \"zero_phase\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\"filter_type\", \"cutoff_frequency\", \"order\"]\n    }\n  },\n  \"required\": [\"transform\", \"input\", \"output\", \"params\"]\n}"
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#minimal-example",
    "href": "user_guide/data_transformations/analog_filter.html#minimal-example",
    "title": "Analog Filter",
    "section": "Minimal Example",
    "text": "Minimal Example\nThis example applies a 4th order lowpass Butterworth filter at 100 Hz.\n{\n  \"transform\": \"Analog Filter\",\n  \"input\": \"my_analog_series\",\n  \"output\": \"filtered_series\",\n  \"params\": {\n    \"filter_type\": \"lowpass\",\n    \"cutoff_frequency\": 100,\n    \"order\": 4\n  }\n}"
  },
  {
    "objectID": "user_guide/behaviors/tongue.html",
    "href": "user_guide/behaviors/tongue.html",
    "title": "Tongue Tracking",
    "section": "",
    "text": "The Tongue Tracking widget deals with operations related to the tongue.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html#loading",
    "href": "user_guide/behaviors/tongue.html#loading",
    "title": "Tongue Tracking",
    "section": "Loading",
    "text": "Loading\nTongue masks can be loaded through the sparse HDF5 format or binary images (where white is part of the mask, black is not).\nJaw keypoint tracking can also be loaded through CSV format. The first column should indicate frame number, the next indicating \\(x\\) position and the next \\(y\\) position.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html#grabcut-tool",
    "href": "user_guide/behaviors/tongue.html#grabcut-tool",
    "title": "Tongue Tracking",
    "section": "GrabCut Tool",
    "text": "GrabCut Tool\nDocumentation on the GrabCut tool can be found here.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "examples/keypoint_labeling.html",
    "href": "examples/keypoint_labeling.html",
    "title": "Keypoint Video Labeling",
    "section": "",
    "text": "Keypoint labeling is a powerful method to systematically study motion, behavior, and anatomical positioning over time. By marking and tracking specific locations on subjects across frames in a video, you can extract detailed data about the dynamics of movement.\nFor example, keypoint data can be used to:\nThis tutorial will guide you through the process of labeling keypoints on a video and saving the keypoint data."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-1-load-video-or-existing-labels",
    "href": "examples/keypoint_labeling.html#step-1-load-video-or-existing-labels",
    "title": "Keypoint Video Labeling",
    "section": "Step 1: Load Video or Existing Labels",
    "text": "Step 1: Load Video or Existing Labels\n\n\n\n\n\nTo begin labeling:\n\nOpen the application and navigate to File &gt; Load Data.\nSelect the video you intend to annotate.\n\nAlternatively, if you’ve already created keypoints and want to review or update them:\n\nSelect File &gt; Load JSON Configuration and choose your .json file.\n\n\n\n\n\n\n\nTip\n\n\n\nSee: Setting up a JSON File for more on configuration options."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-2-open-the-data-manager",
    "href": "examples/keypoint_labeling.html#step-2-open-the-data-manager",
    "title": "Keypoint Video Labeling",
    "section": "Step 2: Open the Data Manager",
    "text": "Step 2: Open the Data Manager\n\n\n\n\n\nTo create or manage point data:\n\nNavigate to Modules &gt; Data Manager to open the Data Manager Module."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-3-create-a-new-keypoint-feature",
    "href": "examples/keypoint_labeling.html#step-3-create-a-new-keypoint-feature",
    "title": "Keypoint Video Labeling",
    "section": "Step 3: Create a New Keypoint Feature",
    "text": "Step 3: Create a New Keypoint Feature\n\n\n\n\n\nIn the Data Manager:\n\nSet the Output Directory: This is where your labeled data (CSV) will be saved.\n\nExample: C:/Users/wanglab/Desktop/Cartoon_Mouse\n\nCreate New Data:\n\nType: point\nTime Frame: Select based on your analysis requirements (e.g., time for temporal tracking).\nName: Choose a descriptive name (e.g., jaw, nose, or paw).\n\n\nClick Create New Data to add the new feature."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-4-edit-and-add-keypoints-in-the-media-widget",
    "href": "examples/keypoint_labeling.html#step-4-edit-and-add-keypoints-in-the-media-widget",
    "title": "Keypoint Video Labeling",
    "section": "Step 4: Edit and Add Keypoints in the Media Widget",
    "text": "Step 4: Edit and Add Keypoints in the Media Widget\n\n\n\n\n\nOnce the point feature is created:\n\nEnable the Feature: Select the feature in the media widget to activate the keypoint editor.\nCustomize Appearance:\n\nChoose a color using the hex selector.\nAdjust the opacity (Alpha) for better contrast against the video.\nModify the size and shape of the marker if needed.\n\nBegin Labeling:\n\nSwitch mouse mode to “Select Point”.\nClick anywhere in the video frame to add a point.\nClicking on an existing point will move it.\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote: The keypoint is recorded as a single-pixel coordinate. Visual shape and style are only for display and are not saved."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-5-review-and-export-your-labels",
    "href": "examples/keypoint_labeling.html#step-5-review-and-export-your-labels",
    "title": "Keypoint Video Labeling",
    "section": "Step 5: Review and Export Your Labels",
    "text": "Step 5: Review and Export Your Labels\n\n\n\n\n\nAfter labeling:\n\nSelect your feature (e.g., jaw) in the Data Manager.\nYou’ll see a spreadsheet with:\n\nFrame indices\n(x, y) coordinates for each labeled keypoint\n\n\n\nExport Options:\n\nFile Name: Choose a meaningful name for your CSV.\nDelimiter: (e.g., comma , or tab \\t) – separates columns.\nLine Ending: Select line break style (\\n, \\r\\n, etc.).\nHeader: Optionally include a header row with custom labels."
  },
  {
    "objectID": "examples/keypoint_labeling.html#optional-export-matching-media-frames",
    "href": "examples/keypoint_labeling.html#optional-export-matching-media-frames",
    "title": "Keypoint Video Labeling",
    "section": "Optional: Export Matching Media Frames",
    "text": "Optional: Export Matching Media Frames\n\n\n\n\n\nYou can export the video frames where keypoints were labeled:\n\nCheck Export Matching Media Frames.\nChoose from export options:\n\nSave by Frame Name\nFrame ID Padding\nImage Name Prefix\nSubfolder for Images\nOverwrite Existing Files\n\n\n\n\n\n\n\n\nWarning\n\n\n\nKeypoints will not be drawn on these frames. Only raw frames are exported."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-6-save-to-csv",
    "href": "examples/keypoint_labeling.html#step-6-save-to-csv",
    "title": "Keypoint Video Labeling",
    "section": "Step 6: Save to CSV",
    "text": "Step 6: Save to CSV\n\n\n\n\n\nOnce you’ve reviewed your labels:\n\nClick “Save to CSV”\n\nA confirmation will appear showing the file path.\nYour labeled keypoint data is now saved and ready for further analysis.\n\n\nSummary\nThis tutorial demonstrated how to:\n\nLoad a video or keypoint configuration\nCreate and configure keypoint data\nAnnotate frames with feature coordinates\nExport label data and relevant frames"
  },
  {
    "objectID": "developer/table_view.html",
    "href": "developer/table_view.html",
    "title": "Table View",
    "section": "",
    "text": "The TableView class is able to take heterogeneous data from the Data Manager and structure it like a spreadsheet.\n\n\nThe columns may represent simple double or floating-point values, but also could take the form of different types like Booleans, counting numbers, or even arrays of values. Columns also may be data that is not directly in the Data Manager, but come as the result of some processing similar to the transformation infrastructure in the Data Manager. Some of these transformations may be redundant. I imagine transformations done for the TableView may be out of convenience or for data that you don’t necessarily care about saving as its own type, but just need for some additional processing.\nThe transformations done on data for a column need not necessarily come from the Data Manager, but also could come from other columns in the TableView. For instance, if we compute some kind of transformation and hold it in a column, we could then choose to z-score it.\n\n\n\nThe other difference from transformations comes from the concept of a row in the TableView structure. A row in the TableView could be a timestamp, just like how data is stored in the Manager, but also could be something that is an aggregator. For instance, a digital interview interval in the Data Manager can serve as a row. In this scheme, a double value in a column may not correspond to some value at an instant time, but rather one that is the average over an interval that corresponds to the row. It also could be the minimum value, the maximum value, or it could represent something like a standard deviation.\n\n\n\nTo create a TableView, the user must define what will make up the rows and what will make up the columns.\nRows are defined by selector objects that currently can be supported by intervals or timestamps. Note that these timestamps could correspond to the different timeframes in the Data Manager class.\nColumns are created by different computer objects. These computer objects work on different data sources that are placed into the TableView, and usually perform some kind of transformation. These data sources are things like analog sources, event sources, or interval sources. Adapter objects exist in the TableView to allow types in the Data Manager to be converted to these forms. Some of these transformations are straightforward. For instance, an analog source can be simply converted into a column of doubles. But something like point data would be broken up into two columns of x and y values for each timestamp.\nThe computers for a column that are available depend on the input data type as well as what row selector is being used (for example, intervals versus timestamps). A factory and registry is available to list the types of computers available for these combinations and can output the computer object that is available from the factory.\nWith the rows, analog sources defined, and computers, a builder object can be used to create an actual TableView.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#columns",
    "href": "developer/table_view.html#columns",
    "title": "Table View",
    "section": "",
    "text": "The columns may represent simple double or floating-point values, but also could take the form of different types like Booleans, counting numbers, or even arrays of values. Columns also may be data that is not directly in the Data Manager, but come as the result of some processing similar to the transformation infrastructure in the Data Manager. Some of these transformations may be redundant. I imagine transformations done for the TableView may be out of convenience or for data that you don’t necessarily care about saving as its own type, but just need for some additional processing.\nThe transformations done on data for a column need not necessarily come from the Data Manager, but also could come from other columns in the TableView. For instance, if we compute some kind of transformation and hold it in a column, we could then choose to z-score it.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#rows",
    "href": "developer/table_view.html#rows",
    "title": "Table View",
    "section": "",
    "text": "The other difference from transformations comes from the concept of a row in the TableView structure. A row in the TableView could be a timestamp, just like how data is stored in the Manager, but also could be something that is an aggregator. For instance, a digital interview interval in the Data Manager can serve as a row. In this scheme, a double value in a column may not correspond to some value at an instant time, but rather one that is the average over an interval that corresponds to the row. It also could be the minimum value, the maximum value, or it could represent something like a standard deviation.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#constructing-a-tableview",
    "href": "developer/table_view.html#constructing-a-tableview",
    "title": "Table View",
    "section": "",
    "text": "To create a TableView, the user must define what will make up the rows and what will make up the columns.\nRows are defined by selector objects that currently can be supported by intervals or timestamps. Note that these timestamps could correspond to the different timeframes in the Data Manager class.\nColumns are created by different computer objects. These computer objects work on different data sources that are placed into the TableView, and usually perform some kind of transformation. These data sources are things like analog sources, event sources, or interval sources. Adapter objects exist in the TableView to allow types in the Data Manager to be converted to these forms. Some of these transformations are straightforward. For instance, an analog source can be simply converted into a column of doubles. But something like point data would be broken up into two columns of x and y values for each timestamp.\nThe computers for a column that are available depend on the input data type as well as what row selector is being used (for example, intervals versus timestamps). A factory and registry is available to list the types of computers available for these combinations and can output the computer object that is available from the factory.\nWith the rows, analog sources defined, and computers, a builder object can be used to create an actual TableView.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#csv-export",
    "href": "developer/table_view.html#csv-export",
    "title": "Table View",
    "section": "CSV Export",
    "text": "CSV Export\nThe first, and most straightforward, use case is simple CSV export. For instance, imagine an experiment where an animal is licking. The intervals are calculated when licks occur. We would like to have some output for each lick where we are aware of the start time of the lick, the end time of the lick, and the duration. Within each of these licks, we would like to know the maximum surface area of the tongue that is stored in an analog data source at each timestamp from a tracked video.\nWe may also wish to know if some other digital events are co-occurring. For instance, licks are often organized in sequences of so-called bouts, so there are multiple intervals of licks within a single bout. That digital interval may be contained within the data manager. We can include that as a column, or each lick can be identified by an integer that represents which bout it belongs to. So there may be three licks within the first bout that all have an ID of 0, then licks four and five have a bout ID of 1, et cetera.\nThis can also be a convenient interface to get time conversion information from data that is in a different timeframe. For instance, imagine if we are also considering the ID of a laser digital interval that is coincident with a lick. This laser may be acquired in a different timeframe with a different sampling rate. In addition to getting the ID of the coincident laser interval, we can also get its start time in units of camera frames, like the lick. In this way, we’ll be finding what is the closest camera frame to the beginning of the laser start time that was around this lick. This can be useful for aligning to different event types.\nThis data can be serialized to a CSV. In this way, we can also include data that would be array-like, like gathering all of the events within some interval. This could be useful for getting, say, the spike times that occur within some meaningful experimental time block.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#analysis-dashboard",
    "href": "developer/table_view.html#analysis-dashboard",
    "title": "Table View",
    "section": "Analysis Dashboard",
    "text": "Analysis Dashboard\nThe next use of the TableView is in the analysis dashboard. This is one of the main plotting interfaces for Neuralyzer. In this widget, we are able to plot different kinds of data that are in the data manager. The TableView then allows us to also plot different transformations of data that is in the data manager. For instance, if we wanted to visualize a raster plot from a spiking neuron, we could use the TableView to use a row selector that is an interval around some event. Let’s imagine aligning to lick onset. We could pick one second on either side of the lick onset and call that an interval, and then gather the event times for each of those. This would give us a data structure that is essentially an array of arrays, and we can also normalize these to that event time. This gives us the exact data structure necessary for creating a raster plot.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#feature-extraction-and-dimensionality-reduction",
    "href": "developer/table_view.html#feature-extraction-and-dimensionality-reduction",
    "title": "Table View",
    "section": "Feature Extraction and Dimensionality Reduction",
    "text": "Feature Extraction and Dimensionality Reduction\nThe TableView architecture also provides a convenient interface to get something that looks like a 2D array. Imagine if we wanted to try to figure out features of a two-dimensional line to determine its identity. We could calculate with computers to make columns for the curvature of the line, and perhaps we would want to get the x and y positions at different fractional line lengths (e.g., where is the x position at 20% along the line, 40% along the line, 60% along the line, etc.). Then we could also include the angle of the line, the follicular position, etc.\nI probably don’t want to have to go through the data transform widget and extract all of these and keep them hanging around, because what I really would want is to have a 2D array and then perform some kind of dimensionality reduction on it. Then, with those arbitrary features, I might be able to look for clusters that ideally would correspond to the same whisker, compared to multiple whiskers that would have differences in the first or second principal component.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#machine-learning-interface",
    "href": "developer/table_view.html#machine-learning-interface",
    "title": "Table View",
    "section": "Machine Learning Interface",
    "text": "Machine Learning Interface\nSimilarly, the TableView provides a convenient interface to try to gather data to feed into some kind of machine learning algorithm. We can use the TableView to get our feature matrix for training some kind of model. In that case, if we are trying to classify specific frames of a movie that correspond to some behavior, we could use time as our row selector. Then we could use different calculated quantities from the video, like, say, the 2D positions of key points as the columns, and feed this to some kind of machine learning classification model if we also have accompanying labels. For instance, a label could specify that this frame corresponds to some behavior and this one does not. That could even be one of the columns of our TableView that we then separate and treat differently.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/media_widget.html",
    "href": "developer/media_widget.html",
    "title": "Media Widget",
    "section": "",
    "text": "The media widget is responsible for displaying data that can be visualized on a canvas. The types of data that can be displayed by the data manager include point data, line data, mask data, tensor data, interval data, and media data. Data currently in the data manager matching these types is displayed on the top left side. In this table, the user can click to enable the visibility of particular data in the accompanying media window canvas.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/media_widget.html#overview-and-data-display",
    "href": "developer/media_widget.html#overview-and-data-display",
    "title": "Media Widget",
    "section": "",
    "text": "The media widget is responsible for displaying data that can be visualized on a canvas. The types of data that can be displayed by the data manager include point data, line data, mask data, tensor data, interval data, and media data. Data currently in the data manager matching these types is displayed on the top left side. In this table, the user can click to enable the visibility of particular data in the accompanying media window canvas.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/media_widget.html#data-specific-interactions-and-manipulations",
    "href": "developer/media_widget.html#data-specific-interactions-and-manipulations",
    "title": "Media Widget",
    "section": "Data-Specific Interactions and Manipulations",
    "text": "Data-Specific Interactions and Manipulations\nClicking on a data item also brings up a sub-widget below the table, offering data type-specific manipulations. For example, this sub-widget would be responsible for changing attributes such as color, alpha value, marker type, etc., for point data within the media window. This sub-widget may also handle data-specific manipulations directly within the media window. For instance, the user might wish to click on a position in the media window to set it as the location for the currently displayed point. Alternatively, the user might click and hold to utilize a paintbrush-type function for extending or erasing mask data.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/media_widget.html#shared-components-and-common-interactions",
    "href": "developer/media_widget.html#shared-components-and-common-interactions",
    "title": "Media Widget",
    "section": "Shared Components and Common Interactions",
    "text": "Shared Components and Common Interactions\nThese data-specific sub-widgets may house common sub-widgets that are shared among others. For instance, color selection is a common feature for multiple data types. An interface for click, hover, and release interactions is common to multiple data types but may perform different actions, such as painting a mask or erasing the media foreground.\nNote that there is some overlap here with the data manager widget. For many data types the data manager widget includes the ability to view all data present in the data manager and select particular instances such as a point at a certain time and the user can delete that point. Consequently we will try to keep media window based manipulation to the media widget. \nThe media window is the widget that houses the actual canvas for display. It is also the owner of the specific drawing options and drawing routines onto the canvas. For instance if the user changes the marker size and color of point data the options that are updated are held by the media window. The media window uses qt-based drawing. It is also responsible for receiving mouse events inside its space and emits signals that other widgets can receive.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/Features/overview.html",
    "href": "developer/Features/overview.html",
    "title": "Features",
    "section": "",
    "text": "Documentation of some of the general widget types in Neuralyzer.",
    "crumbs": [
      "Features"
    ]
  },
  {
    "objectID": "developer/display_options.html",
    "href": "developer/display_options.html",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options.\n\n\n\n\n\n\nRange: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives\n\n\n\n\n\n\n\n\nRange: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nPosition Marker: Display a marker at a specified percentage distance along the line (off by default)\n\nshow_position_marker (bool): Enable/disable position marker display\nposition_percentage (int, 0-100%): Position along line where marker appears\n\nLine Segment: Display only a portion of the line between two percentage points (off by default)\n\nshow_segment (bool): Enable/disable segment-only display mode\nsegment_start_percentage (int, 0-100%): Start percentage for line segment\nsegment_end_percentage (int, 0-100%): End percentage for line segment\n\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nRange: 0-100% along the cumulative length of the line\nUI Controls:\n\nCheckbox to enable/disable the feature (off by default)\nHorizontal slider for quick adjustments\nSpin box for precise numeric input with % suffix\n\nDefault Value: 20% along the line\nVisual Appearance: Distinctive filled circle with white border, same color as the line\nCalculation: Based on cumulative distance along line segments, not point indices\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nPosition marker calculated using cumulative distance along line segments\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots\n\n\n\nThe segment feature allows displaying only a portion of the line between two percentage points along its cumulative distance:\n\nget_segment_between_percentages(): Utility function in lines.hpp that extracts a continuous segment\n\nCalculates cumulative distances along the original line\nPerforms linear interpolation for precise start/end points\nReturns a new Line2D containing only the specified segment\nHandles edge cases (empty lines, invalid percentages, zero-length segments)\n\nUI Validation: Start percentage cannot exceed end percentage (enforced in UI)\nRendering Logic: When enabled, replaces the full line with the extracted segment in all rendering operations\nPosition Marker Compatibility: Position markers work correctly with segments (percentage relative to segment, not original line)\n\n\n\n\nThe bounding box feature provides a visual outline around mask regions:\n\nget_bounding_box(): Utility function in masks.hpp that calculates min/max coordinates\n\nIterates through all mask points to find extrema\nReturns pair of Point2D representing opposite corners\nHandles single-point masks correctly\n\nRendering Logic: Draws unfilled rectangles using Qt’s addRect() with Qt::NoBrush\nCoordinate Scaling: Applies same aspect ratio scaling as mask data\nMultiple Masks: Each mask gets its own bounding box when feature is enabled\nTime Handling: Bounding boxes are drawn for both current time and time -1 masks\nContainer Management: Uses separate _mask_bounding_boxes container (similar to digital intervals) to avoid type conflicts\n\n\n\n\n\n\n\n\n\n\n\nstruct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n    bool show_position_marker{false};\n    int position_percentage{20};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - Position markers use get_position_at_percentage() for accurate placement - All configurations support real-time updates without data loss\n\n\n\n\n\n\n\nSelect the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\nEnable position marker to highlight specific locations along lines\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nUse get_position_at_percentage() for accurate line position calculations\nDocument new features in this file\n\n\n\n\nMask data represents 2D regions or shapes that can be overlaid on the media canvas:\n\nColor and Alpha: Configurable mask color and transparency\nBounding Box: Display rectangular outline around the mask extent (off by default)\n\nshow_bounding_box (bool): Enable/disable bounding box display\n\n\n\n\n\n\n\n\nThe mask display system includes:\n\nColor and Alpha Control: Masks can be displayed with configurable colors and transparency levels\nBounding Box: Option to display a rectangular outline around each mask showing its bounds\nOutline Drawing: Option to display the mask boundary as a thick line by connecting extremal points\n\n\n\nWhen enabled, a bounding box renders a rectangle outline around each mask. The implementation: - Uses the existing get_bounding_box() utility function from masks.hpp - Scales coordinates properly with aspect ratios - Draws unfilled rectangles using Qt::NoBrush for outline-only appearance - Handles both current time and time -1 masks\n\n\n\nWhen enabled, displays the mask boundary as a thick line connecting extremal points. The algorithm: - For each unique x coordinate, finds the maximum y value - For each unique y coordinate, finds the maximum x value\n- Collects all extremal points and sorts them by angle from centroid - Connects points to form a closed boundary outline - Renders as a thick 4-pixel wide line using QPainterPath\nThe outline feature uses the get_mask_outline() function to compute boundary points by finding extremal coordinates, providing a visual representation of the mask’s outer boundary."
  },
  {
    "objectID": "developer/display_options.html#overview",
    "href": "developer/display_options.html#overview",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options."
  },
  {
    "objectID": "developer/display_options.html#point-display-options",
    "href": "developer/display_options.html#point-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives"
  },
  {
    "objectID": "developer/display_options.html#line-display-options",
    "href": "developer/display_options.html#line-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nPosition Marker: Display a marker at a specified percentage distance along the line (off by default)\n\nshow_position_marker (bool): Enable/disable position marker display\nposition_percentage (int, 0-100%): Position along line where marker appears\n\nLine Segment: Display only a portion of the line between two percentage points (off by default)\n\nshow_segment (bool): Enable/disable segment-only display mode\nsegment_start_percentage (int, 0-100%): Start percentage for line segment\nsegment_end_percentage (int, 0-100%): End percentage for line segment\n\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nRange: 0-100% along the cumulative length of the line\nUI Controls:\n\nCheckbox to enable/disable the feature (off by default)\nHorizontal slider for quick adjustments\nSpin box for precise numeric input with % suffix\n\nDefault Value: 20% along the line\nVisual Appearance: Distinctive filled circle with white border, same color as the line\nCalculation: Based on cumulative distance along line segments, not point indices\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nPosition marker calculated using cumulative distance along line segments\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots\n\n\n\nThe segment feature allows displaying only a portion of the line between two percentage points along its cumulative distance:\n\nget_segment_between_percentages(): Utility function in lines.hpp that extracts a continuous segment\n\nCalculates cumulative distances along the original line\nPerforms linear interpolation for precise start/end points\nReturns a new Line2D containing only the specified segment\nHandles edge cases (empty lines, invalid percentages, zero-length segments)\n\nUI Validation: Start percentage cannot exceed end percentage (enforced in UI)\nRendering Logic: When enabled, replaces the full line with the extracted segment in all rendering operations\nPosition Marker Compatibility: Position markers work correctly with segments (percentage relative to segment, not original line)\n\n\n\n\nThe bounding box feature provides a visual outline around mask regions:\n\nget_bounding_box(): Utility function in masks.hpp that calculates min/max coordinates\n\nIterates through all mask points to find extrema\nReturns pair of Point2D representing opposite corners\nHandles single-point masks correctly\n\nRendering Logic: Draws unfilled rectangles using Qt’s addRect() with Qt::NoBrush\nCoordinate Scaling: Applies same aspect ratio scaling as mask data\nMultiple Masks: Each mask gets its own bounding box when feature is enabled\nTime Handling: Bounding boxes are drawn for both current time and time -1 masks\nContainer Management: Uses separate _mask_bounding_boxes container (similar to digital intervals) to avoid type conflicts"
  },
  {
    "objectID": "developer/display_options.html#technical-architecture",
    "href": "developer/display_options.html#technical-architecture",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "struct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n    bool show_position_marker{false};\n    int position_percentage{20};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - Position markers use get_position_at_percentage() for accurate placement - All configurations support real-time updates without data loss"
  },
  {
    "objectID": "developer/display_options.html#usage-guidelines",
    "href": "developer/display_options.html#usage-guidelines",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Select the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\nEnable position marker to highlight specific locations along lines\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nUse get_position_at_percentage() for accurate line position calculations\nDocument new features in this file\n\n\n\n\nMask data represents 2D regions or shapes that can be overlaid on the media canvas:\n\nColor and Alpha: Configurable mask color and transparency\nBounding Box: Display rectangular outline around the mask extent (off by default)\n\nshow_bounding_box (bool): Enable/disable bounding box display\n\n\n\n\n\n\n\n\nThe mask display system includes:\n\nColor and Alpha Control: Masks can be displayed with configurable colors and transparency levels\nBounding Box: Option to display a rectangular outline around each mask showing its bounds\nOutline Drawing: Option to display the mask boundary as a thick line by connecting extremal points\n\n\n\nWhen enabled, a bounding box renders a rectangle outline around each mask. The implementation: - Uses the existing get_bounding_box() utility function from masks.hpp - Scales coordinates properly with aspect ratios - Draws unfilled rectangles using Qt::NoBrush for outline-only appearance - Handles both current time and time -1 masks\n\n\n\nWhen enabled, displays the mask boundary as a thick line connecting extremal points. The algorithm: - For each unique x coordinate, finds the maximum y value - For each unique y coordinate, finds the maximum x value\n- Collects all extremal points and sorts them by angle from centroid - Connects points to form a closed boundary outline - Renders as a thick 4-pixel wide line using QPainterPath\nThe outline feature uses the get_mask_outline() function to compute boundary points by finding extremal coordinates, providing a visual representation of the mask’s outer boundary."
  },
  {
    "objectID": "developer/data_transform_widget.html",
    "href": "developer/data_transform_widget.html",
    "title": "Data Transform Widget",
    "section": "",
    "text": "The data transform interface is used for processing different data types. The core idea is to define various data processing operations that can be dynamically discovered and applied to different types of data, with parameters configurable through the UI. The system employs several design patterns, most notably the Strategy pattern for individual transformations and a Registry pattern for managing them, along with Factory Method for creating UI components.",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_transform_widget.html#core-components-and-workflow",
    "href": "developer/data_transform_widget.html#core-components-and-workflow",
    "title": "Data Transform Widget",
    "section": "Core Components and Workflow",
    "text": "Core Components and Workflow\nThe system revolves around a few key components:\n\nTransformOperation (Strategy Pattern): This is an abstract base class that defines the interface for all data transformation operations. Each concrete operation (e.g., EventThresholdOperation, MaskAreaOperation) inherits from TransformOperation and implements methods like getName(), getTargetInputTypeIndex(), canApply(), and execute(). This allows different algorithms (strategies) for data transformation to be used interchangeably.\nTransformParametersBase and derived structs (e.g., ThresholdParams): These structures hold the parameters for specific transformations. TransformParametersBase is a base class, and each operation can define its own derived struct (like ThresholdParams for thresholding operations) to store specific settings.\nTransformRegistry (Registry Pattern): This class acts as a central repository for all available TransformOperation instances. On initialization, it registers various concrete operation objects (e.g., MaskAreaOperation, EventThresholdOperation). It provides methods to find operations by name and to get a list of applicable operations for a given data type.\nTransformParameter_Widget (UI Abstraction): This is an abstract base class for UI widgets that allow users to set parameters for a TransformOperation. Concrete classes like AnalogEventThreshold_Widget inherit from it and provide the specific UI controls (e.g., spinboxes, comboboxes) for an operation.\nDataTransform_Widget (Main UI Controller): This Qt widget orchestrates the user interaction for data transformations.\n\nIt uses a Feature_Table_Widget to display available data items (features) from a DataManager.\nWhen a feature is selected, it queries the TransformRegistry to find applicable operations for that feature’s data type.\nIt populates a QComboBox with the names of these operations.\nWhen an operation is selected, it uses a map of factory functions (_parameterWidgetFactories) to create and display the appropriate TransformParameter_Widget (e.g., AnalogEventThreshold_Widget) for that operation. This is an example of the Factory Method pattern.\nIt has a “Do Transform” button that, when clicked, retrieves the parameters from the current TransformParameter_Widget, gets the selected TransformOperation from the TransformRegistry, and executes the operation on the selected data.",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_transform_widget.html#features",
    "href": "developer/data_transform_widget.html#features",
    "title": "Data Transform Widget",
    "section": "Features",
    "text": "Features\n\nThe ProgressCallback mechanism allows the TransformOperation to notify the DataTransform_Widget about its progress, which then updates the UI. This is a simple form of the Observer pattern.",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_transform_widget.html#design-example",
    "href": "developer/data_transform_widget.html#design-example",
    "title": "Data Transform Widget",
    "section": "Design Example",
    "text": "Design Example\nFirst, the user will create the transformation translation unit. These are located in the DataManager/transforms directory. The transformations are organized according to their input type. The designer will first need to specify the transformation operation itself, which should take a pointer to the input and return a std::shared_ptr to the output type. The user can also overload this function to take a ProgressCallback for longer operations. For example:\nstruct MaskConnectedComponentParameters : public TransformParametersBase {\n    /**\n     * @brief Minimum size (in pixels) for a connected component to be preserved\n     * \n     * Connected components smaller than this threshold will be removed from the mask.\n     * Must be greater than 0.\n     */\n    int threshold = 10;\n};\n\n///////////////////////////////////////////////////////////////////////////////\n\n/**\n * @brief Remove small connected components from mask data\n * \n * This function applies connected component analysis to remove small isolated\n * regions from masks. Uses 8-connectivity (considers diagonal neighbors as connected).\n * \n * @param mask_data The MaskData to process\n * @param params The connected component parameters\n * @return A new MaskData with small connected components removed\n */\nstd::shared_ptr&lt;MaskData&gt; remove_small_connected_components(\n        MaskData const * mask_data,\n        MaskConnectedComponentParameters const * params = nullptr);\n\n/**\n * @brief Remove small connected components from mask data with progress reporting\n * \n * @param mask_data The MaskData to process\n * @param params The connected component parameters\n * @param progressCallback Progress reporting callback\n * @return A new MaskData with small connected components removed\n */\nstd::shared_ptr&lt;MaskData&gt; remove_small_connected_components(\n        MaskData const * mask_data,\n        MaskConnectedComponentParameters const * params,\n        ProgressCallback progressCallback);\nThe user will also need to define a TransformationOperation interface class that uses this function. This is defined in DataManager/transforms/data_transforms.hpp and this is an example for the MaskConnectedComponentOperation:\n\nclass MaskConnectedComponentOperation final : public TransformOperation {\npublic:\n    [[nodiscard]] std::string getName() const override;\n    [[nodiscard]] std::type_index getTargetInputTypeIndex() const override;\n    [[nodiscard]] bool canApply(DataTypeVariant const & dataVariant) const override;\n    [[nodiscard]] std::unique_ptr&lt;TransformParametersBase&gt; getDefaultParameters() const override;\n    \n    DataTypeVariant execute(DataTypeVariant const & dataVariant,\n                           TransformParametersBase const * transformParameters) override;\n                           \n    DataTypeVariant execute(DataTypeVariant const & dataVariant,\n                           TransformParametersBase const * transformParameters,\n                           ProgressCallback progressCallback) override;\n};\nThe body of these functions is mostly boilerplate that will be verbatim between operations.\n\n\n\n\n\n\nImportant\n\n\n\nThe value return by the getName function will need to be used later in the User Interface exactly. If you do not use the name here to identify your transformation, it may not appear in the UI.\n\n\nAfter you have designed your tranformation, add the files to the CMakeLists.txt for DataManager listed in DataManager/CMakeLists.txt. Then include your header in DataManager/transforms/TransformRegistry.cpp and add your type with the _registerOperation function.\nNow you can create the user interface for your transformation operation. The user interfaces are kept in DataTransform_Widget under folders for the specific input type (same as the transformation). Create a hpp/cpp/ui triplet for your transformation. The purpose of this UI should be to populate parameters structure you created with the tranformation. If you have no options structure, this widget can simply be a label the describes the transformation. See MaskArea_Widget for an example fo a blank UI and LineResample_Widget for a more complex example. Your widget will need to inherit from TransformParameter_Widget as a base class.\nOnce you have completed your widget triplet, add these files to the main CMakeLists.txt for WhiskerToolbox. Then you will modify DataTransform_Widget.cpp to include the header to your UI, and populate _parameterWidgetFactories with the name of your transformation. For example:\n\n_parameterWidgetFactories[\"Remove Small Connected Components\"] = [](QWidget * parent) -&gt; TransformParameter_Widget * {\n        return new MaskConnectedComponent_Widget(parent);\n    };\nNote that the name in this map (e.g. “Remove Small Connected Components”) must match the name that is returned by your transformation operation!\nAfter this, compile and your transformation should appear in the data transformation widget!",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_manager.html",
    "href": "developer/data_manager.html",
    "title": "Data Manager",
    "section": "",
    "text": "The DataManager has 3 core functionalities",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/data_manager.html#time-frame-structure-for-multi-rate-data-visualization",
    "href": "developer/data_manager.html#time-frame-structure-for-multi-rate-data-visualization",
    "title": "Data Manager",
    "section": "Time Frame Structure for Multi-Rate Data Visualization",
    "text": "Time Frame Structure for Multi-Rate Data Visualization\nAn important feature of the program is being able to simultaneously visualize data that was collected at different sampling rates. The time frame structure is designed to indicate when time events occur at a particular sampling frequency. The data manager is responsible for keeping track of which data is located in what time frame. Multiple data objects can belong to one time frame, but one data object can only belong to a single time frame. However, if the relationship between time frame objects is specified, then data requested in one coordinate system can be converted to another.\n\nIllustrative Example: Electrophysiology and Video Data\nConsider the following example: an NI-DAQ box samples at 30000 Hz for electrophysiology. This results in analog data with 30000 samples per second as well as event data for sorted spikes at 30000 Hz resolution. The experiment may have also had a high-speed video camera collecting frames at 500 Hz. A digital event for each frame was recorded with the same NI-DAQ. The user may have processed the video frames to categorize behavior, which would also be at the 500 Hz resolution and be interval data.\n\n\nDefault Time Frame and Customization\nThe default time frame is simply called “time” and defaults to numbers between 1 and the number of frames in a loaded video. The scroll bar operates in this time frame and consequently sends signals in this time frame. The user can override this default time frame and replace it with an event structure with the same number of samples but where each event corresponds to the 30000 Hz resolution digitized camera frame exposures. The user can then create a new clock called “master” that again counts from 1,2,3,… up to the total number of samples collected by the NI-DAQ.\n\n\nData Indexing within Time Frames\nAll data manager data types have a notion of “index,” and these correspond to the time frame they are associated with. This index property is important because data may be sparsely labeled and not have the same number of samples as their time frame.\n\n\nWidget Considerations for Data Synchronization\nWidgets that represent different data simultaneously must be aware of accounting for these differences.\n\n\nPerformance Notes\nThe user should also be aware that pointer indirections sample by sample for large vectors will be quite inefficient. If the user needs to find a series of values in a range in a different coordinate system it is most likely important to.",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/data_manager.html#observer-framework",
    "href": "developer/data_manager.html#observer-framework",
    "title": "Data Manager",
    "section": "Observer Framework",
    "text": "Observer Framework",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/data_manager.html#data-types",
    "href": "developer/data_manager.html#data-types",
    "title": "Data Manager",
    "section": "Data Types",
    "text": "Data Types\n\nPoint\nA Point represents a 2 dimensional (x, y) coordinate, which may vary in time. The PointData structure can hold multiple points per unit time.\n\n\n\nExamples of Point Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLine\nA Line represents a 2 dimensional (x,y) collection of points, which may vary with time. The collection of points is ordered, meaning that each point is positioned relative to its neighbors. The LineData structure can hold multiple lines per unit time.\n\n\n\nExamples of Line Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurrently, lines are represented by a raw list of points. One could imagine lines being parameterized in different ways, such as the coefficients of a polynomial fit. For many calculations, the parameterized version of a line may be less memory intensive and more efficient.\n\n\nMask\nA mask represents a 2 dimensional collection of points, which may vary with time. Compared to a Line, the points in a Mask are not ordered. These would correspond to something like a binary segmentation mask over an image. The MaskData structure can hold multiple masks per unit time.\n\n\n\nExamples of Mask Data\n\n\n\n\nBinary Semantic Segmentation Labels\n\n\n\n\n\n\n\n\n\nA mask may just be thought as a raw pixel, by pixel definition of a shape. Shapes could be defined as bounding boxes, circles, polygons, etc. It may one day be useful to describe shapes in other ways compared to the raw pixel-by-pixel definition.\n\n\nTensors\nTensors are N-Dimensional data structures, and consequently very flexible containers for complex data. A concrete use would be to store a Height x Width x Channel array for different timepoints during an experiment. These may be the features output from an encoder neural network that processes a video.\n\n\n\nExamples of Tensor Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalog Time Series\nAn analog time series has values that can vary continuously\n\n\n\nExamples of Analog Time Series Data\n\n\n\n\nVoltage traces from electrode recordings\n\n\nFluorescence intensity traces from Calcium imaging\n\n\n\n\n\n\n\n\nDigital Event Series\nA digital event represents an ordered sequence of events, where each event is represented as a single instance in time. For instance, spike times from electrophysiology\n\n\n\nExamples of Event Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigital Interval Series\n\n\n\nExamples of Interval Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedia\nMedia is a sequence of images.\n\nImage\n\n\n\nExamples of Image Data\n\n\n\n\nImage sequence from two photon calcium imaging experiment\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\nExamples of Video Data\n\n\n\n\nMP4 video from high speed scientific camera of behavior\n\n\n\n\n\n\n\n\n\n\n\n\nTime Frame",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/building.html",
    "href": "developer/building.html",
    "title": "Building",
    "section": "",
    "text": "CMakeUserPresets.json\n\nIDE Specific Instructions:\n\nCLion\nAt the top to configure build properties, select More Action -&gt; Configuration -&gt; Edit…\nUnder Environment variables, adding this on windows works for me:\nPATH=bin\\;C:\\Qt\\6.7.2\\msvc2019_64\\bin\\;_deps\\torch-src\\lib\\;_deps\\iir-build\\;$PATH$",
    "crumbs": [
      "Building"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "developer/analysis_dashboard.html",
    "href": "developer/analysis_dashboard.html",
    "title": "Analysis Dashboard: Embedding OpenGL Widgets in QGraphicsScene",
    "section": "",
    "text": "Overview\nThis note documents the pattern for embedding a QOpenGLWidget inside a QGraphicsScene via QGraphicsProxyWidget, as used by the Analysis Dashboard plots. The goal is to achieve smooth, interactive pan/zoom and reliable repainting while the plot lives as a QGraphicsItem within a scene.\nThe core pieces:\n\nA plot QGraphicsWidget subclass that owns a QGraphicsProxyWidget hosting the OpenGL widget\nAn OpenGL widget configured for proxy embedding and interactive input\nIntentional event routing so dragging and wheel actions reach the OpenGL widget (not the parent graphics item)\n\n\n\nArchitecture\n\nPlot item: QGraphicsWidget (e.g., SpatialOverlayPlotWidget, ScatterPlotWidget)\n\nDraws the plot frame and title in its paint()\nEmbeds an OpenGL child via QGraphicsProxyWidget\nRoutes mouse events: title area → item movement; content area → OpenGL interactivity\n\nOpenGL child: QOpenGLWidget (e.g., SpatialOverlayOpenGLWidget, ScatterPlotOpenGLWidget)\n\nHolds visualization state and OpenGL resources\nImplements interactive behavior: mouse press/move/release, wheel zoom, tooltips\nEmits signals to trigger lightweight parent/proxy repaints\n\n\n\n\nEmbedding pattern (plot item)\nConfigure the OpenGL widget and proxy to avoid stale frames and input conflicts when embedded in a QGraphicsScene:\n// In PlotWidget::setupOpenGLWidget()\n_opengl_widget = new ScatterPlotOpenGLWidget();\n\n_proxy_widget = new QGraphicsProxyWidget(this);\n_proxy_widget-&gt;setWidget(_opengl_widget);\n\n// OpenGL widget attributes for proxy embedding\n_opengl_widget-&gt;setAttribute(Qt::WA_AlwaysStackOnTop, false);\n_opengl_widget-&gt;setAttribute(Qt::WA_OpaquePaintEvent, true);\n_opengl_widget-&gt;setAttribute(Qt::WA_NoSystemBackground, true);\n_opengl_widget-&gt;setUpdateBehavior(QOpenGLWidget::NoPartialUpdate);\n\n// Prevent the proxy from intercepting movement/selection\n_proxy_widget-&gt;setFlag(QGraphicsItem::ItemIsMovable, false);\n_proxy_widget-&gt;setFlag(QGraphicsItem::ItemIsSelectable, false);\n\n// Make sure we repaint from the child every update\n_proxy_widget-&gt;setCacheMode(QGraphicsItem::NoCache);\n\n// Lay out inside the plot frame (leave room for title/border)\nQRectF rect = boundingRect();\nQRectF content_rect = rect.adjusted(8, 30, -8, -8);\n_proxy_widget-&gt;setGeometry(content_rect);\n_proxy_widget-&gt;widget()-&gt;resize(content_rect.size().toSize());\nEvent routing in the plot item ensures that content-area interaction reaches the OpenGL widget:\n// In PlotWidget::mousePressEvent(QGraphicsSceneMouseEvent* event)\nQRectF title_area = boundingRect().adjusted(0, 0, 0, -boundingRect().height() + 25);\n\nif (title_area.contains(event-&gt;pos())) {\n    // Title: allow moving/selecting the plot item\n    emit plotSelected(getPlotId());\n    setFlag(QGraphicsItem::ItemIsMovable, true);\n    AbstractPlotWidget::mousePressEvent(event);\n} else {\n    // Content: disable item movement so drag goes to the OpenGL child\n    emit plotSelected(getPlotId());\n    setFlag(QGraphicsItem::ItemIsMovable, false);\n    event-&gt;accept();\n}\n\n\nOpenGL widget configuration\nConfigure the QOpenGLWidget so it reliably receives hover/wheel events and repaints well under a proxy:\n// In OpenGLWidget constructor\nsetMouseTracking(true);\nsetFocusPolicy(Qt::StrongFocus);\n\n// Prefer a core profile and multisampling\nQSurfaceFormat fmt;\nfmt.setVersion(4, 1);\nfmt.setProfile(QSurfaceFormat::CoreProfile);\nfmt.setSamples(4);\nsetFormat(fmt);\n\n// Attributes for embedding in a QGraphicsProxyWidget\nsetAttribute(Qt::WA_AlwaysStackOnTop, false);\nsetAttribute(Qt::WA_OpaquePaintEvent, true);\nsetAttribute(Qt::WA_NoSystemBackground, true);\nsetUpdateBehavior(QOpenGLWidget::NoPartialUpdate);\nTypical interactive handlers (pan/zoom):\nvoid OpenGLWidget::mousePressEvent(QMouseEvent* e) {\n    if (e-&gt;button() == Qt::LeftButton) {\n        _is_panning = true;\n        _last_mouse_pos = e-&gt;pos();\n        e-&gt;accept();\n    } else {\n        e-&gt;ignore();\n    }\n}\n\nvoid OpenGLWidget::mouseMoveEvent(QMouseEvent* e) {\n    if (_is_panning && (e-&gt;buttons() & Qt::LeftButton)) {\n        QPoint delta = e-&gt;pos() - _last_mouse_pos;\n        float world_scale = 2.0f / (_zoom_level * std::min(width(), height()));\n        setPanOffset(_pan_offset_x + delta.x() * world_scale,\n                     _pan_offset_y - delta.y() * world_scale);\n        _last_mouse_pos = e-&gt;pos();\n        e-&gt;accept();\n    } else {\n        e-&gt;accept();\n    }\n}\n\nvoid OpenGLWidget::wheelEvent(QWheelEvent* e) {\n    float zoom_factor = 1.0f + (e-&gt;angleDelta().y() / 1200.0f);\n    setZoomLevel(_zoom_level * zoom_factor);\n    e-&gt;accept();\n}\n\n\nRepaint strategy\n\nThe OpenGL widget calls update() (optionally throttled) on interaction; the proxy and parent item listen to signals like zoomLevelChanged / panOffsetChanged to call update() on themselves too.\nDisable caching (QGraphicsItem::NoCache) on the proxy so frames are not reused while the GL child is animating.\n\n\n\nTroubleshooting\n\nSymptom: plot only updates after resize or clicking away\n\nEnsure the GL child has Qt::WA_OpaquePaintEvent, Qt::WA_NoSystemBackground, and NoPartialUpdate\nEnsure the proxy uses NoCache\nVerify event routing: in content area, disable ItemIsMovable and accept the event so drag/wheel reach the GL widget\nEnable setMouseTracking(true) and setFocusPolicy(Qt::StrongFocus) on the GL widget\n\n\n\n\nReferences\n\nPlot items: src/WhiskerToolbox/Analysis_Dashboard/Widgets/SpatialOverlayPlotWidget/SpatialOverlayPlotWidget.cpp, src/WhiskerToolbox/Analysis_Dashboard/Widgets/ScatterPlotWidget/ScatterPlotWidget.cpp\nOpenGL widgets: src/WhiskerToolbox/Analysis_Dashboard/Widgets/SpatialOverlayPlotWidget/SpatialOverlayOpenGLWidget.cpp, src/WhiskerToolbox/Analysis_Dashboard/Widgets/ScatterPlotWidget/ScatterPlotOpenGLWidget.cpp"
  },
  {
    "objectID": "developer/contributing_code.html",
    "href": "developer/contributing_code.html",
    "title": "Contributing Code",
    "section": "",
    "text": "Header files should use the hpp suffix, and source files should use the cpp suffix\nIncludes should follow the “Lakos” include order, that is\n\nThe prototype/interface header for this implementation (ie, the .h/.hh file that corresponds to this .cpp/.cc file).\nOther headers from the same project, as needed.\nHeaders from other non-standard, non-system libraries (for example, Qt, Eigen, etc).\nHeaders from other “almost-standard” libraries (for example, Boost)\nStandard C++ headers (for example, iostream, functional, etc.)\nStandard C headers (for example, cstdint, dirent.h, etc.)\n\nPrefer returning std::optional as a mechanism of error handling\nThis is a scientific computing library. Performance is critical. Helping the user to understand where errors have occurred is helpful, but keeping the program alive after an error is not critical. Functions should fail gracefully and provide informative error messages when they do. Logging should use spdlog.\nPrefer free functions as much as possible. Ideally, class member functions will be simple and pass member variables to free functions.\nPrefer standard library algorithms where possible\nPublic member functions and free function declarations should include doxygen comments above them. Private member function definitions should include doxygen comments above them.\nPrefer forward declarations in header files\nDocument pre-conditions and post-conditions in doxygen comments uses the @pre and @post tags.\nThis is a C++20 project. Prefer standard library algorithms and std::ranges where possible.\nPrivate member variables and functions in classes should be prefaced with an underscore operator (e.g. calculateMean). In a struct with public facing member variables, they should be prefaced with m followed by an underscore (e.g. m_height).",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#design-guidelines",
    "href": "developer/contributing_code.html#design-guidelines",
    "title": "Contributing Code",
    "section": "",
    "text": "Header files should use the hpp suffix, and source files should use the cpp suffix\nIncludes should follow the “Lakos” include order, that is\n\nThe prototype/interface header for this implementation (ie, the .h/.hh file that corresponds to this .cpp/.cc file).\nOther headers from the same project, as needed.\nHeaders from other non-standard, non-system libraries (for example, Qt, Eigen, etc).\nHeaders from other “almost-standard” libraries (for example, Boost)\nStandard C++ headers (for example, iostream, functional, etc.)\nStandard C headers (for example, cstdint, dirent.h, etc.)\n\nPrefer returning std::optional as a mechanism of error handling\nThis is a scientific computing library. Performance is critical. Helping the user to understand where errors have occurred is helpful, but keeping the program alive after an error is not critical. Functions should fail gracefully and provide informative error messages when they do. Logging should use spdlog.\nPrefer free functions as much as possible. Ideally, class member functions will be simple and pass member variables to free functions.\nPrefer standard library algorithms where possible\nPublic member functions and free function declarations should include doxygen comments above them. Private member function definitions should include doxygen comments above them.\nPrefer forward declarations in header files\nDocument pre-conditions and post-conditions in doxygen comments uses the @pre and @post tags.\nThis is a C++20 project. Prefer standard library algorithms and std::ranges where possible.\nPrivate member variables and functions in classes should be prefaced with an underscore operator (e.g. calculateMean). In a struct with public facing member variables, they should be prefaced with m followed by an underscore (e.g. m_height).",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#before-you-submit-a-pull-request",
    "href": "developer/contributing_code.html#before-you-submit-a-pull-request",
    "title": "Contributing Code",
    "section": "Before You Submit a Pull Request",
    "text": "Before You Submit a Pull Request\n\nClang Format\nPlease make sure to run clang-format on all of your submitted files with the style guidelines in the base directory. A CI check will ensure that this happens upon pull request. You can read more about clang-format here:\nhttps://clang.llvm.org/docs/ClangFormat.html\n\n\nClang Tidy\nPlease make sure to run clang-tidy on all of your submitted files with the style guidelines in the base directory. A CI check will ensure that this happens upon pull request. You can read more about clang-tidy here:\nhttps://clang.llvm.org/extra/clang-tidy/",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#testing",
    "href": "developer/contributing_code.html#testing",
    "title": "Contributing Code",
    "section": "Testing",
    "text": "Testing\n\nTesting is performed with Catch2. \nThe component being tested generally has two TEST_CASE parts. The first will test the “happy path” to ensure that the computations work as expected. Different SECTIONs will be used for different computations. A second TEST_CASE will handle error handling and edge cases. Each SECTION will be for a different edge case / error.\nUse descriptive names for each test.\nTEST_CASES should also use useful tags.\nUse REQUIRE instead of CHECK\nSimple setup can be performed in the beginning of a TEST_CASE. Fixtures are only necessary for complex setup/teardown.\nPrefer Catch::Matchers::WithinRel to Catch::Approx\nTest files are included in the same folder as a given translation unit. They should have the same name as the header file with the extension “.test.cpp”. For example mask_to_line.hpp and mask_to_line.cpp will have the test file mask_to_line.test.cpp.",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#further-resources-and-references",
    "href": "developer/contributing_code.html#further-resources-and-references",
    "title": "Contributing Code",
    "section": "Further Resources and References",
    "text": "Further Resources and References\n\nLLM Support\nA configuration file for a plain text generator, repo-to-text, is included in the top level source directory. This can generate a single text file for the entire Neuralyzer repository which can be easily pasted into a LLM of choice.\n\n\nTesting\n\nC++ Development\nMike Shah has an excellent modern C++ design series on youtube. Episodes are a nice ~20 minute length:\nModern Cpp series by Mike Shah\n\n\nGraphics Programming\nMike Shah also has a series on modern OpenGL on youtube:\nIntroduction to OpenGL",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/data_manager_widget.html",
    "href": "developer/data_manager_widget.html",
    "title": "Data Manager Widget",
    "section": "",
    "text": "Data Manager Widget Overview\nThe data manager widget serves as the main user interface for interacting with all data currently loaded into the program. At its top, the widget displays all keys currently in the data manager along with their corresponding data types. It enables the user to specify the folder for saving data. Additionally, users can create new, initially blank data sets, which can subsequently be populated through further manipulations and other widgets.\nWhen a user selects an entry in the top table, a data type-specific soft widget is populated. This widget presents the user with various features of the selected data. For example, when dealing with point data, it shows a table listing every available point within that data set and its associated frame. The user has the ability to scroll through this table; clicking an entry causes the time displayed by the rest of the program to jump to that point’s frame. Users can perform several data manipulations via these specific widgets. For instance, using the point widget, a user can delete any specific point. They can also opt to move a specific point to another point data set available in the manager.\n\n\nData Saving Functionality\nThe data manager widget also functions as the primary interface for saving data to disk. It features a sub-widget interface where the user can select the desired file output type, and specific options for that output type are then displayed. For instance, if a user is viewing point data, they can choose to output CSV files and will be presented with various CSV saving options, such as selecting the delimiter and deciding whether a header should be included, etc.",
    "crumbs": [
      "Data Manager Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html",
    "href": "developer/data_viewer_widget.html",
    "title": "Data Viewer Widget",
    "section": "",
    "text": "The data viewer widget is designed for visualizing plots of time series data. It can operate on multiple distinct data types, such as analog time series, interval series, and event series. The widget contains a table of compatible data types and their corresponding keys, which can be selected for display in the viewer. It houses options to enable the visualization of different types and also includes an OpenGL canvas responsible for rendering the data according to user specifications.",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#event-viewer-features",
    "href": "developer/data_viewer_widget.html#event-viewer-features",
    "title": "Data Viewer Widget",
    "section": "Event Viewer Features",
    "text": "Event Viewer Features\nThe EventViewer_Widget provides controls for digital event series visualization with two main display modes:\n\nDisplay Modes\n\nStacked Mode (Default): Events are positioned in separate horizontal lanes with configurable spacing\n\nEach event series gets its own horizontal “lane”\nConfigurable vertical spacing between lanes\nConfigurable event line height\nAuto-calculation of optimal spacing when event groups are loaded\n\nFull Canvas Mode: Events stretch from top to bottom of the entire canvas\n\nOriginal behavior maintained for compatibility\nAll events span the full height of the display area\n\n\n\n\nAuto-Spacing for Event Groups\nWhen a tree of digital event series is selected and enabled, the system automatically calculates optimal spacing to fit all events on the canvas:\n\nIf 40 events are enabled on a 400-pixel tall canvas, each event gets approximately 10 pixels of space\nSpacing and height are calculated to ensure visual separation between different event series\nUses 80% of available canvas height, leaving margins at top and bottom\nEvent height is set to 60% of calculated spacing to prevent overlap\n\n\n\nUser Controls\nThe EventViewer_Widget provides: - Display Mode: Combo box to switch between Stacked and Full Canvas modes - Vertical Spacing: Adjustable spacing between stacked event series (0.01-1.0 normalized units) - Event Height: Adjustable height of individual event lines (0.01-0.5 normalized units) - Color Controls: Standard color picker for event line colors and transparency\n\n\nImplementation Details\n\nEvent stacking uses normalized coordinates for consistent spacing across different canvas sizes\nState is preserved when switching between data series\nIntegration with existing TreeWidgetStateManager for persistence\nOptimized rendering with proper OpenGL line thickness and positioning\n\n\n\nTime Frame Synchronization\nThe DataViewer Widget properly handles multi-rate data synchronization:\n\nMaster Time Frame: OpenGLWidget maintains a reference to the master time frame (“master” or “time”) used for X-axis coordinates\nTime Frame Conversion: When data series use different time frames from the master, proper coordinate conversion ensures synchronized display\nCross-Rate Compatibility: Supports simultaneous visualization of data collected at different sampling rates (e.g., 30kHz electrophysiology with 500Hz video)\nConsistent X-Axis: All data types (analog time series, digital events, digital intervals) are rendered with consistent X-axis positioning regardless of their native time frame\nRange Query Optimization: Data range queries are optimized for each time frame to ensure efficient rendering of large datasets",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#interactive-interval-editing",
    "href": "developer/data_viewer_widget.html#interactive-interval-editing",
    "title": "Data Viewer Widget",
    "section": "Interactive Interval Editing",
    "text": "Interactive Interval Editing\nThe DataViewer Widget supports interactive editing of digital interval series through a mouse-based dragging interface that handles multi-timeframe data seamlessly.\n\nInterval Selection and Highlighting\n\nClick-to-Select: Users can click on digital intervals to select them for editing\nVisual Feedback: Selected intervals are highlighted with enhanced borders for clear identification\nPer-Series Selection: Each digital interval series maintains its own independent selection state\n\n\n\nInterval Edge Dragging\nThe widget provides precise interval boundary editing through a sophisticated dragging system:\n\nCore Functionality\n\nEdge Detection: Mouse hover near interval boundaries (within 10 pixels) changes cursor to resize indicator\nDrag Initiation: Click and drag on interval edges to modify interval start or end times\nReal-time Preview: During dragging, both original (dimmed) and new (semi-transparent) interval positions are shown\nCollision Prevention: Automatic constraint enforcement prevents intervals from overlapping with existing intervals\n\n\n\nMulti-Timeframe Support\nThe interval dragging system automatically handles time frame conversion for data collected at different sampling rates:\n\nCoordinate Conversion: Mouse coordinates (in master time frame) are automatically converted to the series’ native time frame indices\nPrecision Handling: Dragged positions are rounded to the nearest valid index in the target time frame, accommodating different sampling resolutions\nConstraint Enforcement: Collision detection and boundary constraints are performed in the series’ native time frame for accuracy\nDisplay Consistency: Visual feedback remains in master time frame coordinates for consistent user experience\n\n\n\nError Handling and Robustness\n\nGraceful Degradation: Failed time frame conversions abort the drag operation while preserving original data\nData Integrity: Invalid interval bounds (e.g., start ≥ end) are rejected without modifying existing data\nState Management: Drag operations can be cancelled (ESC key) to restore original interval boundaries\n\n\n\nExample Use Cases\n\nBehavioral Annotation: Researchers can precisely adjust behavioral event boundaries recorded at video frame rates (30-120 Hz) while viewing synchronized neural data at higher sampling rates (20-30 kHz)\nEvent Refinement: Fine-tune automatically detected events by dragging boundaries to match observed signal characteristics across different data modalities\nCross-Modal Synchronization: Align interval boundaries across different measurement systems with varying temporal resolutions\n\n\n\n\nTechnical Implementation\nThe interval editing system leverages the existing TimeFrame infrastructure:\n\nTimeFrame.getIndexAtTime(): Converts master time coordinates to series-specific indices\nTimeFrame.getTimeAtIndex(): Converts series indices back to master time coordinates for display\nAutomatic Snapping: Ensures all interval boundaries align with valid time points in the target series\nThread Safety: All operations maintain data consistency during concurrent access\n\nThis functionality enables precise temporal analysis workflows while abstracting away the complexity of multi-rate data synchronization from the end user.",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#vertical-space-coordination",
    "href": "developer/data_viewer_widget.html#vertical-space-coordination",
    "title": "Data Viewer Widget",
    "section": "Vertical Space Coordination",
    "text": "Vertical Space Coordination\nThe DataViewer_Widget includes a sophisticated vertical space management system that prevents overlap between different data types and ensures optimal use of screen real estate.\n\nVerticalSpaceManager\nThe VerticalSpaceManager class handles automatic positioning and scaling for all data series:\n\nOrder-preserving: New data positioned below existing data\nType-aware spacing: Different configurations for analog (larger heights), digital events (compact), and intervals (moderate)\nAuto-redistribution: Adding new series triggers recalculation to prevent overlap\nCanvas-independent: Uses normalized coordinates for flexibility\n\n\n\nMVP Matrix Architecture\nThe rendering system uses a systematic Model-View-Projection (MVP) matrix approach for consistent positioning and scaling across all data types:\n\nModel Matrix - Series-Specific Transforms\nHandles individual series positioning and scaling:\n// For VerticalSpaceManager-positioned series:\nModel = glm::translate(Model, glm::vec3(0, series_center_y, 0));  // Position\nModel = glm::scale(Model, glm::vec3(1, series_height * 0.5f, 1)); // Scale\n\n// For analog series, additional amplitude scaling:\nfloat amplitude_scale = 1.0f / (stdDev * scale_factor);\nModel = glm::scale(Model, glm::vec3(1, amplitude_scale, 1));\n\n\nView Matrix - Global Operations\nHandles operations applied to all series equally:\nView = glm::translate(View, glm::vec3(0, _verticalPanOffset, 0)); // Global panning\n\n\nProjection Matrix - Coordinate System Mapping\nMaps world coordinates to screen coordinates:\n// X axis: time range [start_time, end_time] → screen width\n// Y axis: world coordinates [min_y, max_y] → screen height\nProjection = glm::ortho(start_time, end_time, min_y, max_y);\n\n\nVertex Coordinate Systems\nVerticalSpaceManager Mode (recommended): - Vertices use normalized coordinates (-1 to +1 in local space) - Model matrix handles all positioning and scaling - Consistent across all data types\nLegacy Mode (backward compatibility): - Vertices use world coordinates directly - Positioning handled by coordinate calculations - Index-based spacing for events\n\n\nDetection of Positioning Mode\nThe system automatically detects which positioning mode to use:\n\nDigital Events: vertical_spacing == 0.0f signals VerticalSpaceManager mode\nAnalog Series: y_offset != 0.0f signals VerticalSpaceManager mode\n\nDigital Intervals: y_offset != 0.0f signals VerticalSpaceManager mode\n\nThis ensures backward compatibility while enabling the new systematic approach.",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#interval-editing-system",
    "href": "developer/data_viewer_widget.html#interval-editing-system",
    "title": "Data Viewer Widget",
    "section": "Interval Editing System",
    "text": "Interval Editing System",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/Features/Collapsible_Widget.html",
    "href": "developer/Features/Collapsible_Widget.html",
    "title": "Collapsible Widget",
    "section": "",
    "text": "We have a Collapsible Widget called a “Section” available. This kind of widget can expand to display the contents of the widget. This is useful for making an interface appear less cluttered.\n\nGeneral process for to add collapsible Widget:\nQT Designer:\n\nEnsure the parent widget is in a vertical layout\nCreate QWidget inside and promote it to a “Section”\nCreate another QWidget within that QWidget. The inner QWidget will hold all the contents of the section.\nPopulate this inner QWidget with widgets as normal.\n\nCode part:\n\nCall autoSetContentLayout() on the Section QWidget after setupUi.\nOptionally, calling setTitle(\"title\") will give it a title.",
    "crumbs": [
      "Features",
      "Collapsible Widget"
    ]
  },
  {
    "objectID": "developer/file_io.html",
    "href": "developer/file_io.html",
    "title": "File IO",
    "section": "",
    "text": "Data Type\nFormat\nDependency\nRaw Data for Testing?\nIO Widget Testing\nDatamanager IO Testing\nJSON Validation Testing\nUser Guide\n\n\n\n\nAnalog\nBinary\n\n\n\n\n\n\n\n\nAnalog\nCSV\n\nSome\n\n\n\n\n\n\nDigitalEvent\nCSV\n\nYes\n\n\n\n\n\n\nDigitalInterval\nBinary\n\n\n\n\n\n\n\n\nDigitalInteral\nCSV\n\n\n\n\n\n\n\n\nLine Data\nBinary\nCap’n Proto\n\n\n\n\n\n\n\nLine Data\nCSV\n\n\n\n\n\n\n\n\nMask Data\nHDF5\nHDF5\n\n\n\n\n\n\n\nMask Data\nImage\nOpenCV\n\n\n\n\n\n\n\nMedia Data\nVideo\nffmpeg\nYes\n\n\n\n\n\n\nMedia Data\nImages\nOpenCV\n\n\n\n\n\n\n\nMedia Data\nHDF5\nHDF5\n\n\n\n\n\n\n\nPoint Data\nCSV\n\nYes\n\n\n\n\n\n\nTensor\nNumpy\nNumpy",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#overview-of-file-io-requirements",
    "href": "developer/file_io.html#overview-of-file-io-requirements",
    "title": "File IO",
    "section": "Overview of File IO Requirements",
    "text": "Overview of File IO Requirements\nBecause Neuralizer is compatible with a wide range of data, its file IO needs to support a wide range of file formats. Some of these file formats will be pre-subscribed, such as data acquisition-specific formats for electrophysiology. Others are more specific to this package, such as for line data that varies over time. We must also be aware from the start that some electrophysiology file sizes can be expected to be larger than easy to work with on a regular desktop machine. For instance, Neuropixel probes record almost 400 channels at 20 kHz and are becoming more routine for use. Consequently, multiple file formats should have the ability to be easily memory-mapped and loaded from disk rather than loading the entire file into RAM.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#incremental-data-adjustments-and-key-value-stores",
    "href": "developer/file_io.html#incremental-data-adjustments-and-key-value-stores",
    "title": "File IO",
    "section": "Incremental Data Adjustments and Key-Value Stores",
    "text": "Incremental Data Adjustments and Key-Value Stores\nAnother important consideration from the start is that many of these data types will be incrementally adjusted. For instance, processing a video sequence frame by frame or manually adjusting the output of an automated algorithm requires changing some subset of data in a much larger scheme. For some of these file types that are greater than hundreds of megabytes, resaving an entire file for point manipulations could be onerous and time-consuming. Consequently, for some file types, it would be advantageous to use a key-value database type structure that allows us to easily make incremental adjustments and only save those changes, consequently saving data much more quickly. To my knowledge, this process is quite common in the wild but uncommon in neuroscience.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#serialization-with-captain-proto",
    "href": "developer/file_io.html#serialization-with-captain-proto",
    "title": "File IO",
    "section": "Serialization with Captain Proto",
    "text": "Serialization with Captain Proto\nThe process of data serialization and deserialization will be accomplished with the Captain Proto library. This is able to easily create binary files from more complex data structures, such as those that hold multiple lines of varying size per unit time. This library can define output file structures that are purely binary and would be saved and loaded as entire objects. Alternatively, different definitions can be used to save objects as key-value pairs; for instance, each time point can be the key and the data at that time can be the value.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#memory-mapping-implementation",
    "href": "developer/file_io.html#memory-mapping-implementation",
    "title": "File IO",
    "section": "Memory Mapping Implementation",
    "text": "Memory Mapping Implementation\nCaptain Proto does not perform memory mapping itself. Memory mapping is different across different types of file systems, so to maintain cross-platform use, we will need to either specify different interfaces for interacting with Captain Proto or we could use another third-party library that does this for us. Boost.Interprocess is commonly cited for this purpose, but there are others.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#key-value-database-libraries",
    "href": "developer/file_io.html#key-value-database-libraries",
    "title": "File IO",
    "section": "Key-Value Database Libraries",
    "text": "Key-Value Database Libraries\nFor key-value database structures, there are multiple libraries available. Many of these are mature and designed to work with servers of extremely large datasets, and some have been developed by very large companies. One of the most popular is called LMDB, but in my test cases, this appears to be not as great with Windows. Using this library requires specifying some maximum file size; on Linux, the resulting file size is only the size of the data, but with my testing on Windows, this large maximum file size seems to persist. An alternative database is RocksDB. This software is developed by Facebook, seems to be quite mature, and should also be able to save as a key-value pair across platforms.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#prioritization-and-future-considerations",
    "href": "developer/file_io.html#prioritization-and-future-considerations",
    "title": "File IO",
    "section": "Prioritization and Future Considerations",
    "text": "Prioritization and Future Considerations\nCurrently, the data that would benefit from key-value updates is only theoretical. Having a line for whiskers for each frame in a high-speed video of up to 200,000 frames results in a file size that is almost 300 MB. However, saving a binary file in its entirety when making whisker-specific updates through manual curation still takes less than a second to save and is not noticeable to the user. Consequently, I think prioritizing having binary outputs to save entire files and memory mapping of those binary file types that are read-only, such as for Neuropixel analog traces, may be the most desirable next feature. Key-value pairs with something like RocksDB should be remembered for the future, but I am going to make that less of a priority until I see a clear use case that would benefit.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/point_display_options.html",
    "href": "developer/point_display_options.html",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options.\n\n\n\n\n\n\nRange: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives\n\n\n\n\n\n\n\n\nRange: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots\n\n\n\n\n\n\n\nstruct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - All configurations support real-time updates without data loss\n\n\n\n\n\n\n\nSelect the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nDocument new features in this file"
  },
  {
    "objectID": "developer/point_display_options.html#overview",
    "href": "developer/point_display_options.html#overview",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options."
  },
  {
    "objectID": "developer/point_display_options.html#point-display-options",
    "href": "developer/point_display_options.html#point-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives"
  },
  {
    "objectID": "developer/point_display_options.html#line-display-options",
    "href": "developer/point_display_options.html#line-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots"
  },
  {
    "objectID": "developer/point_display_options.html#technical-architecture",
    "href": "developer/point_display_options.html#technical-architecture",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "struct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - All configurations support real-time updates without data loss"
  },
  {
    "objectID": "developer/point_display_options.html#usage-guidelines",
    "href": "developer/point_display_options.html#usage-guidelines",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Select the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nDocument new features in this file"
  },
  {
    "objectID": "developer/transforms.html",
    "href": "developer/transforms.html",
    "title": "transforms",
    "section": "",
    "text": "Transform Catalog\nThe below table has all of the transformations currently available in the Data Transform widget. Each transform should have a test.cpp file (Column 2). The test.cpp file should include tests to make sure it can be populated by a JSON template (e.g. it can be run by loading data with a JSON file). Some transforms will “just work” because they are parameterless, but transforms with parameters will need to have them also described with a JSON link and put in the transform parameter factory. Documentation for users should be provided to describe the algorithm and its uses. Finally, a benchmark.cpp file should be created and placed in the same directory.\n\n\n\n\n\n\n\n\n\n\n\n\nTransform Name\nHas .test.cpp?\nJSON-based Test?\nParameters in ParameterFactory?\nUser Docs Exist?\nHas benchmark.cpp?\nBenchmark Results\n\n\n\n\nCalculate Mask Centroid\nYes\nYes\nYes\nNo\nNo\n\n\n\nConvert Mask to Line\nYes\nYes\nYes\nNo\nNo\n\n\n\nSkeletonize Mask\nYes\nYes\nYes\nNo\nNo\n\n\n\nRemove Small Connected Components\nYes\nYes\nYes\nNo\nNo\n\n\n\nApply Median Filter\nYes\nYes\nYes\nNo\nNo\n\n\n\nCalculate Area\nYes\nYes\nYes\nNo\nNo\n\n\n\nFill Mask Holes\nYes\nYes\nYes\nNo\nNo\n\n\n\nCalculate Mask Principal Axis\nYes\nYes\nYes\nNo\nNo\n\n\n\nWhisker Tracing\nNo\nN/A\nNo\nNo\nNo\n\n\n\nClip Line by Reference Line\nYes\nYes\nYes\nNo\nNo\n\n\n\nLine Alignment to Bright Features\nYes\nYes\nYes\nNo\nNo\n\n\n\nCalculate Line to Point Distance\nYes\nYes\nYes\nNo\nNo\n\n\n\nResample Line\nYes\nYes\nYes\nNo\nNo\n\n\n\nExtract Line Subsegment\nYes\nYes\nYes\nNo\nNo\n\n\n\nCalculate Line Angle\nYes\nYes\nYes\nNo\nNo\n\n\n\nCalculate Line Curvature\nYes\nYes\nYes\nNo\nNo\n\n\n\nExtract Point from Line\nYes\nYes\nYes\nNo\nNo\n\n\n\nFilter\nYes\nNo\nNo\nNo\nNo\n\n\n\nHilbert Phase\nYes\nYes\nYes\nNo\nNo\n\n\n\nThreshold Event Detection\nYes\nYes\nYes\nNo\nNo\n\n\n\nThreshold Interval Detection\nYes\nYes\nYes\nNo\nNo\n\n\n\nScale and Normalize\nYes\nYes\nYes\nNo\nNo\n\n\n\nGroup Intervals\nYes\nYes\nYes\nNo\nNo",
    "crumbs": [
      "transforms"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neuralyzer",
    "section": "",
    "text": "Neuralyzer is a cross-platform software package designed for analyzing and visualizing common forms of data generated during systems neuroscience experiments."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Neuralyzer",
    "section": "Installation",
    "text": "Installation\nThe software comes as compiled .exe, .dmg or tar files and can be immediately used on Windows, Mac and Linux - no install required !\nLatest Release: link\nOld Releases: link"
  },
  {
    "objectID": "index.html#supported-data-types",
    "href": "index.html#supported-data-types",
    "title": "Neuralyzer",
    "section": "Supported Data Types",
    "text": "Supported Data Types\nSome currently supported data types include:\n\nMultimedia - High speed video and collections of images\nArrays of simple 2D shapes - Points, Lines, and masks that vary with time.\nDigital time series data - Events, timestamps,\nAnalog time series data - Continuous movement variables"
  },
  {
    "objectID": "index.html#how-documentation-is-organized",
    "href": "index.html#how-documentation-is-organized",
    "title": "Neuralyzer",
    "section": "How Documentation is Organized",
    "text": "How Documentation is Organized\nThis documentation will follow the quadrants of the Diátaxis documentation authoring framework\n\nTutorials\nHow-to Guides\nConcepts\nReference Guides"
  },
  {
    "objectID": "labeling/mask.html",
    "href": "labeling/mask.html",
    "title": "Creating Mask Labels",
    "section": "",
    "text": "Creating mask labels is done through the GrabCut tool. Currently it can be opened through the Tongue Tracking widget."
  },
  {
    "objectID": "labeling/mask.html#grabcut-tool-operation",
    "href": "labeling/mask.html#grabcut-tool-operation",
    "title": "Creating Mask Labels",
    "section": "GrabCut Tool Operation",
    "text": "GrabCut Tool Operation\n\nSelecting ROI\nTo start creating a mask, left click and drag the mouse over the image to form a green rectangle as the region of interest. The objected intended to be masked should be completely within this rectangle. If a mistake is made the reset button returns the tool to the initial state.\nThe GrabCut algorithm works as such: the algorithm keeps track of the mask and for each iteration, attempts to refine it. In between iterations, the user can tweak the mask to provide feedback to the algorithm, which will be noted through subsequent iterations.\nAfter drawing an ROI the “Iterate Grabcut” button becomes functional. Using it completes one iteration of the GrabCut algorithm. The first usage of this button will show the initial mask created by the algorithm. Ideally the user should press this button throughout the editing process.\n\nOpacity of the displayed mask can be adjusted with the transparency slider\n\n\n\nUser Feedback\nIn between iterations the user has access to a drawing bush whose radius can be adjusted. It is used to paint feedback for the GrabCut algorithm, which it will take into account upon pressing the “Iterate Grabcut” button. The brush has four colors:\n\n“Definite Background”: Tells GrabCut the painted area is definitely not part of the mask.\n“Definite Foreground”: Tells GrabCut the painted area is definitely part of the mask.\n“Probable Background”: Suggests to GrabCut the painted area may not be part of the mask. GrabCut uses this information to create a better mask but may partially/fully disobey it.\n“Probable Foreground”: Suggests to GrabCut the painted area is likely part of the mask. GrabCut uses this information to create a better mask but may partially/fully disobey it.\n\n\n\nSaving and Exporting\nThe GrabCut tool may be exited at any time by pressing the “Exit” button. “Save and Exit” will exit the tool and save the mask into the main application and displayed in the media player. All created masks can be saved to disk using the “Save Drawn Masks” button in the Tongue Tracking widget."
  },
  {
    "objectID": "user_guide/behaviors/whisker.html",
    "href": "user_guide/behaviors/whisker.html",
    "title": "Whisker Tracking",
    "section": "",
    "text": "Load Whiskers\n\nSupported Whisker File Formats\n\n\n\n\n\n\nFile Format\nDescription\n\n\n\n\nJanelia\nBinary format output by janelia whisker tracker\n\n\nCSV\nEach row represents a 2d point (x,y) along the whisker. The points should be arranged from follicle to tip\n\n\nHDF5\n\n\n\n\n\n\nLoad Keypoints\n\n\nTrace Button\n\n\nLength Threshold\nWhisker segments below the length threshold will be discarded. Length threshold is in units of pixels\n\n\nWhisker Pad Selection\nThe whisker pad location in pixel coordinates. Candidate whiskers are ordered so that the base of the whisker is nearest the whisker pad. Whiskers with bases beyond some distance from the whisker pad can also be discarded.\n\n\nHead Orientation\nThe head orientation is the direction that the animal’s nose is pointing in the image. The head orientation is used to determine the identity of the whiskers in the frame (most posterior whisker is 0, next most posterior is 1, etc).\n\n\nNumber of Whiskers Selection\n\n\nExport Image and CSV Button\n\n\nFace Mask\nThe face mask corresponds to the part of the image belonging to the face. This can be used in several ways\n\nWhisker bases can be extended to always connect to the face mask. This eliminates jitter that can occur because of fur\nWhisker bases can be clipped to ensure that the whisker does not enter the face mask.\n\n\n\nJanelia Settings\n\n\nContact Detection",
    "crumbs": [
      "Behavioral Modules",
      "Whisker Tracking"
    ]
  },
  {
    "objectID": "user_guide/data_loading/JSON_loading.html",
    "href": "user_guide/data_loading/JSON_loading.html",
    "title": "JSON_loading",
    "section": "",
    "text": "Digital Event Series\nDigital event series are data represented by an ordered sequence of timestamps. Examples include spike timestamps from extracellular recordings or behavioral events (e.g. go cue, reward given).\n\n\nDigital Interval Series\n\n16 bit binary representation\n{\n  \"filepath\": \"ttl.bin\",\n  \"data_type\": \"digital_interval\",\n  \"name\": \"laser\",\n  \"format\": \"uint16\",\n  \"channel\": 2, // REQUIRED, bit (0 based) for channel of interest\n  \"transition\": \"rising\", //optional, \n  \"clock\": \"master\", //optional, clock signal to assign to these events\n  \"header_size\": 0 //optional, number of bytes to skip at start of file\n}\n\n\n\n\n\n\n\n\n\n\nElement\nDescription\nRequired?\nType\nNotes\n\n\n\n\nfilepath\nPath to binary file, relative to JSON file.\nYes\nstring\n\n\n\ndata_type\n\nYes\nstring\n“digital_interval”\n\n\nname\nName of the data once it is loaded into Neuralyzer\nYes\nstring\n\n\n\nformat\n\nYes\nstring\n“uint16”\n\n\nchannel\nSpecifies which bit in binary representation should be extracted as digital interval\nNo\nnumber\nDefault is 0. Valid values range from 0-15\n\n\nTransition\n“rising” will count a TTL interval as one extending from low-to-high transitions to high-to-low transitions. “falling” will count a TTL interval as one extending from high-to-low to low-to-high transitions.\nNo\nstring\nDefault is “rising”. Valid values are “rising” or “falling”.\n\n\nclock\nClock signal to associate with this digital interval\nNo\nstring\nThe clock string must match the name of a loaded clock signal.\n\n\nheader_size\nThis many bytes will be skipped at the beginning of the file before reading the rest.\nNo\nnumber\nDefault is 0. Accepted values range from 0 to size of file in bytes.\n\n\n\n\n\n\n\n\nCSV\n\n{\n  \"filepath\": \"ttl.bin\",\n  \"data_type\": \"digital_interval\",\n  \"name\": \"laser\",\n  \"format\": \"csv\"\n\n}\n\n\n\n\n\n\n\n\n\n\nElement\nDescription\nRequired?\nType\nNotes\n\n\n\n\nfilepath\nPath to csv file, relative to JSON file.\nYes\nstring\n\n\n\ndata_type\n\nYes\nstring\n“digital_interval”\n\n\nname\nName of the data once it is loaded into Neuralyzer\nYes\nstring\n\n\n\nformat\n\nYes\nstring\n“csv”",
    "crumbs": [
      "Data Loading",
      "JSON_loading"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html",
    "title": "Analog Hilbert Phase",
    "section": "",
    "text": "This transform calculates the instantaneous phase of an analog signal, which is useful for analyzing oscillations.\n\n\nThe Hilbert transform is a mathematical operation that shifts the phase of all positive frequency components of a signal by -90 degrees and all negative frequency components by +90 degrees. When this phase-shifted signal (the imaginary part) is combined with the original signal (the real part), it forms a complex-valued “analytic signal”.\nThe instantaneous phase is the angle of this complex number at each point in time. It provides a way to represent an oscillating signal in terms of its phase, which progresses from -π to +π for each cycle of the oscillation. This transform uses an efficient FFT-based method to compute the Hilbert transform.\n\n\n\n\n\nIn neuroscience, analyzing the phase of neural signals is crucial for understanding brain function. Some common applications include:\n\nRhythmic Behaviors: The phase of signals from sensors tracking rhythmic behaviors like whisking, sniffing, or licking can be extracted to correlate them with neural activity.",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html#overview",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html#overview",
    "title": "Analog Hilbert Phase",
    "section": "",
    "text": "This transform calculates the instantaneous phase of an analog signal, which is useful for analyzing oscillations.\n\n\nThe Hilbert transform is a mathematical operation that shifts the phase of all positive frequency components of a signal by -90 degrees and all negative frequency components by +90 degrees. When this phase-shifted signal (the imaginary part) is combined with the original signal (the real part), it forms a complex-valued “analytic signal”.\nThe instantaneous phase is the angle of this complex number at each point in time. It provides a way to represent an oscillating signal in terms of its phase, which progresses from -π to +π for each cycle of the oscillation. This transform uses an efficient FFT-based method to compute the Hilbert transform.\n\n\n\n\n\nIn neuroscience, analyzing the phase of neural signals is crucial for understanding brain function. Some common applications include:\n\nRhythmic Behaviors: The phase of signals from sensors tracking rhythmic behaviors like whisking, sniffing, or licking can be extracted to correlate them with neural activity.",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html#parameters",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html#parameters",
    "title": "Analog Hilbert Phase",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nlow_frequency: The low-cut frequency for the bandpass filter, in Hertz. This determines the lower end of the frequency range you want to analyze.\nhigh_frequency: The high-cut frequency for the bandpass filter, in Hertz. This determines the upper end of the frequency range you want to analyze.\ndiscontinuity_threshold: A time gap, in samples, above which the signal is considered to have a break. The transform will process the continuous segments separately. This is useful for data with missing samples or interruptions.",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html#example-configuration",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html#example-configuration",
    "title": "Analog Hilbert Phase",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that loads data and applies the Analog Hilbert Phase transform.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Hilbert Phase Pipeline\",\n            \"description\": \"Test Hilbert phase calculation on analog signal\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Hilbert Phase\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_signal\",\n                \"output_key\": \"phase_signal\",\n                \"parameters\": {\n                    \"low_frequency\": 5.0,\n                    \"high_frequency\": 15.0,\n                    \"discontinuity_threshold\": 1000\n                }\n            }\n        ]\n    }\n}\n]\n\nReferences\nHill, D.N., Curtis, J.C., Moore, J.D., Kleinfeld, D., 2011. Primary motor cortex reports efferent control of vibrissa motion on multiple timescales. Neuron 72, 344–356. https://doi.org/10.1016/j.neuron.2011.09.020",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/threshold_event_detection.html",
    "href": "user_guide/data_transformations/threshold_event_detection.html",
    "title": "Threshold Event Detection",
    "section": "",
    "text": "The Threshold Event Detection transform is used to detect events in an analog time series based on a threshold.",
    "crumbs": [
      "Data Transformations",
      "Threshold Event Detection"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/threshold_event_detection.html#parameters",
    "href": "user_guide/data_transformations/threshold_event_detection.html#parameters",
    "title": "Threshold Event Detection",
    "section": "Parameters",
    "text": "Parameters\n\nthreshold_value: The value that the signal must cross to be considered an event.\ndirection: The direction of the threshold crossing. Can be “Positive (Rising)”, “Negative (Falling)”, or “Absolute (Magnitude)”.\nlockout_time: The amount of time (in the same units as the time series) to wait after an event is detected before another event can be detected. This is useful for preventing multiple events from being detected for a single peak in the signal.",
    "crumbs": [
      "Data Transformations",
      "Threshold Event Detection"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/threshold_event_detection.html#output",
    "href": "user_guide/data_transformations/threshold_event_detection.html#output",
    "title": "Threshold Event Detection",
    "section": "Output",
    "text": "Output\nThe output of this transform is a new DigitalEventSeries data object that contains the timestamps of the detected events.",
    "crumbs": [
      "Data Transformations",
      "Threshold Event Detection"
    ]
  },
  {
    "objectID": "user_guide/machine_learning/ML_intro.html",
    "href": "user_guide/machine_learning/ML_intro.html",
    "title": "Overview",
    "section": "",
    "text": "The Machine Learning Widget allows us to fit models of relationships between multiple features of our data and then make predictions. This can be useful for semi-automated annotation of datasets, where the user labels some training data, and then predicts the remaining unlabeled frames. Then annotations can then be easily compared with the video data.",
    "crumbs": [
      "Machine Learning",
      "Overview"
    ]
  }
]
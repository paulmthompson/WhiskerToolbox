[
  {
    "objectID": "user_guide/index.html",
    "href": "user_guide/index.html",
    "title": "User Guide",
    "section": "",
    "text": "This is the user guide for neuralyzer",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "user_guide/image_processing/contrast.html",
    "href": "user_guide/image_processing/contrast.html",
    "title": "Contrast and Brightness Adjustment",
    "section": "",
    "text": "These two filters are implemented with a single linear transform taking parameters “alpha” (\\(\\alpha\\)) and “beta” (\\(\\beta\\)). Pixel values are multiplied by \\(\\alpha\\) to adjust contrast, then have \\(\\beta\\) added to adjust brightness: \\[\ng(x) = \\alpha f(x) + \\beta\n\\] In effect the magnitude of \\(\\alpha\\) corresponds to the amount of contrast and the magnitude of \\(\\beta\\) corresponds to the amount of brightness.",
    "crumbs": [
      "Image Processing",
      "Contrast and Brightness Adjustment"
    ]
  },
  {
    "objectID": "user_guide/image_processing/bilateral_filter.html",
    "href": "user_guide/image_processing/bilateral_filter.html",
    "title": "Bilateral Filter",
    "section": "",
    "text": "https://docs.opencv.org/4.x/js_filtering_bilateralFilter.html\nhttps://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga9d7064d478c95d60003cf839430737ed\nhttps://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html",
    "crumbs": [
      "Image Processing",
      "Bilateral Filter"
    ]
  },
  {
    "objectID": "user_guide/image_processing/bilateral_filter.html#references",
    "href": "user_guide/image_processing/bilateral_filter.html#references",
    "title": "Bilateral Filter",
    "section": "",
    "text": "https://docs.opencv.org/4.x/js_filtering_bilateralFilter.html\nhttps://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga9d7064d478c95d60003cf839430737ed\nhttps://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html",
    "crumbs": [
      "Image Processing",
      "Bilateral Filter"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_area.html",
    "href": "user_guide/data_transformations/mask_area.html",
    "title": "Calculate Mask Area",
    "section": "",
    "text": "This transform calculates the total area of all detected masks at each point in time, providing a measure of the overall size of masked regions.\n\n\nThis transformation processes a series of masks and, for each timestamp, calculates the total area covered by all masks at that specific time. The area is determined by summing the number of pixels (or voxels) that constitute each mask.\nThe result is an analog time series where the value at each point in time represents the total mask area for that moment. This is particularly useful for quantifying how the size of a region of interest (ROI) changes over time. Since this operation simply counts pixels within existing masks, it does not require any configuration parameters.\nThis transform takes a mask series as input and produces an analog time series as output.\n\n\n\n\nIn neuroscience research, quantifying the area of specific regions is a common requirement for analyzing imaging data:\n\nCell Swelling or Shrinking: When studying cellular dynamics, this transform can measure changes in the cross-sectional area of a cell over time, which might occur in response to a stimulus or pathological condition.\nPupilometry: In vision and cognitive neuroscience, tracking the area of the pupil provides insights into arousal, attention, and cognitive load. This transform can be applied to masks of the pupil from eye-tracking videos.\nLesion Sizing: In studies of brain injury or disease, this can be used to quantify the size of a lesion or a plaque from histological or in-vivo imaging data, tracking its progression over time.\nCalcium Imaging ROI Activity: For analyzing calcium imaging data, this transform can measure the area of a region of interest (ROI) that shows significant activity (e.g., above a certain brightness threshold), indicating the spatial extent of neural firing.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Area"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_area.html#overview",
    "href": "user_guide/data_transformations/mask_area.html#overview",
    "title": "Calculate Mask Area",
    "section": "",
    "text": "This transform calculates the total area of all detected masks at each point in time, providing a measure of the overall size of masked regions.\n\n\nThis transformation processes a series of masks and, for each timestamp, calculates the total area covered by all masks at that specific time. The area is determined by summing the number of pixels (or voxels) that constitute each mask.\nThe result is an analog time series where the value at each point in time represents the total mask area for that moment. This is particularly useful for quantifying how the size of a region of interest (ROI) changes over time. Since this operation simply counts pixels within existing masks, it does not require any configuration parameters.\nThis transform takes a mask series as input and produces an analog time series as output.\n\n\n\n\nIn neuroscience research, quantifying the area of specific regions is a common requirement for analyzing imaging data:\n\nCell Swelling or Shrinking: When studying cellular dynamics, this transform can measure changes in the cross-sectional area of a cell over time, which might occur in response to a stimulus or pathological condition.\nPupilometry: In vision and cognitive neuroscience, tracking the area of the pupil provides insights into arousal, attention, and cognitive load. This transform can be applied to masks of the pupil from eye-tracking videos.\nLesion Sizing: In studies of brain injury or disease, this can be used to quantify the size of a lesion or a plaque from histological or in-vivo imaging data, tracking its progression over time.\nCalcium Imaging ROI Activity: For analyzing calcium imaging data, this transform can measure the area of a region of interest (ROI) that shows significant activity (e.g., above a certain brightness threshold), indicating the spatial extent of neural firing.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Area"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_area.html#parameters",
    "href": "user_guide/data_transformations/mask_area.html#parameters",
    "title": "Calculate Mask Area",
    "section": "Parameters",
    "text": "Parameters\nThis transform does not have any parameters that need to be configured. It directly calculates the area from the input mask data.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Area"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_area.html#example-configuration",
    "href": "user_guide/data_transformations/mask_area.html#example-configuration",
    "title": "Calculate Mask Area",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to load mask data and run this transformation.\n[\n  {\n    \"transformations\": {\n      \"metadata\": {\n        \"name\": \"Mask Area Calculation Pipeline\",\n        \"description\": \"Test mask area calculation on mask data\",\n        \"version\": \"1.0\"\n      },\n      \"steps\": [\n        {\n          \"step_id\": \"1\",\n          \"transform_name\": \"Calculate Area\",\n          \"phase\": \"analysis\",\n          \"input_key\": \"test_mask_data\",\n          \"output_key\": \"calculated_areas\",\n          \"parameters\": {}\n        }\n      ]\n    }\n  }\n]",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Area"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_curvature.html",
    "href": "user_guide/data_transformations/line_curvature.html",
    "title": "Line Curvature",
    "section": "",
    "text": "This transform calculates the curvature of a 2D line at a specified point. This is useful for analyzing the shape and tortuosity of lines, such as animal trajectories or neuronal processes.\n\n\nThe Line Curvature transform measures how much a line bends at a given point. It does this by fitting a parametric polynomial to the line’s (x, y) coordinates and then calculating the curvature of the fitted polynomial at a user-specified position along the line’s length.\nThe transform takes a LineData object (representing one or more lines over time) as input and produces an AnalogTimeSeries as output, where each value represents the calculated curvature at the corresponding time point.\nThis method allows for a smooth and robust estimation of curvature even when the underlying line data is noisy or unevenly sampled.\n\n\n\n\nIn neuroscience, analyzing the geometry of paths and structures is crucial for understanding behavior and neural architecture:\n\nAnimal Trajectory Analysis: The curvature of an animal’s path (e.g., a rat in a maze) can reveal changes in exploratory strategy, searching behavior, or the effects of neurological manipulations. High curvature might indicate turning or searching, while low curvature suggests straight-line travel.\nNeuronal Arborization: When analyzing the structure of neurons from microscope images, the curvature of dendrites or axons can be quantified to characterize their branching patterns and complexity. This is important for studying neuronal development, plasticity, and disease.\nAnalysis of Saccadic Eye Movements: The trajectory of saccadic eye movements is not perfectly straight. Analyzing the curvature of these paths can provide insights into the underlying motor control mechanisms.",
    "crumbs": [
      "Data Transformations",
      "Line Curvature"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_curvature.html#overview",
    "href": "user_guide/data_transformations/line_curvature.html#overview",
    "title": "Line Curvature",
    "section": "",
    "text": "This transform calculates the curvature of a 2D line at a specified point. This is useful for analyzing the shape and tortuosity of lines, such as animal trajectories or neuronal processes.\n\n\nThe Line Curvature transform measures how much a line bends at a given point. It does this by fitting a parametric polynomial to the line’s (x, y) coordinates and then calculating the curvature of the fitted polynomial at a user-specified position along the line’s length.\nThe transform takes a LineData object (representing one or more lines over time) as input and produces an AnalogTimeSeries as output, where each value represents the calculated curvature at the corresponding time point.\nThis method allows for a smooth and robust estimation of curvature even when the underlying line data is noisy or unevenly sampled.\n\n\n\n\nIn neuroscience, analyzing the geometry of paths and structures is crucial for understanding behavior and neural architecture:\n\nAnimal Trajectory Analysis: The curvature of an animal’s path (e.g., a rat in a maze) can reveal changes in exploratory strategy, searching behavior, or the effects of neurological manipulations. High curvature might indicate turning or searching, while low curvature suggests straight-line travel.\nNeuronal Arborization: When analyzing the structure of neurons from microscope images, the curvature of dendrites or axons can be quantified to characterize their branching patterns and complexity. This is important for studying neuronal development, plasticity, and disease.\nAnalysis of Saccadic Eye Movements: The trajectory of saccadic eye movements is not perfectly straight. Analyzing the curvature of these paths can provide insights into the underlying motor control mechanisms.",
    "crumbs": [
      "Data Transformations",
      "Line Curvature"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_curvature.html#parameters",
    "href": "user_guide/data_transformations/line_curvature.html#parameters",
    "title": "Line Curvature",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nposition: A fractional value between 0.0 and 1.0 indicating the point along the line’s total length at which to calculate the curvature. 0.0 is the start of the line, 0.5 is the midpoint, and 1.0 is the end.\nmethod: The algorithm to use for calculating curvature. Currently, only one option is available:\n\nPolynomialFit: Fits a parametric polynomial to the line data and calculates the analytical curvature.\n\npolynomial_order: The order of the polynomial to fit to the line data. A higher order can capture more complex curves but may be more sensitive to noise. A minimum of 2 is required for curvature calculation.\nfitting_window_percentage: The percentage (from 0.0 to 1.0) of the total line length to use for the polynomial fit, centered on the position parameter. A smaller window focuses the curvature calculation on the local shape, while a larger window provides a more global measure.",
    "crumbs": [
      "Data Transformations",
      "Line Curvature"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_curvature.html#example-configuration",
    "href": "user_guide/data_transformations/line_curvature.html#example-configuration",
    "title": "Line Curvature",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example calculates the curvature at the midpoint (0.5) of a line, using a 3rd-order polynomial fitted to 10% of the line’s data.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Line Curvature Pipeline\",\n            \"description\": \"Test line curvature calculation on a curved line\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Calculate Line Curvature\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"my_line_data\",\n                \"output_key\": \"calculated_curvature\",\n                \"parameters\": {\n                    \"position\": 0.5,\n                    \"method\": \"PolynomialFit\",\n                    \"polynomial_order\": 3,\n                    \"fitting_window_percentage\": 0.1\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Line Curvature"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_angle.html",
    "href": "user_guide/data_transformations/line_angle.html",
    "title": "Line Angle",
    "section": "",
    "text": "This transform calculates the angle of a line at a specific position along its length for each point in time.\n\n\nThe Line Angle transform provides a way to quantify the orientation of a line in a 2D space. This can be particularly useful for analyzing the movement or orientation of objects that are tracked as lines. The angle is measured relative to a customizable reference vector, which defaults to the positive x-axis (0 degrees).\nThere are two methods for calculating the angle:\n\nDirect Points: This method approximates the tangent at a given position by calculating the angle of the vector between the point at that position and the point immediately preceding it. This is a simple and fast method suitable for getting a local orientation.\nPolynomial Fit: This method fits a polynomial of a given order to the line’s points (parameterized by their cumulative distance). The angle is then calculated from the derivative of the polynomial at the specified position. This method can provide a more accurate estimate of the tangent angle at a specific point along a curved line.\n\n\n\n\n\nIn neuroscience, this transform can be used to analyze a variety of data:\n\nWhisker Tracking: The angle of a tracked whisker relative to the animal’s head can be calculated to study sensory input and motor control.\nLimb Tracking: The angle of a limb segment (e.g., the forearm) can be calculated to analyze reaching movements or other motor behaviors.\nTongue Tracking: The angle of the tongue during licking or other oral movements can be quantified.",
    "crumbs": [
      "Data Transformations",
      "Line Angle"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_angle.html#overview",
    "href": "user_guide/data_transformations/line_angle.html#overview",
    "title": "Line Angle",
    "section": "",
    "text": "This transform calculates the angle of a line at a specific position along its length for each point in time.\n\n\nThe Line Angle transform provides a way to quantify the orientation of a line in a 2D space. This can be particularly useful for analyzing the movement or orientation of objects that are tracked as lines. The angle is measured relative to a customizable reference vector, which defaults to the positive x-axis (0 degrees).\nThere are two methods for calculating the angle:\n\nDirect Points: This method approximates the tangent at a given position by calculating the angle of the vector between the point at that position and the point immediately preceding it. This is a simple and fast method suitable for getting a local orientation.\nPolynomial Fit: This method fits a polynomial of a given order to the line’s points (parameterized by their cumulative distance). The angle is then calculated from the derivative of the polynomial at the specified position. This method can provide a more accurate estimate of the tangent angle at a specific point along a curved line.\n\n\n\n\n\nIn neuroscience, this transform can be used to analyze a variety of data:\n\nWhisker Tracking: The angle of a tracked whisker relative to the animal’s head can be calculated to study sensory input and motor control.\nLimb Tracking: The angle of a limb segment (e.g., the forearm) can be calculated to analyze reaching movements or other motor behaviors.\nTongue Tracking: The angle of the tongue during licking or other oral movements can be quantified.",
    "crumbs": [
      "Data Transformations",
      "Line Angle"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_angle.html#parameters",
    "href": "user_guide/data_transformations/line_angle.html#parameters",
    "title": "Line Angle",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nposition: A value between 0.0 and 1.0 that specifies the point along the line at which to calculate the angle. For example, 0.5 would be the midpoint of the line.\nmethod: The calculation method to use.\n\nDirect Points: (Default) Calculates the angle based on the vector from the start of the line to the point at position.\nPolynomial Fit: Fits a polynomial to the line and calculates the angle from the derivative.\n\npolynomial_order: The order of the polynomial to fit to the line when using the Polynomial Fit method. A higher order can capture more complex curves but requires more points and can be prone to overfitting. A typical value is 2 or 3.\nreference_x: The x-component of the reference vector. Defaults to 1.0.\nreference_y: The y-component of the reference vector. Defaults to 0.0. The default vector (1.0, 0.0) corresponds to the positive x-axis, meaning angles are measured with 0 degrees pointing to the right.",
    "crumbs": [
      "Data Transformations",
      "Line Angle"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_angle.html#example-configuration",
    "href": "user_guide/data_transformations/line_angle.html#example-configuration",
    "title": "Line Angle",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example calculates the angle at the midpoint of a line using the “Direct Points” method.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Line Angle Pipeline\",\n            \"description\": \"Test line angle calculation on line data\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Calculate Line Angle\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_line\",\n                \"output_key\": \"line_angles\",\n                \"parameters\": {\n                    \"position\": 0.5,\n                    \"method\": \"Direct Points\",\n                    \"polynomial_order\": 3,\n                    \"reference_x\": 1.0,\n                    \"reference_y\": 0.0\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Line Angle"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_line_subsegment.html",
    "href": "user_guide/data_transformations/extract_line_subsegment.html",
    "title": "Extract Line Subsegment",
    "section": "",
    "text": "This transform extracts a portion of a line (a subsegment) based on specified start and end points.\n\n\nThis transformation allows for the precise extraction of a subsegment from a series of lines, such as those representing the movement of a tracked object over time. You can define the start and end of the subsegment as a fraction of the total line length. For example, you can extract the middle 50% of a line.\nThis is particularly useful for focusing analysis on a specific part of a trajectory or shape. For instance, in neuroscience, you might use this to analyze the movement of an animal’s limb, focusing only on the part of the movement where the limb is closest to a target.\n\nThe extraction can be done in two ways:\n\nDirect: This method pulls the original points from the line that fall within the specified subsegment. This is fast and preserves the original data points.\nParametric: This method fits a polynomial to the line and then generates a new, smooth subsegment with a specified number of points. This is useful for standardizing the length and spacing of the subsegment for further analysis, such as comparing shapes across different trials.\n\n\n\n\n\nWhisker Tracking: When analyzing whisker movements, you might want to extract only the portion of the whisker closest to an object it’s exploring.\nLimb Tracking: In studies of motor control, you could extract the segment of a limb’s trajectory during a specific phase of a reach-to-grasp movement.\nNeural Arborization: When analyzing the structure of neurons, this transform could be used to isolate specific dendritic or axonal segments for morphological analysis.",
    "crumbs": [
      "Data Transformations",
      "Extract Line Subsegment"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_line_subsegment.html#overview",
    "href": "user_guide/data_transformations/extract_line_subsegment.html#overview",
    "title": "Extract Line Subsegment",
    "section": "",
    "text": "This transform extracts a portion of a line (a subsegment) based on specified start and end points.\n\n\nThis transformation allows for the precise extraction of a subsegment from a series of lines, such as those representing the movement of a tracked object over time. You can define the start and end of the subsegment as a fraction of the total line length. For example, you can extract the middle 50% of a line.\nThis is particularly useful for focusing analysis on a specific part of a trajectory or shape. For instance, in neuroscience, you might use this to analyze the movement of an animal’s limb, focusing only on the part of the movement where the limb is closest to a target.\n\nThe extraction can be done in two ways:\n\nDirect: This method pulls the original points from the line that fall within the specified subsegment. This is fast and preserves the original data points.\nParametric: This method fits a polynomial to the line and then generates a new, smooth subsegment with a specified number of points. This is useful for standardizing the length and spacing of the subsegment for further analysis, such as comparing shapes across different trials.\n\n\n\n\n\nWhisker Tracking: When analyzing whisker movements, you might want to extract only the portion of the whisker closest to an object it’s exploring.\nLimb Tracking: In studies of motor control, you could extract the segment of a limb’s trajectory during a specific phase of a reach-to-grasp movement.\nNeural Arborization: When analyzing the structure of neurons, this transform could be used to isolate specific dendritic or axonal segments for morphological analysis.",
    "crumbs": [
      "Data Transformations",
      "Extract Line Subsegment"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_line_subsegment.html#parameters",
    "href": "user_guide/data_transformations/extract_line_subsegment.html#parameters",
    "title": "Extract Line Subsegment",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nstart_position: The starting point of the subsegment, as a fraction of the total line length (from 0.0 to 1.0).\nend_position: The ending point of the subsegment, as a fraction of the total line length (from 0.0 to 1.0).\nmethod: The extraction method to use. This can be:\n\nDirect: Directly extracts the original points within the subsegment range.\nParametric: Uses polynomial interpolation to create a new, smooth subsegment.\n\npolynomial_order: (Parametric method only) The order of the polynomial to fit to the line. A higher order can capture more complex curves but may also be more sensitive to noise.\noutput_points: (Parametric method only) The number of points to generate in the output subsegment.\npreserve_original_spacing: (Direct method only) If true, the subsegment will consist of the original points from the line that fall within the specified range. If false, the subsegment will be resampled to have evenly spaced points.",
    "crumbs": [
      "Data Transformations",
      "Extract Line Subsegment"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_line_subsegment.html#example-configuration",
    "href": "user_guide/data_transformations/extract_line_subsegment.html#example-configuration",
    "title": "Extract Line Subsegment",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example extracts the subsegment from 20% to 80% of the line’s length using the Direct method.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Line Subsegment Extraction Pipeline\",\n            \"description\": \"Test line subsegment extraction\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Extract Line Subsegment\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_line\",\n                \"output_key\": \"extracted_subsegments\",\n                \"parameters\": {\n                    \"start_position\": 0.2,\n                    \"end_position\": 0.8,\n                    \"method\": \"Direct\",\n                    \"preserve_original_spacing\": true\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Extract Line Subsegment"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_line_to_point_distance.html",
    "href": "user_guide/data_transformations/calculate_line_to_point_distance.html",
    "title": "Calculate Line to Point Distance",
    "section": "",
    "text": "This transform calculates the minimum distance from a set of points to a line for each moment in time, which is useful for quantifying the spatial relationship between different tracked objects.\n\n\nThis operation computes the shortest Euclidean distance between a line (composed of connected segments) and one or more points at each corresponding timestamp. If multiple points are present at a single timestamp, the transform identifies the minimum distance among all points to the line. The calculation finds the closest point on any segment of the line to each of the provided points and returns the smallest distance found.\nIf the line and points are associated with different spatial scales (i.e., different image sizes), the points will be automatically scaled to the line’s coordinate system before the distance is computed. The output is an analog time series where the value at each time point is the calculated minimum distance.\n\n\n\n\nThis transformation is valuable for analyzing the interaction between different elements in behavioral or physiological experiments:\n\nWhisker Tracking: In studies of rodent behavior, one might track the position of a whisker (as a line) and the location of an object (as a point). This transform can precisely measure the distance from the whisker to the object at each frame of a video, helping to identify moments of contact or near-contact.\nLimb and Body Coordination: When tracking the movement of an animal’s limb (represented as a line) relative to a target or another body part (represented as a point), this transform can quantify their spatial relationship over time. This is useful for studies of motor control and coordination.\nNeural Prosthetics and Brain-Computer Interfaces: In experiments where an animal controls a cursor or robotic arm, this transform could be used to measure the distance from the effector (the line) to a target (the point), providing a continuous measure of performance.",
    "crumbs": [
      "Data Transformations",
      "Calculate Line to Point Distance"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_line_to_point_distance.html#overview",
    "href": "user_guide/data_transformations/calculate_line_to_point_distance.html#overview",
    "title": "Calculate Line to Point Distance",
    "section": "",
    "text": "This transform calculates the minimum distance from a set of points to a line for each moment in time, which is useful for quantifying the spatial relationship between different tracked objects.\n\n\nThis operation computes the shortest Euclidean distance between a line (composed of connected segments) and one or more points at each corresponding timestamp. If multiple points are present at a single timestamp, the transform identifies the minimum distance among all points to the line. The calculation finds the closest point on any segment of the line to each of the provided points and returns the smallest distance found.\nIf the line and points are associated with different spatial scales (i.e., different image sizes), the points will be automatically scaled to the line’s coordinate system before the distance is computed. The output is an analog time series where the value at each time point is the calculated minimum distance.\n\n\n\n\nThis transformation is valuable for analyzing the interaction between different elements in behavioral or physiological experiments:\n\nWhisker Tracking: In studies of rodent behavior, one might track the position of a whisker (as a line) and the location of an object (as a point). This transform can precisely measure the distance from the whisker to the object at each frame of a video, helping to identify moments of contact or near-contact.\nLimb and Body Coordination: When tracking the movement of an animal’s limb (represented as a line) relative to a target or another body part (represented as a point), this transform can quantify their spatial relationship over time. This is useful for studies of motor control and coordination.\nNeural Prosthetics and Brain-Computer Interfaces: In experiments where an animal controls a cursor or robotic arm, this transform could be used to measure the distance from the effector (the line) to a target (the point), providing a continuous measure of performance.",
    "crumbs": [
      "Data Transformations",
      "Calculate Line to Point Distance"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_line_to_point_distance.html#parameters",
    "href": "user_guide/data_transformations/calculate_line_to_point_distance.html#parameters",
    "title": "Calculate Line to Point Distance",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameter:\n\npoint_data: The key of the PointData object in the DataManager. This object contains the set of points from which the minimum distance to the line will be calculated at each timestamp.",
    "crumbs": [
      "Data Transformations",
      "Calculate Line to Point Distance"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_line_to_point_distance.html#example-configuration",
    "href": "user_guide/data_transformations/calculate_line_to_point_distance.html#example-configuration",
    "title": "Calculate Line to Point Distance",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example calculates the distance between a line stored with the key test_line and a set of points stored with the key test_points.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Line to Point Distance Pipeline\",\n            \"description\": \"Test line to point minimum distance calculation\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Calculate Line to Point Distance\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_line\",\n                \"output_key\": \"line_point_distances\",\n                \"parameters\": {\n                    \"point_data\": \"test_points\"\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Calculate Line to Point Distance"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html",
    "title": "Analog Hilbert Phase",
    "section": "",
    "text": "This transform calculates the instantaneous phase of an analog signal, which is useful for analyzing oscillations.\n\n\nThe Hilbert transform is a mathematical operation that shifts the phase of all positive frequency components of a signal by -90 degrees and all negative frequency components by +90 degrees. When this phase-shifted signal (the imaginary part) is combined with the original signal (the real part), it forms a complex-valued “analytic signal”.\nThe instantaneous phase is the angle of this complex number at each point in time. It provides a way to represent an oscillating signal in terms of its phase, which progresses from -π to +π for each cycle of the oscillation. This transform uses an efficient FFT-based method to compute the Hilbert transform.\n\n\n\n\n\nIn neuroscience, analyzing the phase of neural signals is crucial for understanding brain function. Some common applications include:\n\nRhythmic Behaviors: The phase of signals from sensors tracking rhythmic behaviors like whisking, sniffing, or licking can be extracted to correlate them with neural activity.",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html#overview",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html#overview",
    "title": "Analog Hilbert Phase",
    "section": "",
    "text": "This transform calculates the instantaneous phase of an analog signal, which is useful for analyzing oscillations.\n\n\nThe Hilbert transform is a mathematical operation that shifts the phase of all positive frequency components of a signal by -90 degrees and all negative frequency components by +90 degrees. When this phase-shifted signal (the imaginary part) is combined with the original signal (the real part), it forms a complex-valued “analytic signal”.\nThe instantaneous phase is the angle of this complex number at each point in time. It provides a way to represent an oscillating signal in terms of its phase, which progresses from -π to +π for each cycle of the oscillation. This transform uses an efficient FFT-based method to compute the Hilbert transform.\n\n\n\n\n\nIn neuroscience, analyzing the phase of neural signals is crucial for understanding brain function. Some common applications include:\n\nRhythmic Behaviors: The phase of signals from sensors tracking rhythmic behaviors like whisking, sniffing, or licking can be extracted to correlate them with neural activity.",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html#parameters",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html#parameters",
    "title": "Analog Hilbert Phase",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nlow_frequency: The low-cut frequency for the bandpass filter, in Hertz. This determines the lower end of the frequency range you want to analyze.\nhigh_frequency: The high-cut frequency for the bandpass filter, in Hertz. This determines the upper end of the frequency range you want to analyze.\ndiscontinuity_threshold: A time gap, in samples, above which the signal is considered to have a break. The transform will process the continuous segments separately. This is useful for data with missing samples or interruptions.",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html#example-configuration",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html#example-configuration",
    "title": "Analog Hilbert Phase",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that loads data and applies the Analog Hilbert Phase transform.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Hilbert Phase Pipeline\",\n            \"description\": \"Test Hilbert phase calculation on analog signal\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Hilbert Phase\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_signal\",\n                \"output_key\": \"phase_signal\",\n                \"parameters\": {\n                    \"low_frequency\": 5.0,\n                    \"high_frequency\": 15.0,\n                    \"discontinuity_threshold\": 1000\n                }\n            }\n        ]\n    }\n}\n]\n\nReferences\nHill, D.N., Curtis, J.C., Moore, J.D., Kleinfeld, D., 2011. Primary motor cortex reports efferent control of vibrissa motion on multiple timescales. Neuron 72, 344–356. https://doi.org/10.1016/j.neuron.2011.09.020",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_event_threshold.html",
    "href": "user_guide/data_transformations/analog_event_threshold.html",
    "title": "Analog Event Threshold",
    "section": "",
    "text": "This transform detects moments in time (events) when an analog signal crosses a specified value, which is useful for identifying significant occurrences in the data.\n\n\nThresholding is a fundamental method for event detection in time-series data. This operation identifies every point in time where the signal’s amplitude crosses a defined threshold in a specific direction (either rising above it, falling below it, or crossing it in either direction).\nA “lockout” period can also be set. After an event is detected, the algorithm will ignore any further threshold crossings for the duration of this period, which helps prevent a single, noisy event from being counted multiple times.\nThis transform takes an analog time series as input and produces a digital event series as output, where each event corresponds to a single point in time when a threshold crossing was detected.\n\n\n\n\n\nIn neuroscience, thresholding is a common technique for identifying meaningful events in continuous data streams:\n\nSpike Detection: In extracellular recordings, thresholding can be used to identify action potentials (spikes) that stand out from the background noise. A lockout time is critical here to avoid detecting the same spike multiple times as it repolarizes.\nBehavioral Event Marking: For data from sensors like accelerometers or force transducers, thresholding can mark the onset of specific behaviors, such as a mouse starting to run on a wheel or a bird pecking a key.\nCalcium Imaging Analysis: In calcium imaging data, which reflects neural activity, thresholding can be used to detect significant calcium transients that indicate a neuron or a group of neurons are firing.",
    "crumbs": [
      "Data Transformations",
      "Analog Event Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_event_threshold.html#overview",
    "href": "user_guide/data_transformations/analog_event_threshold.html#overview",
    "title": "Analog Event Threshold",
    "section": "",
    "text": "This transform detects moments in time (events) when an analog signal crosses a specified value, which is useful for identifying significant occurrences in the data.\n\n\nThresholding is a fundamental method for event detection in time-series data. This operation identifies every point in time where the signal’s amplitude crosses a defined threshold in a specific direction (either rising above it, falling below it, or crossing it in either direction).\nA “lockout” period can also be set. After an event is detected, the algorithm will ignore any further threshold crossings for the duration of this period, which helps prevent a single, noisy event from being counted multiple times.\nThis transform takes an analog time series as input and produces a digital event series as output, where each event corresponds to a single point in time when a threshold crossing was detected.\n\n\n\n\n\nIn neuroscience, thresholding is a common technique for identifying meaningful events in continuous data streams:\n\nSpike Detection: In extracellular recordings, thresholding can be used to identify action potentials (spikes) that stand out from the background noise. A lockout time is critical here to avoid detecting the same spike multiple times as it repolarizes.\nBehavioral Event Marking: For data from sensors like accelerometers or force transducers, thresholding can mark the onset of specific behaviors, such as a mouse starting to run on a wheel or a bird pecking a key.\nCalcium Imaging Analysis: In calcium imaging data, which reflects neural activity, thresholding can be used to detect significant calcium transients that indicate a neuron or a group of neurons are firing.",
    "crumbs": [
      "Data Transformations",
      "Analog Event Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_event_threshold.html#parameters",
    "href": "user_guide/data_transformations/analog_event_threshold.html#parameters",
    "title": "Analog Event Threshold",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nthreshold_value: The amplitude value the signal must cross to be considered an event.\ndirection: Specifies the direction of the crossing required to trigger an event. This can be:\n\nPositive (Rising): An event is detected when the signal crosses the threshold from a lower to a higher value.\nNegative (Falling): An event is detected when the signal crosses the threshold from a higher to a lower value.\nAbsolute: An event is detected whenever the absolute value of the signal crosses the threshold value (useful for detecting deviations from baseline in either direction).\n\nlockout_time: A duration (in the same time units as the data, e.g., seconds) after each detected event during which no new events will be registered. This is useful for preventing multiple detections of a single, noisy event.",
    "crumbs": [
      "Data Transformations",
      "Analog Event Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_event_threshold.html#example-configuration",
    "href": "user_guide/data_transformations/analog_event_threshold.html#example-configuration",
    "title": "Analog Event Threshold",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example detects events where the signal rises above an amplitude of 1.0, with no lockout period.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Threshold Detection Pipeline\",\n            \"description\": \"Test threshold event detection on analog signal\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Analog Event Threshold\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_signal\",\n                \"output_key\": \"detected_events\",\n                \"parameters\": {\n                    \"threshold_value\": 1.0,\n                    \"direction\": \"Positive\",\n                    \"lockout_time\": 0.0\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Analog Event Threshold"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html",
    "href": "user_guide/behaviors/tongue.html",
    "title": "Tongue Tracking",
    "section": "",
    "text": "The Tongue Tracking widget deals with operations related to the tongue.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html#loading",
    "href": "user_guide/behaviors/tongue.html#loading",
    "title": "Tongue Tracking",
    "section": "Loading",
    "text": "Loading\nTongue masks can be loaded through the sparse HDF5 format or binary images (where white is part of the mask, black is not).\nJaw keypoint tracking can also be loaded through CSV format. The first column should indicate frame number, the next indicating \\(x\\) position and the next \\(y\\) position.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html#grabcut-tool",
    "href": "user_guide/behaviors/tongue.html#grabcut-tool",
    "title": "Tongue Tracking",
    "section": "GrabCut Tool",
    "text": "GrabCut Tool\nDocumentation on the GrabCut tool can be found here.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "examples/keypoint_labeling.html",
    "href": "examples/keypoint_labeling.html",
    "title": "Keypoint Video Labeling",
    "section": "",
    "text": "Keypoint labeling is a powerful method to systematically study motion, behavior, and anatomical positioning over time. By marking and tracking specific locations on subjects across frames in a video, you can extract detailed data about the dynamics of movement.\nFor example, keypoint data can be used to:\nThis tutorial will guide you through the process of labeling keypoints on a video and saving the keypoint data."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-1-load-video-or-existing-labels",
    "href": "examples/keypoint_labeling.html#step-1-load-video-or-existing-labels",
    "title": "Keypoint Video Labeling",
    "section": "Step 1: Load Video or Existing Labels",
    "text": "Step 1: Load Video or Existing Labels\n\n\n\n\n\nTo begin labeling:\n\nOpen the application and navigate to File &gt; Load Data.\nSelect the video you intend to annotate.\n\nAlternatively, if you’ve already created keypoints and want to review or update them:\n\nSelect File &gt; Load JSON Configuration and choose your .json file.\n\n\n\n\n\n\n\nTip\n\n\n\nSee: Setting up a JSON File for more on configuration options."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-2-open-the-data-manager",
    "href": "examples/keypoint_labeling.html#step-2-open-the-data-manager",
    "title": "Keypoint Video Labeling",
    "section": "Step 2: Open the Data Manager",
    "text": "Step 2: Open the Data Manager\n\n\n\n\n\nTo create or manage point data:\n\nNavigate to Modules &gt; Data Manager to open the Data Manager Module."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-3-create-a-new-keypoint-feature",
    "href": "examples/keypoint_labeling.html#step-3-create-a-new-keypoint-feature",
    "title": "Keypoint Video Labeling",
    "section": "Step 3: Create a New Keypoint Feature",
    "text": "Step 3: Create a New Keypoint Feature\n\n\n\n\n\nIn the Data Manager:\n\nSet the Output Directory: This is where your labeled data (CSV) will be saved.\n\nExample: C:/Users/wanglab/Desktop/Cartoon_Mouse\n\nCreate New Data:\n\nType: point\nTime Frame: Select based on your analysis requirements (e.g., time for temporal tracking).\nName: Choose a descriptive name (e.g., jaw, nose, or paw).\n\n\nClick Create New Data to add the new feature."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-4-edit-and-add-keypoints-in-the-media-widget",
    "href": "examples/keypoint_labeling.html#step-4-edit-and-add-keypoints-in-the-media-widget",
    "title": "Keypoint Video Labeling",
    "section": "Step 4: Edit and Add Keypoints in the Media Widget",
    "text": "Step 4: Edit and Add Keypoints in the Media Widget\n\n\n\n\n\nOnce the point feature is created:\n\nEnable the Feature: Select the feature in the media widget to activate the keypoint editor.\nCustomize Appearance:\n\nChoose a color using the hex selector.\nAdjust the opacity (Alpha) for better contrast against the video.\nModify the size and shape of the marker if needed.\n\nBegin Labeling:\n\nSwitch mouse mode to “Select Point”.\nClick anywhere in the video frame to add a point.\nClicking on an existing point will move it.\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote: The keypoint is recorded as a single-pixel coordinate. Visual shape and style are only for display and are not saved."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-5-review-and-export-your-labels",
    "href": "examples/keypoint_labeling.html#step-5-review-and-export-your-labels",
    "title": "Keypoint Video Labeling",
    "section": "Step 5: Review and Export Your Labels",
    "text": "Step 5: Review and Export Your Labels\n\n\n\n\n\nAfter labeling:\n\nSelect your feature (e.g., jaw) in the Data Manager.\nYou’ll see a spreadsheet with:\n\nFrame indices\n(x, y) coordinates for each labeled keypoint\n\n\n\nExport Options:\n\nFile Name: Choose a meaningful name for your CSV.\nDelimiter: (e.g., comma , or tab \\t) – separates columns.\nLine Ending: Select line break style (\\n, \\r\\n, etc.).\nHeader: Optionally include a header row with custom labels."
  },
  {
    "objectID": "examples/keypoint_labeling.html#optional-export-matching-media-frames",
    "href": "examples/keypoint_labeling.html#optional-export-matching-media-frames",
    "title": "Keypoint Video Labeling",
    "section": "Optional: Export Matching Media Frames",
    "text": "Optional: Export Matching Media Frames\n\n\n\n\n\nYou can export the video frames where keypoints were labeled:\n\nCheck Export Matching Media Frames.\nChoose from export options:\n\nSave by Frame Name\nFrame ID Padding\nImage Name Prefix\nSubfolder for Images\nOverwrite Existing Files\n\n\n\n\n\n\n\n\nWarning\n\n\n\nKeypoints will not be drawn on these frames. Only raw frames are exported."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-6-save-to-csv",
    "href": "examples/keypoint_labeling.html#step-6-save-to-csv",
    "title": "Keypoint Video Labeling",
    "section": "Step 6: Save to CSV",
    "text": "Step 6: Save to CSV\n\n\n\n\n\nOnce you’ve reviewed your labels:\n\nClick “Save to CSV”\n\nA confirmation will appear showing the file path.\nYour labeled keypoint data is now saved and ready for further analysis.\n\n\nSummary\nThis tutorial demonstrated how to:\n\nLoad a video or keypoint configuration\nCreate and configure keypoint data\nAnnotate frames with feature coordinates\nExport label data and relevant frames"
  },
  {
    "objectID": "developer/table_view.html",
    "href": "developer/table_view.html",
    "title": "Table View",
    "section": "",
    "text": "The TableView class is able to take heterogeneous data from the Data Manager and structure it like a spreadsheet.\n\n\nThe columns may represent simple double or floating-point values, but also could take the form of different types like Booleans, counting numbers, or even arrays of values. Columns also may be data that is not directly in the Data Manager, but come as the result of some processing similar to the transformation infrastructure in the Data Manager. Some of these transformations may be redundant. I imagine transformations done for the TableView may be out of convenience or for data that you don’t necessarily care about saving as its own type, but just need for some additional processing.\nThe transformations done on data for a column need not necessarily come from the Data Manager, but also could come from other columns in the TableView. For instance, if we compute some kind of transformation and hold it in a column, we could then choose to z-score it.\n\n\n\nThe other difference from transformations comes from the concept of a row in the TableView structure. A row in the TableView could be a timestamp, just like how data is stored in the Manager, but also could be something that is an aggregator. For instance, a digital interview interval in the Data Manager can serve as a row. In this scheme, a double value in a column may not correspond to some value at an instant time, but rather one that is the average over an interval that corresponds to the row. It also could be the minimum value, the maximum value, or it could represent something like a standard deviation.\n\n\n\nTo create a TableView, the user must define what will make up the rows and what will make up the columns.\nRows are defined by selector objects that currently can be supported by intervals or timestamps. Note that these timestamps could correspond to the different timeframes in the Data Manager class.\nColumns are created by different computer objects. These computer objects work on different data sources that are placed into the TableView, and usually perform some kind of transformation. These data sources are things like analog sources, event sources, or interval sources. Adapter objects exist in the TableView to allow types in the Data Manager to be converted to these forms. Some of these transformations are straightforward. For instance, an analog source can be simply converted into a column of doubles. But something like point data would be broken up into two columns of x and y values for each timestamp.\nThe computers for a column that are available depend on the input data type as well as what row selector is being used (for example, intervals versus timestamps). A factory and registry is available to list the types of computers available for these combinations and can output the computer object that is available from the factory.\nWith the rows, analog sources defined, and computers, a builder object can be used to create an actual TableView.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#columns",
    "href": "developer/table_view.html#columns",
    "title": "Table View",
    "section": "",
    "text": "The columns may represent simple double or floating-point values, but also could take the form of different types like Booleans, counting numbers, or even arrays of values. Columns also may be data that is not directly in the Data Manager, but come as the result of some processing similar to the transformation infrastructure in the Data Manager. Some of these transformations may be redundant. I imagine transformations done for the TableView may be out of convenience or for data that you don’t necessarily care about saving as its own type, but just need for some additional processing.\nThe transformations done on data for a column need not necessarily come from the Data Manager, but also could come from other columns in the TableView. For instance, if we compute some kind of transformation and hold it in a column, we could then choose to z-score it.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#rows",
    "href": "developer/table_view.html#rows",
    "title": "Table View",
    "section": "",
    "text": "The other difference from transformations comes from the concept of a row in the TableView structure. A row in the TableView could be a timestamp, just like how data is stored in the Manager, but also could be something that is an aggregator. For instance, a digital interview interval in the Data Manager can serve as a row. In this scheme, a double value in a column may not correspond to some value at an instant time, but rather one that is the average over an interval that corresponds to the row. It also could be the minimum value, the maximum value, or it could represent something like a standard deviation.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#constructing-a-tableview",
    "href": "developer/table_view.html#constructing-a-tableview",
    "title": "Table View",
    "section": "",
    "text": "To create a TableView, the user must define what will make up the rows and what will make up the columns.\nRows are defined by selector objects that currently can be supported by intervals or timestamps. Note that these timestamps could correspond to the different timeframes in the Data Manager class.\nColumns are created by different computer objects. These computer objects work on different data sources that are placed into the TableView, and usually perform some kind of transformation. These data sources are things like analog sources, event sources, or interval sources. Adapter objects exist in the TableView to allow types in the Data Manager to be converted to these forms. Some of these transformations are straightforward. For instance, an analog source can be simply converted into a column of doubles. But something like point data would be broken up into two columns of x and y values for each timestamp.\nThe computers for a column that are available depend on the input data type as well as what row selector is being used (for example, intervals versus timestamps). A factory and registry is available to list the types of computers available for these combinations and can output the computer object that is available from the factory.\nWith the rows, analog sources defined, and computers, a builder object can be used to create an actual TableView.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#csv-export",
    "href": "developer/table_view.html#csv-export",
    "title": "Table View",
    "section": "CSV Export",
    "text": "CSV Export\nThe first, and most straightforward, use case is simple CSV export. For instance, imagine an experiment where an animal is licking. The intervals are calculated when licks occur. We would like to have some output for each lick where we are aware of the start time of the lick, the end time of the lick, and the duration. Within each of these licks, we would like to know the maximum surface area of the tongue that is stored in an analog data source at each timestamp from a tracked video.\nWe may also wish to know if some other digital events are co-occurring. For instance, licks are often organized in sequences of so-called bouts, so there are multiple intervals of licks within a single bout. That digital interval may be contained within the data manager. We can include that as a column, or each lick can be identified by an integer that represents which bout it belongs to. So there may be three licks within the first bout that all have an ID of 0, then licks four and five have a bout ID of 1, et cetera.\nThis can also be a convenient interface to get time conversion information from data that is in a different timeframe. For instance, imagine if we are also considering the ID of a laser digital interval that is coincident with a lick. This laser may be acquired in a different timeframe with a different sampling rate. In addition to getting the ID of the coincident laser interval, we can also get its start time in units of camera frames, like the lick. In this way, we’ll be finding what is the closest camera frame to the beginning of the laser start time that was around this lick. This can be useful for aligning to different event types.\nThis data can be serialized to a CSV. In this way, we can also include data that would be array-like, like gathering all of the events within some interval. This could be useful for getting, say, the spike times that occur within some meaningful experimental time block.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#analysis-dashboard",
    "href": "developer/table_view.html#analysis-dashboard",
    "title": "Table View",
    "section": "Analysis Dashboard",
    "text": "Analysis Dashboard\nThe next use of the TableView is in the analysis dashboard. This is one of the main plotting interfaces for Neuralyzer. In this widget, we are able to plot different kinds of data that are in the data manager. The TableView then allows us to also plot different transformations of data that is in the data manager. For instance, if we wanted to visualize a raster plot from a spiking neuron, we could use the TableView to use a row selector that is an interval around some event. Let’s imagine aligning to lick onset. We could pick one second on either side of the lick onset and call that an interval, and then gather the event times for each of those. This would give us a data structure that is essentially an array of arrays, and we can also normalize these to that event time. This gives us the exact data structure necessary for creating a raster plot.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#feature-extraction-and-dimensionality-reduction",
    "href": "developer/table_view.html#feature-extraction-and-dimensionality-reduction",
    "title": "Table View",
    "section": "Feature Extraction and Dimensionality Reduction",
    "text": "Feature Extraction and Dimensionality Reduction\nThe TableView architecture also provides a convenient interface to get something that looks like a 2D array. Imagine if we wanted to try to figure out features of a two-dimensional line to determine its identity. We could calculate with computers to make columns for the curvature of the line, and perhaps we would want to get the x and y positions at different fractional line lengths (e.g., where is the x position at 20% along the line, 40% along the line, 60% along the line, etc.). Then we could also include the angle of the line, the follicular position, etc.\nI probably don’t want to have to go through the data transform widget and extract all of these and keep them hanging around, because what I really would want is to have a 2D array and then perform some kind of dimensionality reduction on it. Then, with those arbitrary features, I might be able to look for clusters that ideally would correspond to the same whisker, compared to multiple whiskers that would have differences in the first or second principal component.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#machine-learning-interface",
    "href": "developer/table_view.html#machine-learning-interface",
    "title": "Table View",
    "section": "Machine Learning Interface",
    "text": "Machine Learning Interface\nSimilarly, the TableView provides a convenient interface to try to gather data to feed into some kind of machine learning algorithm. We can use the TableView to get our feature matrix for training some kind of model. In that case, if we are trying to classify specific frames of a movie that correspond to some behavior, we could use time as our row selector. Then we could use different calculated quantities from the video, like, say, the 2D positions of key points as the columns, and feed this to some kind of machine learning classification model if we also have accompanying labels. For instance, a label could specify that this frame corresponds to some behavior and this one does not. That could even be one of the columns of our TableView that we then separate and treat differently.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/media_widget.html",
    "href": "developer/media_widget.html",
    "title": "Media Widget",
    "section": "",
    "text": "The media widget is responsible for displaying data that can be visualized on a canvas. The types of data that can be displayed by the data manager include point data, line data, mask data, tensor data, interval data, and media data. Data currently in the data manager matching these types is displayed on the top left side. In this table, the user can click to enable the visibility of particular data in the accompanying media window canvas.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/media_widget.html#overview-and-data-display",
    "href": "developer/media_widget.html#overview-and-data-display",
    "title": "Media Widget",
    "section": "",
    "text": "The media widget is responsible for displaying data that can be visualized on a canvas. The types of data that can be displayed by the data manager include point data, line data, mask data, tensor data, interval data, and media data. Data currently in the data manager matching these types is displayed on the top left side. In this table, the user can click to enable the visibility of particular data in the accompanying media window canvas.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/media_widget.html#data-specific-interactions-and-manipulations",
    "href": "developer/media_widget.html#data-specific-interactions-and-manipulations",
    "title": "Media Widget",
    "section": "Data-Specific Interactions and Manipulations",
    "text": "Data-Specific Interactions and Manipulations\nClicking on a data item also brings up a sub-widget below the table, offering data type-specific manipulations. For example, this sub-widget would be responsible for changing attributes such as color, alpha value, marker type, etc., for point data within the media window. This sub-widget may also handle data-specific manipulations directly within the media window. For instance, the user might wish to click on a position in the media window to set it as the location for the currently displayed point. Alternatively, the user might click and hold to utilize a paintbrush-type function for extending or erasing mask data.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/media_widget.html#shared-components-and-common-interactions",
    "href": "developer/media_widget.html#shared-components-and-common-interactions",
    "title": "Media Widget",
    "section": "Shared Components and Common Interactions",
    "text": "Shared Components and Common Interactions\nThese data-specific sub-widgets may house common sub-widgets that are shared among others. For instance, color selection is a common feature for multiple data types. An interface for click, hover, and release interactions is common to multiple data types but may perform different actions, such as painting a mask or erasing the media foreground.\nNote that there is some overlap here with the data manager widget. For many data types the data manager widget includes the ability to view all data present in the data manager and select particular instances such as a point at a certain time and the user can delete that point. Consequently we will try to keep media window based manipulation to the media widget. \nThe media window is the widget that houses the actual canvas for display. It is also the owner of the specific drawing options and drawing routines onto the canvas. For instance if the user changes the marker size and color of point data the options that are updated are held by the media window. The media window uses qt-based drawing. It is also responsible for receiving mouse events inside its space and emits signals that other widgets can receive.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/Features/overview.html",
    "href": "developer/Features/overview.html",
    "title": "Features",
    "section": "",
    "text": "Documentation of some of the general widget types in Neuralyzer.",
    "crumbs": [
      "Features"
    ]
  },
  {
    "objectID": "developer/display_options.html",
    "href": "developer/display_options.html",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options.\n\n\n\n\n\n\nRange: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives\n\n\n\n\n\n\n\n\nRange: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nPosition Marker: Display a marker at a specified percentage distance along the line (off by default)\n\nshow_position_marker (bool): Enable/disable position marker display\nposition_percentage (int, 0-100%): Position along line where marker appears\n\nLine Segment: Display only a portion of the line between two percentage points (off by default)\n\nshow_segment (bool): Enable/disable segment-only display mode\nsegment_start_percentage (int, 0-100%): Start percentage for line segment\nsegment_end_percentage (int, 0-100%): End percentage for line segment\n\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nRange: 0-100% along the cumulative length of the line\nUI Controls:\n\nCheckbox to enable/disable the feature (off by default)\nHorizontal slider for quick adjustments\nSpin box for precise numeric input with % suffix\n\nDefault Value: 20% along the line\nVisual Appearance: Distinctive filled circle with white border, same color as the line\nCalculation: Based on cumulative distance along line segments, not point indices\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nPosition marker calculated using cumulative distance along line segments\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots\n\n\n\nThe segment feature allows displaying only a portion of the line between two percentage points along its cumulative distance:\n\nget_segment_between_percentages(): Utility function in lines.hpp that extracts a continuous segment\n\nCalculates cumulative distances along the original line\nPerforms linear interpolation for precise start/end points\nReturns a new Line2D containing only the specified segment\nHandles edge cases (empty lines, invalid percentages, zero-length segments)\n\nUI Validation: Start percentage cannot exceed end percentage (enforced in UI)\nRendering Logic: When enabled, replaces the full line with the extracted segment in all rendering operations\nPosition Marker Compatibility: Position markers work correctly with segments (percentage relative to segment, not original line)\n\n\n\n\nThe bounding box feature provides a visual outline around mask regions:\n\nget_bounding_box(): Utility function in masks.hpp that calculates min/max coordinates\n\nIterates through all mask points to find extrema\nReturns pair of Point2D representing opposite corners\nHandles single-point masks correctly\n\nRendering Logic: Draws unfilled rectangles using Qt’s addRect() with Qt::NoBrush\nCoordinate Scaling: Applies same aspect ratio scaling as mask data\nMultiple Masks: Each mask gets its own bounding box when feature is enabled\nTime Handling: Bounding boxes are drawn for both current time and time -1 masks\nContainer Management: Uses separate _mask_bounding_boxes container (similar to digital intervals) to avoid type conflicts\n\n\n\n\n\n\n\n\n\n\n\nstruct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n    bool show_position_marker{false};\n    int position_percentage{20};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - Position markers use get_position_at_percentage() for accurate placement - All configurations support real-time updates without data loss\n\n\n\n\n\n\n\nSelect the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\nEnable position marker to highlight specific locations along lines\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nUse get_position_at_percentage() for accurate line position calculations\nDocument new features in this file\n\n\n\n\nMask data represents 2D regions or shapes that can be overlaid on the media canvas:\n\nColor and Alpha: Configurable mask color and transparency\nBounding Box: Display rectangular outline around the mask extent (off by default)\n\nshow_bounding_box (bool): Enable/disable bounding box display\n\n\n\n\n\n\n\n\nThe mask display system includes:\n\nColor and Alpha Control: Masks can be displayed with configurable colors and transparency levels\nBounding Box: Option to display a rectangular outline around each mask showing its bounds\nOutline Drawing: Option to display the mask boundary as a thick line by connecting extremal points\n\n\n\nWhen enabled, a bounding box renders a rectangle outline around each mask. The implementation: - Uses the existing get_bounding_box() utility function from masks.hpp - Scales coordinates properly with aspect ratios - Draws unfilled rectangles using Qt::NoBrush for outline-only appearance - Handles both current time and time -1 masks\n\n\n\nWhen enabled, displays the mask boundary as a thick line connecting extremal points. The algorithm: - For each unique x coordinate, finds the maximum y value - For each unique y coordinate, finds the maximum x value\n- Collects all extremal points and sorts them by angle from centroid - Connects points to form a closed boundary outline - Renders as a thick 4-pixel wide line using QPainterPath\nThe outline feature uses the get_mask_outline() function to compute boundary points by finding extremal coordinates, providing a visual representation of the mask’s outer boundary."
  },
  {
    "objectID": "developer/display_options.html#overview",
    "href": "developer/display_options.html#overview",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options."
  },
  {
    "objectID": "developer/display_options.html#point-display-options",
    "href": "developer/display_options.html#point-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives"
  },
  {
    "objectID": "developer/display_options.html#line-display-options",
    "href": "developer/display_options.html#line-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nPosition Marker: Display a marker at a specified percentage distance along the line (off by default)\n\nshow_position_marker (bool): Enable/disable position marker display\nposition_percentage (int, 0-100%): Position along line where marker appears\n\nLine Segment: Display only a portion of the line between two percentage points (off by default)\n\nshow_segment (bool): Enable/disable segment-only display mode\nsegment_start_percentage (int, 0-100%): Start percentage for line segment\nsegment_end_percentage (int, 0-100%): End percentage for line segment\n\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nRange: 0-100% along the cumulative length of the line\nUI Controls:\n\nCheckbox to enable/disable the feature (off by default)\nHorizontal slider for quick adjustments\nSpin box for precise numeric input with % suffix\n\nDefault Value: 20% along the line\nVisual Appearance: Distinctive filled circle with white border, same color as the line\nCalculation: Based on cumulative distance along line segments, not point indices\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nPosition marker calculated using cumulative distance along line segments\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots\n\n\n\nThe segment feature allows displaying only a portion of the line between two percentage points along its cumulative distance:\n\nget_segment_between_percentages(): Utility function in lines.hpp that extracts a continuous segment\n\nCalculates cumulative distances along the original line\nPerforms linear interpolation for precise start/end points\nReturns a new Line2D containing only the specified segment\nHandles edge cases (empty lines, invalid percentages, zero-length segments)\n\nUI Validation: Start percentage cannot exceed end percentage (enforced in UI)\nRendering Logic: When enabled, replaces the full line with the extracted segment in all rendering operations\nPosition Marker Compatibility: Position markers work correctly with segments (percentage relative to segment, not original line)\n\n\n\n\nThe bounding box feature provides a visual outline around mask regions:\n\nget_bounding_box(): Utility function in masks.hpp that calculates min/max coordinates\n\nIterates through all mask points to find extrema\nReturns pair of Point2D representing opposite corners\nHandles single-point masks correctly\n\nRendering Logic: Draws unfilled rectangles using Qt’s addRect() with Qt::NoBrush\nCoordinate Scaling: Applies same aspect ratio scaling as mask data\nMultiple Masks: Each mask gets its own bounding box when feature is enabled\nTime Handling: Bounding boxes are drawn for both current time and time -1 masks\nContainer Management: Uses separate _mask_bounding_boxes container (similar to digital intervals) to avoid type conflicts"
  },
  {
    "objectID": "developer/display_options.html#technical-architecture",
    "href": "developer/display_options.html#technical-architecture",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "struct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n    bool show_position_marker{false};\n    int position_percentage{20};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - Position markers use get_position_at_percentage() for accurate placement - All configurations support real-time updates without data loss"
  },
  {
    "objectID": "developer/display_options.html#usage-guidelines",
    "href": "developer/display_options.html#usage-guidelines",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Select the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\nEnable position marker to highlight specific locations along lines\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nUse get_position_at_percentage() for accurate line position calculations\nDocument new features in this file\n\n\n\n\nMask data represents 2D regions or shapes that can be overlaid on the media canvas:\n\nColor and Alpha: Configurable mask color and transparency\nBounding Box: Display rectangular outline around the mask extent (off by default)\n\nshow_bounding_box (bool): Enable/disable bounding box display\n\n\n\n\n\n\n\n\nThe mask display system includes:\n\nColor and Alpha Control: Masks can be displayed with configurable colors and transparency levels\nBounding Box: Option to display a rectangular outline around each mask showing its bounds\nOutline Drawing: Option to display the mask boundary as a thick line by connecting extremal points\n\n\n\nWhen enabled, a bounding box renders a rectangle outline around each mask. The implementation: - Uses the existing get_bounding_box() utility function from masks.hpp - Scales coordinates properly with aspect ratios - Draws unfilled rectangles using Qt::NoBrush for outline-only appearance - Handles both current time and time -1 masks\n\n\n\nWhen enabled, displays the mask boundary as a thick line connecting extremal points. The algorithm: - For each unique x coordinate, finds the maximum y value - For each unique y coordinate, finds the maximum x value\n- Collects all extremal points and sorts them by angle from centroid - Connects points to form a closed boundary outline - Renders as a thick 4-pixel wide line using QPainterPath\nThe outline feature uses the get_mask_outline() function to compute boundary points by finding extremal coordinates, providing a visual representation of the mask’s outer boundary."
  },
  {
    "objectID": "developer/data_transform_widget.html",
    "href": "developer/data_transform_widget.html",
    "title": "Data Transform Widget",
    "section": "",
    "text": "The data transform interface is used for processing different data types. The core idea is to define various data processing operations that can be dynamically discovered and applied to different types of data, with parameters configurable through the UI. The system employs several design patterns, most notably the Strategy pattern for individual transformations and a Registry pattern for managing them, along with Factory Method for creating UI components.",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_transform_widget.html#core-components-and-workflow",
    "href": "developer/data_transform_widget.html#core-components-and-workflow",
    "title": "Data Transform Widget",
    "section": "Core Components and Workflow",
    "text": "Core Components and Workflow\nThe system revolves around a few key components:\n\nTransformOperation (Strategy Pattern): This is an abstract base class that defines the interface for all data transformation operations. Each concrete operation (e.g., EventThresholdOperation, MaskAreaOperation) inherits from TransformOperation and implements methods like getName(), getTargetInputTypeIndex(), canApply(), and execute(). This allows different algorithms (strategies) for data transformation to be used interchangeably.\nTransformParametersBase and derived structs (e.g., ThresholdParams): These structures hold the parameters for specific transformations. TransformParametersBase is a base class, and each operation can define its own derived struct (like ThresholdParams for thresholding operations) to store specific settings.\nTransformRegistry (Registry Pattern): This class acts as a central repository for all available TransformOperation instances. On initialization, it registers various concrete operation objects (e.g., MaskAreaOperation, EventThresholdOperation). It provides methods to find operations by name and to get a list of applicable operations for a given data type.\nTransformParameter_Widget (UI Abstraction): This is an abstract base class for UI widgets that allow users to set parameters for a TransformOperation. Concrete classes like AnalogEventThreshold_Widget inherit from it and provide the specific UI controls (e.g., spinboxes, comboboxes) for an operation.\nDataTransform_Widget (Main UI Controller): This Qt widget orchestrates the user interaction for data transformations.\n\nIt uses a Feature_Table_Widget to display available data items (features) from a DataManager.\nWhen a feature is selected, it queries the TransformRegistry to find applicable operations for that feature’s data type.\nIt populates a QComboBox with the names of these operations.\nWhen an operation is selected, it uses a map of factory functions (_parameterWidgetFactories) to create and display the appropriate TransformParameter_Widget (e.g., AnalogEventThreshold_Widget) for that operation. This is an example of the Factory Method pattern.\nIt has a “Do Transform” button that, when clicked, retrieves the parameters from the current TransformParameter_Widget, gets the selected TransformOperation from the TransformRegistry, and executes the operation on the selected data.",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_transform_widget.html#features",
    "href": "developer/data_transform_widget.html#features",
    "title": "Data Transform Widget",
    "section": "Features",
    "text": "Features\n\nThe ProgressCallback mechanism allows the TransformOperation to notify the DataTransform_Widget about its progress, which then updates the UI. This is a simple form of the Observer pattern.",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_transform_widget.html#design-example",
    "href": "developer/data_transform_widget.html#design-example",
    "title": "Data Transform Widget",
    "section": "Design Example",
    "text": "Design Example\nFirst, the user will create the transformation translation unit. These are located in the DataManager/transforms directory. The transformations are organized according to their input type. The designer will first need to specify the transformation operation itself, which should take a pointer to the input and return a std::shared_ptr to the output type. The user can also overload this function to take a ProgressCallback for longer operations. For example:\nstruct MaskConnectedComponentParameters : public TransformParametersBase {\n    /**\n     * @brief Minimum size (in pixels) for a connected component to be preserved\n     * \n     * Connected components smaller than this threshold will be removed from the mask.\n     * Must be greater than 0.\n     */\n    int threshold = 10;\n};\n\n///////////////////////////////////////////////////////////////////////////////\n\n/**\n * @brief Remove small connected components from mask data\n * \n * This function applies connected component analysis to remove small isolated\n * regions from masks. Uses 8-connectivity (considers diagonal neighbors as connected).\n * \n * @param mask_data The MaskData to process\n * @param params The connected component parameters\n * @return A new MaskData with small connected components removed\n */\nstd::shared_ptr&lt;MaskData&gt; remove_small_connected_components(\n        MaskData const * mask_data,\n        MaskConnectedComponentParameters const * params = nullptr);\n\n/**\n * @brief Remove small connected components from mask data with progress reporting\n * \n * @param mask_data The MaskData to process\n * @param params The connected component parameters\n * @param progressCallback Progress reporting callback\n * @return A new MaskData with small connected components removed\n */\nstd::shared_ptr&lt;MaskData&gt; remove_small_connected_components(\n        MaskData const * mask_data,\n        MaskConnectedComponentParameters const * params,\n        ProgressCallback progressCallback);\nThe user will also need to define a TransformationOperation interface class that uses this function. This is defined in DataManager/transforms/data_transforms.hpp and this is an example for the MaskConnectedComponentOperation:\n\nclass MaskConnectedComponentOperation final : public TransformOperation {\npublic:\n    [[nodiscard]] std::string getName() const override;\n    [[nodiscard]] std::type_index getTargetInputTypeIndex() const override;\n    [[nodiscard]] bool canApply(DataTypeVariant const & dataVariant) const override;\n    [[nodiscard]] std::unique_ptr&lt;TransformParametersBase&gt; getDefaultParameters() const override;\n    \n    DataTypeVariant execute(DataTypeVariant const & dataVariant,\n                           TransformParametersBase const * transformParameters) override;\n                           \n    DataTypeVariant execute(DataTypeVariant const & dataVariant,\n                           TransformParametersBase const * transformParameters,\n                           ProgressCallback progressCallback) override;\n};\nThe body of these functions is mostly boilerplate that will be verbatim between operations.\n\n\n\n\n\n\nImportant\n\n\n\nThe value return by the getName function will need to be used later in the User Interface exactly. If you do not use the name here to identify your transformation, it may not appear in the UI.\n\n\nAfter you have designed your tranformation, add the files to the CMakeLists.txt for DataManager listed in DataManager/CMakeLists.txt. Then include your header in DataManager/transforms/TransformRegistry.cpp and add your type with the _registerOperation function.\nNow you can create the user interface for your transformation operation. The user interfaces are kept in DataTransform_Widget under folders for the specific input type (same as the transformation). Create a hpp/cpp/ui triplet for your transformation. The purpose of this UI should be to populate parameters structure you created with the tranformation. If you have no options structure, this widget can simply be a label the describes the transformation. See MaskArea_Widget for an example fo a blank UI and LineResample_Widget for a more complex example. Your widget will need to inherit from TransformParameter_Widget as a base class.\nOnce you have completed your widget triplet, add these files to the main CMakeLists.txt for WhiskerToolbox. Then you will modify DataTransform_Widget.cpp to include the header to your UI, and populate _parameterWidgetFactories with the name of your transformation. For example:\n\n_parameterWidgetFactories[\"Remove Small Connected Components\"] = [](QWidget * parent) -&gt; TransformParameter_Widget * {\n        return new MaskConnectedComponent_Widget(parent);\n    };\nNote that the name in this map (e.g. “Remove Small Connected Components”) must match the name that is returned by your transformation operation!\nAfter this, compile and your transformation should appear in the data transformation widget!",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_manager.html",
    "href": "developer/data_manager.html",
    "title": "Data Manager",
    "section": "",
    "text": "The DataManager has 3 core functionalities",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/data_manager.html#time-frame-structure-for-multi-rate-data-visualization",
    "href": "developer/data_manager.html#time-frame-structure-for-multi-rate-data-visualization",
    "title": "Data Manager",
    "section": "Time Frame Structure for Multi-Rate Data Visualization",
    "text": "Time Frame Structure for Multi-Rate Data Visualization\nAn important feature of the program is being able to simultaneously visualize data that was collected at different sampling rates. The time frame structure is designed to indicate when time events occur at a particular sampling frequency. The data manager is responsible for keeping track of which data is located in what time frame. Multiple data objects can belong to one time frame, but one data object can only belong to a single time frame. However, if the relationship between time frame objects is specified, then data requested in one coordinate system can be converted to another.\n\nIllustrative Example: Electrophysiology and Video Data\nConsider the following example: an NI-DAQ box samples at 30000 Hz for electrophysiology. This results in analog data with 30000 samples per second as well as event data for sorted spikes at 30000 Hz resolution. The experiment may have also had a high-speed video camera collecting frames at 500 Hz. A digital event for each frame was recorded with the same NI-DAQ. The user may have processed the video frames to categorize behavior, which would also be at the 500 Hz resolution and be interval data.\n\n\nDefault Time Frame and Customization\nThe default time frame is simply called “time” and defaults to numbers between 1 and the number of frames in a loaded video. The scroll bar operates in this time frame and consequently sends signals in this time frame. The user can override this default time frame and replace it with an event structure with the same number of samples but where each event corresponds to the 30000 Hz resolution digitized camera frame exposures. The user can then create a new clock called “master” that again counts from 1,2,3,… up to the total number of samples collected by the NI-DAQ.\n\n\nData Indexing within Time Frames\nAll data manager data types have a notion of “index,” and these correspond to the time frame they are associated with. This index property is important because data may be sparsely labeled and not have the same number of samples as their time frame.\n\n\nWidget Considerations for Data Synchronization\nWidgets that represent different data simultaneously must be aware of accounting for these differences.\n\n\nPerformance Notes\nThe user should also be aware that pointer indirections sample by sample for large vectors will be quite inefficient. If the user needs to find a series of values in a range in a different coordinate system it is most likely important to.",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/data_manager.html#observer-framework",
    "href": "developer/data_manager.html#observer-framework",
    "title": "Data Manager",
    "section": "Observer Framework",
    "text": "Observer Framework",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/data_manager.html#data-types",
    "href": "developer/data_manager.html#data-types",
    "title": "Data Manager",
    "section": "Data Types",
    "text": "Data Types\n\nPoint\nA Point represents a 2 dimensional (x, y) coordinate, which may vary in time. The PointData structure can hold multiple points per unit time.\n\n\n\nExamples of Point Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLine\nA Line represents a 2 dimensional (x,y) collection of points, which may vary with time. The collection of points is ordered, meaning that each point is positioned relative to its neighbors. The LineData structure can hold multiple lines per unit time.\n\n\n\nExamples of Line Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurrently, lines are represented by a raw list of points. One could imagine lines being parameterized in different ways, such as the coefficients of a polynomial fit. For many calculations, the parameterized version of a line may be less memory intensive and more efficient.\n\n\nMask\nA mask represents a 2 dimensional collection of points, which may vary with time. Compared to a Line, the points in a Mask are not ordered. These would correspond to something like a binary segmentation mask over an image. The MaskData structure can hold multiple masks per unit time.\n\n\n\nExamples of Mask Data\n\n\n\n\nBinary Semantic Segmentation Labels\n\n\n\n\n\n\n\n\n\nA mask may just be thought as a raw pixel, by pixel definition of a shape. Shapes could be defined as bounding boxes, circles, polygons, etc. It may one day be useful to describe shapes in other ways compared to the raw pixel-by-pixel definition.\n\n\nTensors\nTensors are N-Dimensional data structures, and consequently very flexible containers for complex data. A concrete use would be to store a Height x Width x Channel array for different timepoints during an experiment. These may be the features output from an encoder neural network that processes a video.\n\n\n\nExamples of Tensor Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalog Time Series\nAn analog time series has values that can vary continuously\n\n\n\nExamples of Analog Time Series Data\n\n\n\n\nVoltage traces from electrode recordings\n\n\nFluorescence intensity traces from Calcium imaging\n\n\n\n\n\n\n\n\nDigital Event Series\nA digital event represents an ordered sequence of events, where each event is represented as a single instance in time. For instance, spike times from electrophysiology\n\n\n\nExamples of Event Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigital Interval Series\n\n\n\nExamples of Interval Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedia\nMedia is a sequence of images.\n\nImage\n\n\n\nExamples of Image Data\n\n\n\n\nImage sequence from two photon calcium imaging experiment\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\nExamples of Video Data\n\n\n\n\nMP4 video from high speed scientific camera of behavior\n\n\n\n\n\n\n\n\n\n\n\n\nTime Frame",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/building.html",
    "href": "developer/building.html",
    "title": "Building",
    "section": "",
    "text": "CMakeUserPresets.json\n\nIDE Specific Instructions:\n\nCLion\nAt the top to configure build properties, select More Action -&gt; Configuration -&gt; Edit…\nUnder Environment variables, adding this on windows works for me:\nPATH=bin\\;C:\\Qt\\6.7.2\\msvc2019_64\\bin\\;_deps\\torch-src\\lib\\;_deps\\iir-build\\;$PATH$",
    "crumbs": [
      "Building"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "developer/analysis_dashboard.html",
    "href": "developer/analysis_dashboard.html",
    "title": "Analysis Dashboard: Embedding OpenGL Widgets in QGraphicsScene",
    "section": "",
    "text": "Overview\nThis note documents the pattern for embedding a QOpenGLWidget inside a QGraphicsScene via QGraphicsProxyWidget, as used by the Analysis Dashboard plots. The goal is to achieve smooth, interactive pan/zoom and reliable repainting while the plot lives as a QGraphicsItem within a scene.\nThe core pieces:\n\nA plot QGraphicsWidget subclass that owns a QGraphicsProxyWidget hosting the OpenGL widget\nAn OpenGL widget configured for proxy embedding and interactive input\nIntentional event routing so dragging and wheel actions reach the OpenGL widget (not the parent graphics item)\n\n\n\nArchitecture\n\nPlot item: QGraphicsWidget (e.g., SpatialOverlayPlotWidget, ScatterPlotWidget)\n\nDraws the plot frame and title in its paint()\nEmbeds an OpenGL child via QGraphicsProxyWidget\nRoutes mouse events: title area → item movement; content area → OpenGL interactivity\n\nOpenGL child: QOpenGLWidget (e.g., SpatialOverlayOpenGLWidget, ScatterPlotOpenGLWidget)\n\nHolds visualization state and OpenGL resources\nImplements interactive behavior: mouse press/move/release, wheel zoom, tooltips\nEmits signals to trigger lightweight parent/proxy repaints\n\n\n\n\nEmbedding pattern (plot item)\nConfigure the OpenGL widget and proxy to avoid stale frames and input conflicts when embedded in a QGraphicsScene:\n// In PlotWidget::setupOpenGLWidget()\n_opengl_widget = new ScatterPlotOpenGLWidget();\n\n_proxy_widget = new QGraphicsProxyWidget(this);\n_proxy_widget-&gt;setWidget(_opengl_widget);\n\n// OpenGL widget attributes for proxy embedding\n_opengl_widget-&gt;setAttribute(Qt::WA_AlwaysStackOnTop, false);\n_opengl_widget-&gt;setAttribute(Qt::WA_OpaquePaintEvent, true);\n_opengl_widget-&gt;setAttribute(Qt::WA_NoSystemBackground, true);\n_opengl_widget-&gt;setUpdateBehavior(QOpenGLWidget::NoPartialUpdate);\n\n// Prevent the proxy from intercepting movement/selection\n_proxy_widget-&gt;setFlag(QGraphicsItem::ItemIsMovable, false);\n_proxy_widget-&gt;setFlag(QGraphicsItem::ItemIsSelectable, false);\n\n// Make sure we repaint from the child every update\n_proxy_widget-&gt;setCacheMode(QGraphicsItem::NoCache);\n\n// Lay out inside the plot frame (leave room for title/border)\nQRectF rect = boundingRect();\nQRectF content_rect = rect.adjusted(8, 30, -8, -8);\n_proxy_widget-&gt;setGeometry(content_rect);\n_proxy_widget-&gt;widget()-&gt;resize(content_rect.size().toSize());\nEvent routing in the plot item ensures that content-area interaction reaches the OpenGL widget:\n// In PlotWidget::mousePressEvent(QGraphicsSceneMouseEvent* event)\nQRectF title_area = boundingRect().adjusted(0, 0, 0, -boundingRect().height() + 25);\n\nif (title_area.contains(event-&gt;pos())) {\n    // Title: allow moving/selecting the plot item\n    emit plotSelected(getPlotId());\n    setFlag(QGraphicsItem::ItemIsMovable, true);\n    AbstractPlotWidget::mousePressEvent(event);\n} else {\n    // Content: disable item movement so drag goes to the OpenGL child\n    emit plotSelected(getPlotId());\n    setFlag(QGraphicsItem::ItemIsMovable, false);\n    event-&gt;accept();\n}\n\n\nOpenGL widget configuration\nConfigure the QOpenGLWidget so it reliably receives hover/wheel events and repaints well under a proxy:\n// In OpenGLWidget constructor\nsetMouseTracking(true);\nsetFocusPolicy(Qt::StrongFocus);\n\n// Prefer a core profile and multisampling\nQSurfaceFormat fmt;\nfmt.setVersion(4, 1);\nfmt.setProfile(QSurfaceFormat::CoreProfile);\nfmt.setSamples(4);\nsetFormat(fmt);\n\n// Attributes for embedding in a QGraphicsProxyWidget\nsetAttribute(Qt::WA_AlwaysStackOnTop, false);\nsetAttribute(Qt::WA_OpaquePaintEvent, true);\nsetAttribute(Qt::WA_NoSystemBackground, true);\nsetUpdateBehavior(QOpenGLWidget::NoPartialUpdate);\nTypical interactive handlers (pan/zoom):\nvoid OpenGLWidget::mousePressEvent(QMouseEvent* e) {\n    if (e-&gt;button() == Qt::LeftButton) {\n        _is_panning = true;\n        _last_mouse_pos = e-&gt;pos();\n        e-&gt;accept();\n    } else {\n        e-&gt;ignore();\n    }\n}\n\nvoid OpenGLWidget::mouseMoveEvent(QMouseEvent* e) {\n    if (_is_panning && (e-&gt;buttons() & Qt::LeftButton)) {\n        QPoint delta = e-&gt;pos() - _last_mouse_pos;\n        float world_scale = 2.0f / (_zoom_level * std::min(width(), height()));\n        setPanOffset(_pan_offset_x + delta.x() * world_scale,\n                     _pan_offset_y - delta.y() * world_scale);\n        _last_mouse_pos = e-&gt;pos();\n        e-&gt;accept();\n    } else {\n        e-&gt;accept();\n    }\n}\n\nvoid OpenGLWidget::wheelEvent(QWheelEvent* e) {\n    float zoom_factor = 1.0f + (e-&gt;angleDelta().y() / 1200.0f);\n    setZoomLevel(_zoom_level * zoom_factor);\n    e-&gt;accept();\n}\n\n\nRepaint strategy\n\nThe OpenGL widget calls update() (optionally throttled) on interaction; the proxy and parent item listen to signals like zoomLevelChanged / panOffsetChanged to call update() on themselves too.\nDisable caching (QGraphicsItem::NoCache) on the proxy so frames are not reused while the GL child is animating.\n\n\n\nTroubleshooting\n\nSymptom: plot only updates after resize or clicking away\n\nEnsure the GL child has Qt::WA_OpaquePaintEvent, Qt::WA_NoSystemBackground, and NoPartialUpdate\nEnsure the proxy uses NoCache\nVerify event routing: in content area, disable ItemIsMovable and accept the event so drag/wheel reach the GL widget\nEnable setMouseTracking(true) and setFocusPolicy(Qt::StrongFocus) on the GL widget\n\n\n\n\nReferences\n\nPlot items: src/WhiskerToolbox/Analysis_Dashboard/Widgets/SpatialOverlayPlotWidget/SpatialOverlayPlotWidget.cpp, src/WhiskerToolbox/Analysis_Dashboard/Widgets/ScatterPlotWidget/ScatterPlotWidget.cpp\nOpenGL widgets: src/WhiskerToolbox/Analysis_Dashboard/Widgets/SpatialOverlayPlotWidget/SpatialOverlayOpenGLWidget.cpp, src/WhiskerToolbox/Analysis_Dashboard/Widgets/ScatterPlotWidget/ScatterPlotOpenGLWidget.cpp"
  },
  {
    "objectID": "developer/contributing_code.html",
    "href": "developer/contributing_code.html",
    "title": "Contributing Code",
    "section": "",
    "text": "Header files should use the hpp suffix, and source files should use the cpp suffix\nIncludes should follow the “Lakos” include order, that is\n\nThe prototype/interface header for this implementation (ie, the .h/.hh file that corresponds to this .cpp/.cc file).\nOther headers from the same project, as needed.\nHeaders from other non-standard, non-system libraries (for example, Qt, Eigen, etc).\nHeaders from other “almost-standard” libraries (for example, Boost)\nStandard C++ headers (for example, iostream, functional, etc.)\nStandard C headers (for example, cstdint, dirent.h, etc.)\n\nPrefer returning std::optional as a mechanism of error handling\nThis is a scientific computing library. Performance is critical. Helping the user to understand where errors have occurred is helpful, but keeping the program alive after an error is not critical. Functions should fail gracefully and provide informative error messages when they do. Logging should use spdlog.\nPrefer free functions as much as possible. Ideally, class member functions will be simple and pass member variables to free functions.\nPrefer standard library algorithms where possible\nPublic member functions and free function declarations should include doxygen comments above them. Private member function definitions should include doxygen comments above them.\nPrefer forward declarations in header files\nDocument pre-conditions and post-conditions in doxygen comments uses the @pre and @post tags.\nThis is a C++20 project. Prefer standard library algorithms and std::ranges where possible.\nPrivate member variables and functions in classes should be prefaced with an underscore operator (e.g. calculateMean). In a struct with public facing member variables, they should be prefaced with m followed by an underscore (e.g. m_height).",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#design-guidelines",
    "href": "developer/contributing_code.html#design-guidelines",
    "title": "Contributing Code",
    "section": "",
    "text": "Header files should use the hpp suffix, and source files should use the cpp suffix\nIncludes should follow the “Lakos” include order, that is\n\nThe prototype/interface header for this implementation (ie, the .h/.hh file that corresponds to this .cpp/.cc file).\nOther headers from the same project, as needed.\nHeaders from other non-standard, non-system libraries (for example, Qt, Eigen, etc).\nHeaders from other “almost-standard” libraries (for example, Boost)\nStandard C++ headers (for example, iostream, functional, etc.)\nStandard C headers (for example, cstdint, dirent.h, etc.)\n\nPrefer returning std::optional as a mechanism of error handling\nThis is a scientific computing library. Performance is critical. Helping the user to understand where errors have occurred is helpful, but keeping the program alive after an error is not critical. Functions should fail gracefully and provide informative error messages when they do. Logging should use spdlog.\nPrefer free functions as much as possible. Ideally, class member functions will be simple and pass member variables to free functions.\nPrefer standard library algorithms where possible\nPublic member functions and free function declarations should include doxygen comments above them. Private member function definitions should include doxygen comments above them.\nPrefer forward declarations in header files\nDocument pre-conditions and post-conditions in doxygen comments uses the @pre and @post tags.\nThis is a C++20 project. Prefer standard library algorithms and std::ranges where possible.\nPrivate member variables and functions in classes should be prefaced with an underscore operator (e.g. calculateMean). In a struct with public facing member variables, they should be prefaced with m followed by an underscore (e.g. m_height).",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#before-you-submit-a-pull-request",
    "href": "developer/contributing_code.html#before-you-submit-a-pull-request",
    "title": "Contributing Code",
    "section": "Before You Submit a Pull Request",
    "text": "Before You Submit a Pull Request\n\nClang Format\nPlease make sure to run clang-format on all of your submitted files with the style guidelines in the base directory. A CI check will ensure that this happens upon pull request. You can read more about clang-format here:\nhttps://clang.llvm.org/docs/ClangFormat.html\n\n\nClang Tidy\nPlease make sure to run clang-tidy on all of your submitted files with the style guidelines in the base directory. A CI check will ensure that this happens upon pull request. You can read more about clang-tidy here:\nhttps://clang.llvm.org/extra/clang-tidy/",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#testing",
    "href": "developer/contributing_code.html#testing",
    "title": "Contributing Code",
    "section": "Testing",
    "text": "Testing\n\nTesting is performed with Catch2. \nThe component being tested generally has two TEST_CASE parts. The first will test the “happy path” to ensure that the computations work as expected. Different SECTIONs will be used for different computations. A second TEST_CASE will handle error handling and edge cases. Each SECTION will be for a different edge case / error.\nUse descriptive names for each test.\nTEST_CASES should also use useful tags.\nUse REQUIRE instead of CHECK\nSimple setup can be performed in the beginning of a TEST_CASE. Fixtures are only necessary for complex setup/teardown.\nPrefer Catch::Matchers::WithinRel to Catch::Approx\nTest files are included in the same folder as a given translation unit. They should have the same name as the header file with the extension “.test.cpp”. For example mask_to_line.hpp and mask_to_line.cpp will have the test file mask_to_line.test.cpp.",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#further-resources-and-references",
    "href": "developer/contributing_code.html#further-resources-and-references",
    "title": "Contributing Code",
    "section": "Further Resources and References",
    "text": "Further Resources and References\n\nLLM Support\nA configuration file for a plain text generator, repo-to-text, is included in the top level source directory. This can generate a single text file for the entire Neuralyzer repository which can be easily pasted into a LLM of choice.\n\n\nTesting\n\nC++ Development\nMike Shah has an excellent modern C++ design series on youtube. Episodes are a nice ~20 minute length:\nModern Cpp series by Mike Shah\n\n\nGraphics Programming\nMike Shah also has a series on modern OpenGL on youtube:\nIntroduction to OpenGL",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/data_manager_widget.html",
    "href": "developer/data_manager_widget.html",
    "title": "Data Manager Widget",
    "section": "",
    "text": "Data Manager Widget Overview\nThe data manager widget serves as the main user interface for interacting with all data currently loaded into the program. At its top, the widget displays all keys currently in the data manager along with their corresponding data types. It enables the user to specify the folder for saving data. Additionally, users can create new, initially blank data sets, which can subsequently be populated through further manipulations and other widgets.\nWhen a user selects an entry in the top table, a data type-specific soft widget is populated. This widget presents the user with various features of the selected data. For example, when dealing with point data, it shows a table listing every available point within that data set and its associated frame. The user has the ability to scroll through this table; clicking an entry causes the time displayed by the rest of the program to jump to that point’s frame. Users can perform several data manipulations via these specific widgets. For instance, using the point widget, a user can delete any specific point. They can also opt to move a specific point to another point data set available in the manager.\n\n\nData Saving Functionality\nThe data manager widget also functions as the primary interface for saving data to disk. It features a sub-widget interface where the user can select the desired file output type, and specific options for that output type are then displayed. For instance, if a user is viewing point data, they can choose to output CSV files and will be presented with various CSV saving options, such as selecting the delimiter and deciding whether a header should be included, etc.",
    "crumbs": [
      "Data Manager Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html",
    "href": "developer/data_viewer_widget.html",
    "title": "Data Viewer Widget",
    "section": "",
    "text": "The data viewer widget is designed for visualizing plots of time series data. It can operate on multiple distinct data types, such as analog time series, interval series, and event series. The widget contains a table of compatible data types and their corresponding keys, which can be selected for display in the viewer. It houses options to enable the visualization of different types and also includes an OpenGL canvas responsible for rendering the data according to user specifications.",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#event-viewer-features",
    "href": "developer/data_viewer_widget.html#event-viewer-features",
    "title": "Data Viewer Widget",
    "section": "Event Viewer Features",
    "text": "Event Viewer Features\nThe EventViewer_Widget provides controls for digital event series visualization with two main display modes:\n\nDisplay Modes\n\nStacked Mode (Default): Events are positioned in separate horizontal lanes with configurable spacing\n\nEach event series gets its own horizontal “lane”\nConfigurable vertical spacing between lanes\nConfigurable event line height\nAuto-calculation of optimal spacing when event groups are loaded\n\nFull Canvas Mode: Events stretch from top to bottom of the entire canvas\n\nOriginal behavior maintained for compatibility\nAll events span the full height of the display area\n\n\n\n\nAuto-Spacing for Event Groups\nWhen a tree of digital event series is selected and enabled, the system automatically calculates optimal spacing to fit all events on the canvas:\n\nIf 40 events are enabled on a 400-pixel tall canvas, each event gets approximately 10 pixels of space\nSpacing and height are calculated to ensure visual separation between different event series\nUses 80% of available canvas height, leaving margins at top and bottom\nEvent height is set to 60% of calculated spacing to prevent overlap\n\n\n\nUser Controls\nThe EventViewer_Widget provides: - Display Mode: Combo box to switch between Stacked and Full Canvas modes - Vertical Spacing: Adjustable spacing between stacked event series (0.01-1.0 normalized units) - Event Height: Adjustable height of individual event lines (0.01-0.5 normalized units) - Color Controls: Standard color picker for event line colors and transparency\n\n\nImplementation Details\n\nEvent stacking uses normalized coordinates for consistent spacing across different canvas sizes\nState is preserved when switching between data series\nIntegration with existing TreeWidgetStateManager for persistence\nOptimized rendering with proper OpenGL line thickness and positioning\n\n\n\nTime Frame Synchronization\nThe DataViewer Widget properly handles multi-rate data synchronization:\n\nMaster Time Frame: OpenGLWidget maintains a reference to the master time frame (“master” or “time”) used for X-axis coordinates\nTime Frame Conversion: When data series use different time frames from the master, proper coordinate conversion ensures synchronized display\nCross-Rate Compatibility: Supports simultaneous visualization of data collected at different sampling rates (e.g., 30kHz electrophysiology with 500Hz video)\nConsistent X-Axis: All data types (analog time series, digital events, digital intervals) are rendered with consistent X-axis positioning regardless of their native time frame\nRange Query Optimization: Data range queries are optimized for each time frame to ensure efficient rendering of large datasets",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#interactive-interval-editing",
    "href": "developer/data_viewer_widget.html#interactive-interval-editing",
    "title": "Data Viewer Widget",
    "section": "Interactive Interval Editing",
    "text": "Interactive Interval Editing\nThe DataViewer Widget supports interactive editing of digital interval series through a mouse-based dragging interface that handles multi-timeframe data seamlessly.\n\nInterval Selection and Highlighting\n\nClick-to-Select: Users can click on digital intervals to select them for editing\nVisual Feedback: Selected intervals are highlighted with enhanced borders for clear identification\nPer-Series Selection: Each digital interval series maintains its own independent selection state\n\n\n\nInterval Edge Dragging\nThe widget provides precise interval boundary editing through a sophisticated dragging system:\n\nCore Functionality\n\nEdge Detection: Mouse hover near interval boundaries (within 10 pixels) changes cursor to resize indicator\nDrag Initiation: Click and drag on interval edges to modify interval start or end times\nReal-time Preview: During dragging, both original (dimmed) and new (semi-transparent) interval positions are shown\nCollision Prevention: Automatic constraint enforcement prevents intervals from overlapping with existing intervals\n\n\n\nMulti-Timeframe Support\nThe interval dragging system automatically handles time frame conversion for data collected at different sampling rates:\n\nCoordinate Conversion: Mouse coordinates (in master time frame) are automatically converted to the series’ native time frame indices\nPrecision Handling: Dragged positions are rounded to the nearest valid index in the target time frame, accommodating different sampling resolutions\nConstraint Enforcement: Collision detection and boundary constraints are performed in the series’ native time frame for accuracy\nDisplay Consistency: Visual feedback remains in master time frame coordinates for consistent user experience\n\n\n\nError Handling and Robustness\n\nGraceful Degradation: Failed time frame conversions abort the drag operation while preserving original data\nData Integrity: Invalid interval bounds (e.g., start ≥ end) are rejected without modifying existing data\nState Management: Drag operations can be cancelled (ESC key) to restore original interval boundaries\n\n\n\nExample Use Cases\n\nBehavioral Annotation: Researchers can precisely adjust behavioral event boundaries recorded at video frame rates (30-120 Hz) while viewing synchronized neural data at higher sampling rates (20-30 kHz)\nEvent Refinement: Fine-tune automatically detected events by dragging boundaries to match observed signal characteristics across different data modalities\nCross-Modal Synchronization: Align interval boundaries across different measurement systems with varying temporal resolutions\n\n\n\n\nTechnical Implementation\nThe interval editing system leverages the existing TimeFrame infrastructure:\n\nTimeFrame.getIndexAtTime(): Converts master time coordinates to series-specific indices\nTimeFrame.getTimeAtIndex(): Converts series indices back to master time coordinates for display\nAutomatic Snapping: Ensures all interval boundaries align with valid time points in the target series\nThread Safety: All operations maintain data consistency during concurrent access\n\nThis functionality enables precise temporal analysis workflows while abstracting away the complexity of multi-rate data synchronization from the end user.",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#vertical-space-coordination",
    "href": "developer/data_viewer_widget.html#vertical-space-coordination",
    "title": "Data Viewer Widget",
    "section": "Vertical Space Coordination",
    "text": "Vertical Space Coordination\nThe DataViewer_Widget includes a sophisticated vertical space management system that prevents overlap between different data types and ensures optimal use of screen real estate.\n\nVerticalSpaceManager\nThe VerticalSpaceManager class handles automatic positioning and scaling for all data series:\n\nOrder-preserving: New data positioned below existing data\nType-aware spacing: Different configurations for analog (larger heights), digital events (compact), and intervals (moderate)\nAuto-redistribution: Adding new series triggers recalculation to prevent overlap\nCanvas-independent: Uses normalized coordinates for flexibility\n\n\n\nMVP Matrix Architecture\nThe rendering system uses a systematic Model-View-Projection (MVP) matrix approach for consistent positioning and scaling across all data types:\n\nModel Matrix - Series-Specific Transforms\nHandles individual series positioning and scaling:\n// For VerticalSpaceManager-positioned series:\nModel = glm::translate(Model, glm::vec3(0, series_center_y, 0));  // Position\nModel = glm::scale(Model, glm::vec3(1, series_height * 0.5f, 1)); // Scale\n\n// For analog series, additional amplitude scaling:\nfloat amplitude_scale = 1.0f / (stdDev * scale_factor);\nModel = glm::scale(Model, glm::vec3(1, amplitude_scale, 1));\n\n\nView Matrix - Global Operations\nHandles operations applied to all series equally:\nView = glm::translate(View, glm::vec3(0, _verticalPanOffset, 0)); // Global panning\n\n\nProjection Matrix - Coordinate System Mapping\nMaps world coordinates to screen coordinates:\n// X axis: time range [start_time, end_time] → screen width\n// Y axis: world coordinates [min_y, max_y] → screen height\nProjection = glm::ortho(start_time, end_time, min_y, max_y);\n\n\nVertex Coordinate Systems\nVerticalSpaceManager Mode (recommended): - Vertices use normalized coordinates (-1 to +1 in local space) - Model matrix handles all positioning and scaling - Consistent across all data types\nLegacy Mode (backward compatibility): - Vertices use world coordinates directly - Positioning handled by coordinate calculations - Index-based spacing for events\n\n\nDetection of Positioning Mode\nThe system automatically detects which positioning mode to use:\n\nDigital Events: vertical_spacing == 0.0f signals VerticalSpaceManager mode\nAnalog Series: y_offset != 0.0f signals VerticalSpaceManager mode\n\nDigital Intervals: y_offset != 0.0f signals VerticalSpaceManager mode\n\nThis ensures backward compatibility while enabling the new systematic approach.",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#interval-editing-system",
    "href": "developer/data_viewer_widget.html#interval-editing-system",
    "title": "Data Viewer Widget",
    "section": "Interval Editing System",
    "text": "Interval Editing System",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/Features/Collapsible_Widget.html",
    "href": "developer/Features/Collapsible_Widget.html",
    "title": "Collapsible Widget",
    "section": "",
    "text": "We have a Collapsible Widget called a “Section” available. This kind of widget can expand to display the contents of the widget. This is useful for making an interface appear less cluttered.\n\nGeneral process for to add collapsible Widget:\nQT Designer:\n\nEnsure the parent widget is in a vertical layout\nCreate QWidget inside and promote it to a “Section”\nCreate another QWidget within that QWidget. The inner QWidget will hold all the contents of the section.\nPopulate this inner QWidget with widgets as normal.\n\nCode part:\n\nCall autoSetContentLayout() on the Section QWidget after setupUi.\nOptionally, calling setTitle(\"title\") will give it a title.",
    "crumbs": [
      "Features",
      "Collapsible Widget"
    ]
  },
  {
    "objectID": "developer/file_io.html",
    "href": "developer/file_io.html",
    "title": "File IO",
    "section": "",
    "text": "Data Type\nFormat\nDependency\nRaw Data for Testing?\nIO Widget Testing\nDatamanager IO Testing\nJSON Validation Testing\nUser Guide\n\n\n\n\nAnalog\nBinary\n\n\n\n\n\n\n\n\nAnalog\nCSV\n\nSome\n\n\n\n\n\n\nDigitalEvent\nCSV\n\nYes\n\n\n\n\n\n\nDigitalInterval\nBinary\n\n\n\n\n\n\n\n\nDigitalInteral\nCSV\n\n\n\n\n\n\n\n\nLine Data\nBinary\nCap’n Proto\n\n\n\n\n\n\n\nLine Data\nCSV\n\n\n\n\n\n\n\n\nMask Data\nHDF5\nHDF5\n\n\n\n\n\n\n\nMask Data\nImage\nOpenCV\n\n\n\n\n\n\n\nMedia Data\nVideo\nffmpeg\nYes\n\n\n\n\n\n\nMedia Data\nImages\nOpenCV\n\n\n\n\n\n\n\nMedia Data\nHDF5\nHDF5\n\n\n\n\n\n\n\nPoint Data\nCSV\n\nYes\n\n\n\n\n\n\nTensor\nNumpy\nNumpy",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#overview-of-file-io-requirements",
    "href": "developer/file_io.html#overview-of-file-io-requirements",
    "title": "File IO",
    "section": "Overview of File IO Requirements",
    "text": "Overview of File IO Requirements\nBecause Neuralizer is compatible with a wide range of data, its file IO needs to support a wide range of file formats. Some of these file formats will be pre-subscribed, such as data acquisition-specific formats for electrophysiology. Others are more specific to this package, such as for line data that varies over time. We must also be aware from the start that some electrophysiology file sizes can be expected to be larger than easy to work with on a regular desktop machine. For instance, Neuropixel probes record almost 400 channels at 20 kHz and are becoming more routine for use. Consequently, multiple file formats should have the ability to be easily memory-mapped and loaded from disk rather than loading the entire file into RAM.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#incremental-data-adjustments-and-key-value-stores",
    "href": "developer/file_io.html#incremental-data-adjustments-and-key-value-stores",
    "title": "File IO",
    "section": "Incremental Data Adjustments and Key-Value Stores",
    "text": "Incremental Data Adjustments and Key-Value Stores\nAnother important consideration from the start is that many of these data types will be incrementally adjusted. For instance, processing a video sequence frame by frame or manually adjusting the output of an automated algorithm requires changing some subset of data in a much larger scheme. For some of these file types that are greater than hundreds of megabytes, resaving an entire file for point manipulations could be onerous and time-consuming. Consequently, for some file types, it would be advantageous to use a key-value database type structure that allows us to easily make incremental adjustments and only save those changes, consequently saving data much more quickly. To my knowledge, this process is quite common in the wild but uncommon in neuroscience.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#serialization-with-captain-proto",
    "href": "developer/file_io.html#serialization-with-captain-proto",
    "title": "File IO",
    "section": "Serialization with Captain Proto",
    "text": "Serialization with Captain Proto\nThe process of data serialization and deserialization will be accomplished with the Captain Proto library. This is able to easily create binary files from more complex data structures, such as those that hold multiple lines of varying size per unit time. This library can define output file structures that are purely binary and would be saved and loaded as entire objects. Alternatively, different definitions can be used to save objects as key-value pairs; for instance, each time point can be the key and the data at that time can be the value.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#memory-mapping-implementation",
    "href": "developer/file_io.html#memory-mapping-implementation",
    "title": "File IO",
    "section": "Memory Mapping Implementation",
    "text": "Memory Mapping Implementation\nCaptain Proto does not perform memory mapping itself. Memory mapping is different across different types of file systems, so to maintain cross-platform use, we will need to either specify different interfaces for interacting with Captain Proto or we could use another third-party library that does this for us. Boost.Interprocess is commonly cited for this purpose, but there are others.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#key-value-database-libraries",
    "href": "developer/file_io.html#key-value-database-libraries",
    "title": "File IO",
    "section": "Key-Value Database Libraries",
    "text": "Key-Value Database Libraries\nFor key-value database structures, there are multiple libraries available. Many of these are mature and designed to work with servers of extremely large datasets, and some have been developed by very large companies. One of the most popular is called LMDB, but in my test cases, this appears to be not as great with Windows. Using this library requires specifying some maximum file size; on Linux, the resulting file size is only the size of the data, but with my testing on Windows, this large maximum file size seems to persist. An alternative database is RocksDB. This software is developed by Facebook, seems to be quite mature, and should also be able to save as a key-value pair across platforms.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#prioritization-and-future-considerations",
    "href": "developer/file_io.html#prioritization-and-future-considerations",
    "title": "File IO",
    "section": "Prioritization and Future Considerations",
    "text": "Prioritization and Future Considerations\nCurrently, the data that would benefit from key-value updates is only theoretical. Having a line for whiskers for each frame in a high-speed video of up to 200,000 frames results in a file size that is almost 300 MB. However, saving a binary file in its entirety when making whisker-specific updates through manual curation still takes less than a second to save and is not noticeable to the user. Consequently, I think prioritizing having binary outputs to save entire files and memory mapping of those binary file types that are read-only, such as for Neuropixel analog traces, may be the most desirable next feature. Key-value pairs with something like RocksDB should be remembered for the future, but I am going to make that less of a priority until I see a clear use case that would benefit.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/point_display_options.html",
    "href": "developer/point_display_options.html",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options.\n\n\n\n\n\n\nRange: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives\n\n\n\n\n\n\n\n\nRange: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots\n\n\n\n\n\n\n\nstruct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - All configurations support real-time updates without data loss\n\n\n\n\n\n\n\nSelect the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nDocument new features in this file"
  },
  {
    "objectID": "developer/point_display_options.html#overview",
    "href": "developer/point_display_options.html#overview",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options."
  },
  {
    "objectID": "developer/point_display_options.html#point-display-options",
    "href": "developer/point_display_options.html#point-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives"
  },
  {
    "objectID": "developer/point_display_options.html#line-display-options",
    "href": "developer/point_display_options.html#line-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots"
  },
  {
    "objectID": "developer/point_display_options.html#technical-architecture",
    "href": "developer/point_display_options.html#technical-architecture",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "struct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - All configurations support real-time updates without data loss"
  },
  {
    "objectID": "developer/point_display_options.html#usage-guidelines",
    "href": "developer/point_display_options.html#usage-guidelines",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Select the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nDocument new features in this file"
  },
  {
    "objectID": "developer/transforms.html",
    "href": "developer/transforms.html",
    "title": "transforms",
    "section": "",
    "text": "Transform Catalog\nThe below table has all of the transformations currently available in the Data Transform widget. Each transform should have a test.cpp file (Column 2). The test.cpp file should include tests to make sure it can be populated by a JSON template (e.g. it can be run by loading data with a JSON file). Some transforms will “just work” because they are parameterless, but transforms with parameters will need to have them also described with a JSON link and put in the transform parameter factory. Documentation for users should be provided to describe the algorithm and its uses. Finally, a benchmark.cpp file should be created and placed in the same directory.\n\n\n\n\n\n\n\n\n\n\n\n\nTransform Name\nHas .test.cpp?\nJSON-based Test?\nParameters in ParameterFactory?\nUser Docs Exist?\nHas benchmark.cpp?\nBenchmark Results\n\n\n\n\nCalculate Mask Centroid\nYes\nYes\nYes\nNo\nNo\n\n\n\nConvert Mask to Line\nYes\nYes\nYes\nNo\nNo\n\n\n\nSkeletonize Mask\nYes\nYes\nYes\nNo\nNo\n\n\n\nRemove Small Connected Components\nYes\nYes\nYes\nNo\nNo\n\n\n\nApply Median Filter\nYes\nYes\nYes\nNo\nNo\n\n\n\nCalculate Area\nYes\nYes\nYes\nYes\nNo\n\n\n\nFill Mask Holes\nYes\nYes\nYes\nNo\nNo\n\n\n\nCalculate Mask Principal Axis\nYes\nYes\nYes\nNo\nNo\n\n\n\nWhisker Tracing\nNo\nN/A\nNo\nNo\nNo\n\n\n\nClip Line by Reference Line\nYes\nYes\nYes\nYes\nNo\n\n\n\nLine Alignment to Bright Features\nYes\nYes\nYes\nNo\nYes\n10 images in 2ms\n\n\nCalculate Line to Point Distance\nYes\nYes\nYes\nYes\nNo\n\n\n\nResample Line\nYes\nYes\nYes\nYes\nNo\n\n\n\nExtract Line Subsegment\nYes\nYes\nYes\nYes\nNo\n\n\n\nCalculate Line Angle\nYes\nYes\nYes\nYes\nNo\n\n\n\nCalculate Line Curvature\nYes\nYes\nYes\nYes\nNo\n\n\n\nExtract Point from Line\nYes\nYes\nYes\nYes\nNo\n\n\n\nFilter\nYes\nNo\nNo\nNo\nNo\n\n\n\nHilbert Phase\nYes\nYes\nYes\nYes\nNo\n\n\n\nThreshold Event Detection\nYes\nYes\nYes\nYes\nNo\n\n\n\nThreshold Interval Detection\nYes\nYes\nYes\nYes\nNo\n\n\n\nScale and Normalize\nYes\nYes\nYes\nNo\nNo\n\n\n\nGroup Intervals\nYes\nYes\nYes\nYes\nNo",
    "crumbs": [
      "transforms"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neuralyzer",
    "section": "",
    "text": "Neuralyzer is a cross-platform software package designed for analyzing and visualizing common forms of data generated during systems neuroscience experiments."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Neuralyzer",
    "section": "Installation",
    "text": "Installation\nThe software comes as compiled .exe, .dmg or tar files and can be immediately used on Windows, Mac and Linux - no install required !\nLatest Release: link\nOld Releases: link"
  },
  {
    "objectID": "index.html#supported-data-types",
    "href": "index.html#supported-data-types",
    "title": "Neuralyzer",
    "section": "Supported Data Types",
    "text": "Supported Data Types\nSome currently supported data types include:\n\nMultimedia - High speed video and collections of images\nArrays of simple 2D shapes - Points, Lines, and masks that vary with time.\nDigital time series data - Events, timestamps,\nAnalog time series data - Continuous movement variables"
  },
  {
    "objectID": "index.html#how-documentation-is-organized",
    "href": "index.html#how-documentation-is-organized",
    "title": "Neuralyzer",
    "section": "How Documentation is Organized",
    "text": "How Documentation is Organized\nThis documentation will follow the quadrants of the Diátaxis documentation authoring framework\n\nTutorials\nHow-to Guides\nConcepts\nReference Guides"
  },
  {
    "objectID": "labeling/mask.html",
    "href": "labeling/mask.html",
    "title": "Creating Mask Labels",
    "section": "",
    "text": "Creating mask labels is done through the GrabCut tool. Currently it can be opened through the Tongue Tracking widget."
  },
  {
    "objectID": "labeling/mask.html#grabcut-tool-operation",
    "href": "labeling/mask.html#grabcut-tool-operation",
    "title": "Creating Mask Labels",
    "section": "GrabCut Tool Operation",
    "text": "GrabCut Tool Operation\n\nSelecting ROI\nTo start creating a mask, left click and drag the mouse over the image to form a green rectangle as the region of interest. The objected intended to be masked should be completely within this rectangle. If a mistake is made the reset button returns the tool to the initial state.\nThe GrabCut algorithm works as such: the algorithm keeps track of the mask and for each iteration, attempts to refine it. In between iterations, the user can tweak the mask to provide feedback to the algorithm, which will be noted through subsequent iterations.\nAfter drawing an ROI the “Iterate Grabcut” button becomes functional. Using it completes one iteration of the GrabCut algorithm. The first usage of this button will show the initial mask created by the algorithm. Ideally the user should press this button throughout the editing process.\n\nOpacity of the displayed mask can be adjusted with the transparency slider\n\n\n\nUser Feedback\nIn between iterations the user has access to a drawing bush whose radius can be adjusted. It is used to paint feedback for the GrabCut algorithm, which it will take into account upon pressing the “Iterate Grabcut” button. The brush has four colors:\n\n“Definite Background”: Tells GrabCut the painted area is definitely not part of the mask.\n“Definite Foreground”: Tells GrabCut the painted area is definitely part of the mask.\n“Probable Background”: Suggests to GrabCut the painted area may not be part of the mask. GrabCut uses this information to create a better mask but may partially/fully disobey it.\n“Probable Foreground”: Suggests to GrabCut the painted area is likely part of the mask. GrabCut uses this information to create a better mask but may partially/fully disobey it.\n\n\n\nSaving and Exporting\nThe GrabCut tool may be exited at any time by pressing the “Exit” button. “Save and Exit” will exit the tool and save the mask into the main application and displayed in the media player. All created masks can be saved to disk using the “Save Drawn Masks” button in the Tongue Tracking widget."
  },
  {
    "objectID": "user_guide/behaviors/whisker.html",
    "href": "user_guide/behaviors/whisker.html",
    "title": "Whisker Tracking",
    "section": "",
    "text": "Load Whiskers\n\nSupported Whisker File Formats\n\n\n\n\n\n\nFile Format\nDescription\n\n\n\n\nJanelia\nBinary format output by janelia whisker tracker\n\n\nCSV\nEach row represents a 2d point (x,y) along the whisker. The points should be arranged from follicle to tip\n\n\nHDF5\n\n\n\n\n\n\nLoad Keypoints\n\n\nTrace Button\n\n\nLength Threshold\nWhisker segments below the length threshold will be discarded. Length threshold is in units of pixels\n\n\nWhisker Pad Selection\nThe whisker pad location in pixel coordinates. Candidate whiskers are ordered so that the base of the whisker is nearest the whisker pad. Whiskers with bases beyond some distance from the whisker pad can also be discarded.\n\n\nHead Orientation\nThe head orientation is the direction that the animal’s nose is pointing in the image. The head orientation is used to determine the identity of the whiskers in the frame (most posterior whisker is 0, next most posterior is 1, etc).\n\n\nNumber of Whiskers Selection\n\n\nExport Image and CSV Button\n\n\nFace Mask\nThe face mask corresponds to the part of the image belonging to the face. This can be used in several ways\n\nWhisker bases can be extended to always connect to the face mask. This eliminates jitter that can occur because of fur\nWhisker bases can be clipped to ensure that the whisker does not enter the face mask.\n\n\n\nJanelia Settings\n\n\nContact Detection",
    "crumbs": [
      "Behavioral Modules",
      "Whisker Tracking"
    ]
  },
  {
    "objectID": "user_guide/data_loading/JSON_loading.html",
    "href": "user_guide/data_loading/JSON_loading.html",
    "title": "JSON_loading",
    "section": "",
    "text": "Digital Event Series\nDigital event series are data represented by an ordered sequence of timestamps. Examples include spike timestamps from extracellular recordings or behavioral events (e.g. go cue, reward given).\n\n\nDigital Interval Series\n\n16 bit binary representation\n{\n  \"filepath\": \"ttl.bin\",\n  \"data_type\": \"digital_interval\",\n  \"name\": \"laser\",\n  \"format\": \"uint16\",\n  \"channel\": 2, // REQUIRED, bit (0 based) for channel of interest\n  \"transition\": \"rising\", //optional, \n  \"clock\": \"master\", //optional, clock signal to assign to these events\n  \"header_size\": 0 //optional, number of bytes to skip at start of file\n}\n\n\n\n\n\n\n\n\n\n\nElement\nDescription\nRequired?\nType\nNotes\n\n\n\n\nfilepath\nPath to binary file, relative to JSON file.\nYes\nstring\n\n\n\ndata_type\n\nYes\nstring\n“digital_interval”\n\n\nname\nName of the data once it is loaded into Neuralyzer\nYes\nstring\n\n\n\nformat\n\nYes\nstring\n“uint16”\n\n\nchannel\nSpecifies which bit in binary representation should be extracted as digital interval\nNo\nnumber\nDefault is 0. Valid values range from 0-15\n\n\nTransition\n“rising” will count a TTL interval as one extending from low-to-high transitions to high-to-low transitions. “falling” will count a TTL interval as one extending from high-to-low to low-to-high transitions.\nNo\nstring\nDefault is “rising”. Valid values are “rising” or “falling”.\n\n\nclock\nClock signal to associate with this digital interval\nNo\nstring\nThe clock string must match the name of a loaded clock signal.\n\n\nheader_size\nThis many bytes will be skipped at the beginning of the file before reading the rest.\nNo\nnumber\nDefault is 0. Accepted values range from 0 to size of file in bytes.\n\n\n\n\n\n\n\n\nCSV\n\n{\n  \"filepath\": \"ttl.bin\",\n  \"data_type\": \"digital_interval\",\n  \"name\": \"laser\",\n  \"format\": \"csv\"\n\n}\n\n\n\n\n\n\n\n\n\n\nElement\nDescription\nRequired?\nType\nNotes\n\n\n\n\nfilepath\nPath to csv file, relative to JSON file.\nYes\nstring\n\n\n\ndata_type\n\nYes\nstring\n“digital_interval”\n\n\nname\nName of the data once it is loaded into Neuralyzer\nYes\nstring\n\n\n\nformat\n\nYes\nstring\n“csv”",
    "crumbs": [
      "Data Loading",
      "JSON_loading"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html",
    "href": "user_guide/data_transformations/analog_filter.html",
    "title": "Analog Filter",
    "section": "",
    "text": "The Analog Filter transform applies a digital filter to an AnalogTimeSeries. This can be used to remove noise, isolate specific frequency bands, or perform other signal processing tasks."
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#purpose",
    "href": "user_guide/data_transformations/analog_filter.html#purpose",
    "title": "Analog Filter",
    "section": "",
    "text": "The Analog Filter transform applies a digital filter to an AnalogTimeSeries. This can be used to remove noise, isolate specific frequency bands, or perform other signal processing tasks."
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#parameters",
    "href": "user_guide/data_transformations/analog_filter.html#parameters",
    "title": "Analog Filter",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilter_type\nstring\nThe type of filter to apply. One of “lowpass”, “highpass”, “bandpass”, “bandstop”.\n“lowpass”\n\n\ncutoff_frequency\ndouble\nThe cutoff frequency for lowpass and highpass filters, or the lower cutoff frequency for bandpass and bandstop filters.\n10.0\n\n\ncutoff_frequency2\ndouble\nThe upper cutoff frequency for bandpass and bandstop filters.\n0.0\n\n\norder\ninteger\nThe order of the filter. Must be between 1 and 8.\n4\n\n\nripple\ndouble\nThe ripple for Chebyshev filters (in dB). Not used for Butterworth filters.\n0.0\n\n\nzero_phase\nboolean\nIf true, applies the filter forward and backward to eliminate phase distortion.\nfalse"
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#json-schema",
    "href": "user_guide/data_transformations/analog_filter.html#json-schema",
    "title": "Analog Filter",
    "section": "JSON Schema",
    "text": "JSON Schema\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"transform\": {\n      \"type\": \"string\",\n      \"const\": \"Analog Filter\"\n    },\n    \"input\": {\n      \"type\": \"string\"\n    },\n    \"output\": {\n      \"type\": \"string\"\n    },\n    \"params\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"filter_type\": {\n          \"type\": \"string\",\n          \"enum\": [\"lowpass\", \"highpass\", \"bandpass\", \"bandstop\"]\n        },\n        \"cutoff_frequency\": {\n          \"type\": \"number\"\n        },\n        \"cutoff_frequency2\": {\n          \"type\": \"number\"\n        },\n        \"order\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"maximum\": 8\n        },\n        \"ripple\": {\n          \"type\": \"number\"\n        },\n        \"zero_phase\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\"filter_type\", \"cutoff_frequency\", \"order\"]\n    }\n  },\n  \"required\": [\"transform\", \"input\", \"output\", \"params\"]\n}"
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#minimal-example",
    "href": "user_guide/data_transformations/analog_filter.html#minimal-example",
    "title": "Analog Filter",
    "section": "Minimal Example",
    "text": "Minimal Example\nThis example applies a 4th order lowpass Butterworth filter at 100 Hz.\n{\n  \"transform\": \"Analog Filter\",\n  \"input\": \"my_analog_series\",\n  \"output\": \"filtered_series\",\n  \"params\": {\n    \"filter_type\": \"lowpass\",\n    \"cutoff_frequency\": 100,\n    \"order\": 4\n  }\n}"
  },
  {
    "objectID": "user_guide/data_transformations/analog_interval_threshold.html",
    "href": "user_guide/data_transformations/analog_interval_threshold.html",
    "title": "Analog Interval Threshold",
    "section": "",
    "text": "This transform detects continuous time intervals during which an analog signal’s value meets a specified threshold condition (e.g., staying above a value).\n\n\nThis transformation is used to identify continuous periods, or “intervals,” where a signal’s amplitude meets certain criteria. Unlike event detection, which marks a single point in time, interval detection finds the start and end times of periods that satisfy the condition.\nThe user can define a threshold and specify whether the signal must be above (Positive), below (Negative), or have its absolute value exceed (Absolute) that threshold.\nAdditionally, two timing parameters help refine the detection: - A lockout_time prevents the detection of new intervals for a set duration after the start of a previous interval. - A min_duration ensures that only intervals lasting longer than a specified duration are included in the final output.\nThis transform is useful for isolating epochs of interest in a continuous signal for further analysis. It takes an analog time series as input and produces a digital interval series as output.\n\n\n\n\nThis transform is particularly useful for identifying specific “states” or “epochs” in neural or behavioral data.\n\nIdentifying Periods of Movement: From accelerometer or video tracking data, one can find intervals of high activity that correspond to an animal moving, versus periods of rest.\nAnalyzing Muscle Activation: In electromyography (EMG), this can be used to determine the duration of muscle contractions by finding intervals where the rectified signal remains above a certain level of activity.",
    "crumbs": [
      "Data Transformations",
      "Analog Interval Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_interval_threshold.html#overview",
    "href": "user_guide/data_transformations/analog_interval_threshold.html#overview",
    "title": "Analog Interval Threshold",
    "section": "",
    "text": "This transform detects continuous time intervals during which an analog signal’s value meets a specified threshold condition (e.g., staying above a value).\n\n\nThis transformation is used to identify continuous periods, or “intervals,” where a signal’s amplitude meets certain criteria. Unlike event detection, which marks a single point in time, interval detection finds the start and end times of periods that satisfy the condition.\nThe user can define a threshold and specify whether the signal must be above (Positive), below (Negative), or have its absolute value exceed (Absolute) that threshold.\nAdditionally, two timing parameters help refine the detection: - A lockout_time prevents the detection of new intervals for a set duration after the start of a previous interval. - A min_duration ensures that only intervals lasting longer than a specified duration are included in the final output.\nThis transform is useful for isolating epochs of interest in a continuous signal for further analysis. It takes an analog time series as input and produces a digital interval series as output.\n\n\n\n\nThis transform is particularly useful for identifying specific “states” or “epochs” in neural or behavioral data.\n\nIdentifying Periods of Movement: From accelerometer or video tracking data, one can find intervals of high activity that correspond to an animal moving, versus periods of rest.\nAnalyzing Muscle Activation: In electromyography (EMG), this can be used to determine the duration of muscle contractions by finding intervals where the rectified signal remains above a certain level of activity.",
    "crumbs": [
      "Data Transformations",
      "Analog Interval Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_interval_threshold.html#parameters",
    "href": "user_guide/data_transformations/analog_interval_threshold.html#parameters",
    "title": "Analog Interval Threshold",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nthreshold_value: The amplitude value the signal must be above/below for the duration of the interval.\ndirection: Specifies the condition the signal must meet. Valid options are: \"Positive\", \"Negative\", or \"Absolute\".\nlockout_time: A duration (in the same time units as the data) after the start of a detected interval, during which no new intervals can begin. This is useful for isolating the onset of distinct episodes of activity.\nmin_duration: The minimum required length of an interval. Any detected period that is shorter than this value will be discarded.\nmissing_data_mode: Defines how to handle non-consecutive time points in the signal.\n\nTreat as Zero: Missing time points are treated as if the signal’s value is zero. This can terminate an interval if zero does not meet the threshold condition.\nIgnore: The algorithm proceeds to the next available data point, effectively ignoring the time gap.",
    "crumbs": [
      "Data Transformations",
      "Analog Interval Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_interval_threshold.html#example-configuration",
    "href": "user_guide/data_transformations/analog_interval_threshold.html#example-configuration",
    "title": "Analog Interval Threshold",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example finds all intervals where a signal named “LFP_channel_1” remains above 1.5 for at least 50 milliseconds.\n[\n  {\n    \"transformations\": {\n      \"metadata\": {\n        \"name\": \"Interval Threshold Detection Pipeline\",\n        \"description\": \"Detects intervals of high activity in an LFP signal.\",\n        \"version\": \"1.0\"\n      },\n      \"steps\": [\n        {\n          \"step_id\": \"1\",\n          \"transform_name\": \"Threshold Interval Detection\",\n          \"phase\": \"analysis\",\n          \"input_key\": \"LFP_channel_1\",\n          \"output_key\": \"detected_activity_intervals\",\n          \"parameters\": {\n            \"threshold_value\": 1.5,\n            \"direction\": \"Positive\",\n            \"lockout_time\": 0.0,\n            \"min_duration\": 0.050,\n            \"missing_data_mode\": \"Treat as Zero\"\n          }\n        }\n      ]\n    }\n  }\n]",
    "crumbs": [
      "Data Transformations",
      "Analog Interval Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/digital_interval_group.html",
    "href": "user_guide/data_transformations/digital_interval_group.html",
    "title": "Digital Interval Group",
    "section": "",
    "text": "This transform groups nearby intervals in a digital interval series based on a specified maximum spacing, which is useful for consolidating fragmented temporal events.\n\n\nThis transform merges digital intervals that are close to each other. If the gap between the end of one interval and the start of the next is less than or equal to the max_spacing value, the two intervals are combined into a single, larger interval. This process is repeated until no more intervals can be grouped.\nThis is particularly useful for cleaning up data where a single continuous event might be recorded as multiple, closely-spaced but separate intervals due to noise or transient interruptions in signal detection.\n\n\n\n\nIn neuroscience, this transform can be applied to refine event data:\n\nBout Analysis: When analyzing behaviors that occur in bouts (e.g., sniffing, grooming, vocalizations), this transform can merge closely spaced, individual instances into a single, continuous behavioral bout. For example, a series of short sniffs could be grouped into one “sniffing bout”.",
    "crumbs": [
      "Data Transformations",
      "Digital Interval Group"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/digital_interval_group.html#overview",
    "href": "user_guide/data_transformations/digital_interval_group.html#overview",
    "title": "Digital Interval Group",
    "section": "",
    "text": "This transform groups nearby intervals in a digital interval series based on a specified maximum spacing, which is useful for consolidating fragmented temporal events.\n\n\nThis transform merges digital intervals that are close to each other. If the gap between the end of one interval and the start of the next is less than or equal to the max_spacing value, the two intervals are combined into a single, larger interval. This process is repeated until no more intervals can be grouped.\nThis is particularly useful for cleaning up data where a single continuous event might be recorded as multiple, closely-spaced but separate intervals due to noise or transient interruptions in signal detection.\n\n\n\n\nIn neuroscience, this transform can be applied to refine event data:\n\nBout Analysis: When analyzing behaviors that occur in bouts (e.g., sniffing, grooming, vocalizations), this transform can merge closely spaced, individual instances into a single, continuous behavioral bout. For example, a series of short sniffs could be grouped into one “sniffing bout”.",
    "crumbs": [
      "Data Transformations",
      "Digital Interval Group"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/digital_interval_group.html#parameters",
    "href": "user_guide/data_transformations/digital_interval_group.html#parameters",
    "title": "Digital Interval Group",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameter:\n\nmax_spacing: The maximum allowed gap between two consecutive intervals for them to be merged. The time unit should be consistent with the data’s time representation (e.g., seconds or frames). If the gap is greater than this value, the intervals remain separate.",
    "crumbs": [
      "Data Transformations",
      "Digital Interval Group"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/digital_interval_group.html#example-configuration",
    "href": "user_guide/data_transformations/digital_interval_group.html#example-configuration",
    "title": "Digital Interval Group",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example groups intervals that are separated by 3.0 time units or less.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Interval Grouping Pipeline\",\n            \"description\": \"Test interval grouping on digital interval series\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Group Intervals\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_intervals\",\n                \"output_key\": \"grouped_intervals\",\n                \"parameters\": {\n                    \"max_spacing\": 3.0\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Digital Interval Group"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_point_from_line.html",
    "href": "user_guide/data_transformations/extract_point_from_line.html",
    "title": "Extract Point from Line",
    "section": "",
    "text": "This transform extracts a specific point from a line at a given fractional position. This is useful for isolating a single point of interest along a tracked object, such as the tip, base, or midpoint of a whisker or limb.\n\n\nThis operation pinpoints and extracts the coordinates of a single point along a line for each time step. The position of the point is determined by a fractional value from 0.0 (the start of the line) to 1.0 (the end of the line). Two different methods are available for extracting the point:\n\nDirect Method: This method calculates the cumulative length of the line’s segments and finds the point that lies at the specified fractional distance along this length. If use_interpolation is enabled, it will linearly interpolate between two adjacent vertices of the line to find the exact position. If disabled, it will select the nearest vertex.\nParametric Method: This method fits a polynomial to the line’s x and y coordinates separately as a function of the parametric distance along the line. It then evaluates the polynomial at the specified fractional position to determine the point’s coordinates. This can be useful for smoothing out irregularities in the line.\n\nThe output is a PointData object containing the extracted point for each time step.\n\n\n\n\nThis transformation is particularly useful for detailed analysis of movement and morphology:\n\nWhisker Analysis: Researchers can extract the tip (position 1.0), base (position 0.0), or any other consistent point along a tracked whisker. This allows for precise analysis of whisker contact, bending, or movement relative to other objects or whiskers.\nLimb Tracking: In studies of locomotion or reaching, this transform can isolate specific points on a limb, such as the endpoint (e.g., a hand or paw) or a joint (e.g., an elbow), to analyze trajectories and kinematics.\nMorphological Measurement: For organisms that change shape, like larvae or worms, this transform can be used to track specific points on the body midline to quantify bending or undulation.",
    "crumbs": [
      "Data Transformations",
      "Extract Point from Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_point_from_line.html#overview",
    "href": "user_guide/data_transformations/extract_point_from_line.html#overview",
    "title": "Extract Point from Line",
    "section": "",
    "text": "This transform extracts a specific point from a line at a given fractional position. This is useful for isolating a single point of interest along a tracked object, such as the tip, base, or midpoint of a whisker or limb.\n\n\nThis operation pinpoints and extracts the coordinates of a single point along a line for each time step. The position of the point is determined by a fractional value from 0.0 (the start of the line) to 1.0 (the end of the line). Two different methods are available for extracting the point:\n\nDirect Method: This method calculates the cumulative length of the line’s segments and finds the point that lies at the specified fractional distance along this length. If use_interpolation is enabled, it will linearly interpolate between two adjacent vertices of the line to find the exact position. If disabled, it will select the nearest vertex.\nParametric Method: This method fits a polynomial to the line’s x and y coordinates separately as a function of the parametric distance along the line. It then evaluates the polynomial at the specified fractional position to determine the point’s coordinates. This can be useful for smoothing out irregularities in the line.\n\nThe output is a PointData object containing the extracted point for each time step.\n\n\n\n\nThis transformation is particularly useful for detailed analysis of movement and morphology:\n\nWhisker Analysis: Researchers can extract the tip (position 1.0), base (position 0.0), or any other consistent point along a tracked whisker. This allows for precise analysis of whisker contact, bending, or movement relative to other objects or whiskers.\nLimb Tracking: In studies of locomotion or reaching, this transform can isolate specific points on a limb, such as the endpoint (e.g., a hand or paw) or a joint (e.g., an elbow), to analyze trajectories and kinematics.\nMorphological Measurement: For organisms that change shape, like larvae or worms, this transform can be used to track specific points on the body midline to quantify bending or undulation.",
    "crumbs": [
      "Data Transformations",
      "Extract Point from Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_point_from_line.html#parameters",
    "href": "user_guide/data_transformations/extract_point_from_line.html#parameters",
    "title": "Extract Point from Line",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nposition (float): The fractional distance along the line where the point should be extracted. Must be between 0.0 (start) and 1.0 (end).\nmethod (string): The method to use for point extraction. Can be either \"Direct\" or \"Parametric\".\npolynomial_order (integer, optional): The order of the polynomial to fit to the line if using the Parametric method. Defaults to 3.\nuse_interpolation (boolean, optional): Whether to use linear interpolation to find the exact point when using the Direct method. If false, the nearest vertex on the line is returned. Defaults to true.",
    "crumbs": [
      "Data Transformations",
      "Extract Point from Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_point_from_line.html#example-configuration",
    "href": "user_guide/data_transformations/extract_point_from_line.html#example-configuration",
    "title": "Extract Point from Line",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example extracts the point at the 75% position along a line stored with the key whisker_1.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Whisker Tip Extraction Pipeline\",\n            \"description\": \"Extracts the tip of a whisker from line data.\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Extract Point from Line\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"whisker_1\",\n                \"output_key\": \"whisker_1_tip\",\n                \"parameters\": {\n                    \"position\": 0.75,\n                    \"method\": \"Direct\",\n                    \"use_interpolation\": true\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Extract Point from Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_clip.html",
    "href": "user_guide/data_transformations/line_clip.html",
    "title": "Line Clip by Reference Line",
    "section": "",
    "text": "This transform clips a set of lines based on where they intersect with a single, user-defined reference line.\n\n\nThis transformation is used to shorten or trim lines at the point where they cross a specified reference line. For each line in the input data, the algorithm finds the first point of intersection with the reference line. Based on the clip_side parameter, it then keeps either the segment of the line from its start to the intersection point (KeepBase) or the segment from the intersection point to its end (KeepDistal).\nIf a line does not intersect the reference line, it remains unchanged in the output. This is useful for standardizing line data, for example, by trimming all lines to a common boundary.\n\n\n\n\n\nStandardizing Whisker Tracking Data: In neuroscience experiments involving whisker tracking, it’s often necessary to analyze the whisker’s movement relative to a fixed point, like the edge of a pole or object. This transform can clip all tracked whisker lines at the boundary of that object, ensuring that only the relevant segment of the whisker is considered for analysis.\nAnalyzing Neurite Outgrowth: When studying neurite or axon growth in microscopy images, researchers might want to measure growth only up to a certain landmark or boundary. This transform can be used to clip the traced neurites at that boundary.\nPath Analysis in Behavioral Studies: In studies of animal behavior, if an animal’s path is tracked, this transform could be used to clip the path at the entrance to a specific zone or maze arm, isolating the behavior within or outside that area.",
    "crumbs": [
      "Data Transformations",
      "Line Clip by Reference Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_clip.html#overview",
    "href": "user_guide/data_transformations/line_clip.html#overview",
    "title": "Line Clip by Reference Line",
    "section": "",
    "text": "This transform clips a set of lines based on where they intersect with a single, user-defined reference line.\n\n\nThis transformation is used to shorten or trim lines at the point where they cross a specified reference line. For each line in the input data, the algorithm finds the first point of intersection with the reference line. Based on the clip_side parameter, it then keeps either the segment of the line from its start to the intersection point (KeepBase) or the segment from the intersection point to its end (KeepDistal).\nIf a line does not intersect the reference line, it remains unchanged in the output. This is useful for standardizing line data, for example, by trimming all lines to a common boundary.\n\n\n\n\n\nStandardizing Whisker Tracking Data: In neuroscience experiments involving whisker tracking, it’s often necessary to analyze the whisker’s movement relative to a fixed point, like the edge of a pole or object. This transform can clip all tracked whisker lines at the boundary of that object, ensuring that only the relevant segment of the whisker is considered for analysis.\nAnalyzing Neurite Outgrowth: When studying neurite or axon growth in microscopy images, researchers might want to measure growth only up to a certain landmark or boundary. This transform can be used to clip the traced neurites at that boundary.\nPath Analysis in Behavioral Studies: In studies of animal behavior, if an animal’s path is tracked, this transform could be used to clip the path at the entrance to a specific zone or maze arm, isolating the behavior within or outside that area.",
    "crumbs": [
      "Data Transformations",
      "Line Clip by Reference Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_clip.html#parameters",
    "href": "user_guide/data_transformations/line_clip.html#parameters",
    "title": "Line Clip by Reference Line",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nreference_line_data_name: The key identifying the LineData object in the data manager that will be used as the reference for clipping.\nreference_frame: The specific time frame within the reference line data to use for clipping. All lines in the input data will be clipped against this single frame of the reference line.\nclip_side: Determines which part of the line to keep after clipping.\n\nKeepBase: Keeps the portion of the line from its starting point to the intersection point.\nKeepDistal: Keeps the portion of the line from the intersection point to its end point.",
    "crumbs": [
      "Data Transformations",
      "Line Clip by Reference Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_clip.html#example-configuration",
    "href": "user_guide/data_transformations/line_clip.html#example-configuration",
    "title": "Line Clip by Reference Line",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example clips a set of lines against a reference line, keeping the base segment of each line.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Line Clip Pipeline\",\n            \"description\": \"Test line clipping on line data\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Clip Line by Reference Line\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_line\",\n                \"output_key\": \"clipped_lines\",\n                \"parameters\": {\n                    \"reference_line_data_name\": \"reference_line\",\n                    \"reference_frame\": 0,\n                    \"clip_side\": \"KeepBase\"\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Line Clip by Reference Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_resample.html",
    "href": "user_guide/data_transformations/line_resample.html",
    "title": "Line Resample",
    "section": "",
    "text": "This transform resamples a line to either simplify its geometry or to ensure it has points that are evenly spaced.\n\n\nLine resampling is a common operation in data processing, used to either reduce the number of points in a line (simplification) or to create a new line with a specific number of points or spacing. This transform provides two different algorithms for resampling:\n\nFixed Spacing: This algorithm creates a new line with points that are a fixed distance apart. This is useful for standardizing the number of points in a line, or for preparing a line for further analysis that requires a uniform distribution of points.\nDouglas-Peucker: This algorithm simplifies a line by removing points that are close to the line segment. This is useful for reducing the complexity of a line while preserving its overall shape.\n\n\n\n\n\nIn neuroscience, line resampling can be used in a variety of applications:\n\nWhisker Tracking: When tracking the movement of a whisker, the output is often a line with a variable number of points. Resampling the line to a fixed number of points can be useful for standardizing the data for further analysis, such as calculating the curvature or angle of the whisker over time.\nDendrite Tracing: When tracing the path of a dendrite from a neuron, the resulting line can be very complex. The Douglas-Peucker algorithm can be used to simplify the line, making it easier to visualize and analyze the overall shape of the dendrite.\nAnimal Tracking: When tracking the path of an animal in an open field, the resulting line can be very long and detailed. Resampling the line can help to smooth out the path and reduce the amount of data that needs to be processed.",
    "crumbs": [
      "Data Transformations",
      "Line Resample"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_resample.html#overview",
    "href": "user_guide/data_transformations/line_resample.html#overview",
    "title": "Line Resample",
    "section": "",
    "text": "This transform resamples a line to either simplify its geometry or to ensure it has points that are evenly spaced.\n\n\nLine resampling is a common operation in data processing, used to either reduce the number of points in a line (simplification) or to create a new line with a specific number of points or spacing. This transform provides two different algorithms for resampling:\n\nFixed Spacing: This algorithm creates a new line with points that are a fixed distance apart. This is useful for standardizing the number of points in a line, or for preparing a line for further analysis that requires a uniform distribution of points.\nDouglas-Peucker: This algorithm simplifies a line by removing points that are close to the line segment. This is useful for reducing the complexity of a line while preserving its overall shape.\n\n\n\n\n\nIn neuroscience, line resampling can be used in a variety of applications:\n\nWhisker Tracking: When tracking the movement of a whisker, the output is often a line with a variable number of points. Resampling the line to a fixed number of points can be useful for standardizing the data for further analysis, such as calculating the curvature or angle of the whisker over time.\nDendrite Tracing: When tracing the path of a dendrite from a neuron, the resulting line can be very complex. The Douglas-Peucker algorithm can be used to simplify the line, making it easier to visualize and analyze the overall shape of the dendrite.\nAnimal Tracking: When tracking the path of an animal in an open field, the resulting line can be very long and detailed. Resampling the line can help to smooth out the path and reduce the amount of data that needs to be processed.",
    "crumbs": [
      "Data Transformations",
      "Line Resample"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_resample.html#parameters",
    "href": "user_guide/data_transformations/line_resample.html#parameters",
    "title": "Line Resample",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nalgorithm: The resampling algorithm to use. This can be one of the following:\n\nFixedSpacing: Resamples the line to have points that are a fixed distance apart.\nDouglasPeucker: Simplifies the line using the Douglas-Peucker algorithm.\n\ntarget_spacing: The desired distance between points in the resampled line. This parameter is only used when the algorithm is set to FixedSpacing.\nepsilon: The tolerance for the Douglas-Peucker algorithm. This parameter is only used when the algorithm is set to DouglasPeucker. A larger value of epsilon will result in a more simplified line.",
    "crumbs": [
      "Data Transformations",
      "Line Resample"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_resample.html#example-configuration",
    "href": "user_guide/data_transformations/line_resample.html#example-configuration",
    "title": "Line Resample",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example resamples a line to have points that are 15 pixels apart.\n[\n    {\n        \"transformations\": {\n            \"metadata\": {\n                \"name\": \"Line Resample Pipeline\",\n                \"description\": \"Test line resampling on line data\",\n                \"version\": \"1.0\"\n            },\n            \"steps\": [\n                {\n                    \"step_id\": \"1\",\n                    \"transform_name\": \"Resample Line\",\n                    \"phase\": \"analysis\",\n                    \"input_key\": \"test_lines\",\n                    \"output_key\": \"resampled_lines\",\n                    \"parameters\": {\n                        \"algorithm\": \"FixedSpacing\",\n                        \"target_spacing\": 15.0,\n                        \"epsilon\": 2.0\n                    }\n                }\n            ]\n        }\n    }\n]",
    "crumbs": [
      "Data Transformations",
      "Line Resample"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/overview.html",
    "href": "user_guide/data_transformations/overview.html",
    "title": "Data Transformations",
    "section": "",
    "text": "Data transformations are operations we perform on one kind of data that result in another kind of data.\n\n\n\nFigure: 1) Feature select from available loaded data 2) Tranformation select Dialog box; 3) Output name of new data created by transformation; 4) Execution Button; 5) Parameter window for selected transformation",
    "crumbs": [
      "Data Transformations",
      "Data Transformations"
    ]
  },
  {
    "objectID": "user_guide/machine_learning/ML_intro.html",
    "href": "user_guide/machine_learning/ML_intro.html",
    "title": "Overview",
    "section": "",
    "text": "The Machine Learning Widget allows us to fit models of relationships between multiple features of our data and then make predictions. This can be useful for semi-automated annotation of datasets, where the user labels some training data, and then predicts the remaining unlabeled frames. Then annotations can then be easily compared with the video data.",
    "crumbs": [
      "Machine Learning",
      "Overview"
    ]
  }
]
[
  {
    "objectID": "user_guide/media_viewer/points.html",
    "href": "user_guide/media_viewer/points.html",
    "title": "Point Data",
    "section": "",
    "text": "The Points widget appears when Points are enabled in the Feature Tree. It’s designed for viewing, styling, adding, and editing keypoints visible in the Media Viewer.\nIf you haven’t loaded a keypoint dataset, you can still add keypoints; these changes will be tracked in the Data Manager (see Data Manager → points [link user-ref here]).\n\n\n\n\n\n\nNote\n\n\n\nSaving and Export: Any additions or edits to keypoints are recorded by the Data Manager and can be exported later as part of your dataset.\n\n\n\n\n\n\n\nRGB (0–255): Enter the Red, Green, and Blue channels to set the keypoint color.\n\nHex Color: Optionally specify a color using a hex code (e.g., #FF6600).\n\nColor Preview: Shows the current color selection.\n\nAlpha: Controls opacity/transparency of the keypoint in the Media Viewer (higher = more opaque).\n\n\n\n\n\nPoint Size: Sets how large the keypoint appears in the Media Viewer.\n\n\n\n\n\n\nNote\n\n\n\nData size vs. display size: The stored keypoint in the Data Manager is always one pixel (a single coordinate). Point Size/Shape only affects on-screen display, not the underlying saved data.\n\n\nMarker Shape: Choose how the keypoint is drawn in the display:\n\ncircle, square, triangle, cross, x, diamond\n\n\n\n\n\n\n\nIn the Points widget, find Mouse Mode (dropdown).\nSelect Select Point.\nClick anywhere in the Media Viewer to add a keypoint at that location.\nTo modify a keypoint, use the same mode to select it and then adjust its location in the media window.\n\n\n\n\n\n\nNote\n\n\n\nDeleting Keypoints: To delete accidentally added or misplaced keypoints, open the data manager and find the frame that contains the keypoint and delete it (see Data Manager → points [link user-ref here]).\n\n\n\n\n\n\n\nKeep Alpha low to ensure that imagery remains visible under points.\n\nWhen working with multiple keypoints, use distinct shapes or colors to differentiate labels, annotators, or classes.",
    "crumbs": [
      "Media Viewer",
      "Point Data"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/points.html#widget-layout-controls",
    "href": "user_guide/media_viewer/points.html#widget-layout-controls",
    "title": "Point Data",
    "section": "",
    "text": "RGB (0–255): Enter the Red, Green, and Blue channels to set the keypoint color.\n\nHex Color: Optionally specify a color using a hex code (e.g., #FF6600).\n\nColor Preview: Shows the current color selection.\n\nAlpha: Controls opacity/transparency of the keypoint in the Media Viewer (higher = more opaque).\n\n\n\n\n\nPoint Size: Sets how large the keypoint appears in the Media Viewer.\n\n\n\n\n\n\nNote\n\n\n\nData size vs. display size: The stored keypoint in the Data Manager is always one pixel (a single coordinate). Point Size/Shape only affects on-screen display, not the underlying saved data.\n\n\nMarker Shape: Choose how the keypoint is drawn in the display:\n\ncircle, square, triangle, cross, x, diamond",
    "crumbs": [
      "Media Viewer",
      "Point Data"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/points.html#adding-editing-keypoints",
    "href": "user_guide/media_viewer/points.html#adding-editing-keypoints",
    "title": "Point Data",
    "section": "",
    "text": "In the Points widget, find Mouse Mode (dropdown).\nSelect Select Point.\nClick anywhere in the Media Viewer to add a keypoint at that location.\nTo modify a keypoint, use the same mode to select it and then adjust its location in the media window.\n\n\n\n\n\n\nNote\n\n\n\nDeleting Keypoints: To delete accidentally added or misplaced keypoints, open the data manager and find the frame that contains the keypoint and delete it (see Data Manager → points [link user-ref here]).",
    "crumbs": [
      "Media Viewer",
      "Point Data"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/points.html#tips",
    "href": "user_guide/media_viewer/points.html#tips",
    "title": "Point Data",
    "section": "",
    "text": "Keep Alpha low to ensure that imagery remains visible under points.\n\nWhen working with multiple keypoints, use distinct shapes or colors to differentiate labels, annotators, or classes.",
    "crumbs": [
      "Media Viewer",
      "Point Data"
    ]
  },
  {
    "objectID": "user_guide/index.html",
    "href": "user_guide/index.html",
    "title": "User Guide",
    "section": "",
    "text": "This is the user guide for neuralyzer",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "user_guide/image_processing/contrast.html",
    "href": "user_guide/image_processing/contrast.html",
    "title": "Contrast and Brightness Adjustment",
    "section": "",
    "text": "These two filters are implemented with a single linear transform taking parameters “alpha” (\\(\\alpha\\)) and “beta” (\\(\\beta\\)). Pixel values are multiplied by \\(\\alpha\\) to adjust contrast, then have \\(\\beta\\) added to adjust brightness: \\[\ng(x) = \\alpha f(x) + \\beta\n\\] In effect the magnitude of \\(\\alpha\\) corresponds to the amount of contrast and the magnitude of \\(\\beta\\) corresponds to the amount of brightness.",
    "crumbs": [
      "Image Processing",
      "Contrast and Brightness Adjustment"
    ]
  },
  {
    "objectID": "user_guide/image_processing/bilateral_filter.html",
    "href": "user_guide/image_processing/bilateral_filter.html",
    "title": "Bilateral Filter",
    "section": "",
    "text": "https://docs.opencv.org/4.x/js_filtering_bilateralFilter.html\nhttps://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga9d7064d478c95d60003cf839430737ed\nhttps://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html",
    "crumbs": [
      "Image Processing",
      "Bilateral Filter"
    ]
  },
  {
    "objectID": "user_guide/image_processing/bilateral_filter.html#references",
    "href": "user_guide/image_processing/bilateral_filter.html#references",
    "title": "Bilateral Filter",
    "section": "",
    "text": "https://docs.opencv.org/4.x/js_filtering_bilateralFilter.html\nhttps://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga9d7064d478c95d60003cf839430737ed\nhttps://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html",
    "crumbs": [
      "Image Processing",
      "Bilateral Filter"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/remove_small_connected_components.html",
    "href": "user_guide/data_transformations/remove_small_connected_components.html",
    "title": "Remove Small Connected Components",
    "section": "",
    "text": "This transform removes small, isolated groups of pixels (connected components) from a binary mask, which is useful for cleaning up noise or irrelevant objects.\n\n\nIn image analysis, particularly with binary masks, it’s common to have small, spurious regions of interest that are not relevant to the main analysis. This transform uses a connected components algorithm to identify all distinct, contiguous regions of pixels in the mask. It then filters out any region whose total number of pixels is below a specified threshold.\nThis process helps to isolate and preserve only the most significant structures in the mask, making subsequent analysis, such as object tracking or morphological measurement, more robust. The connectivity analysis considers pixels to be connected if they touch at their edges or corners (8-connectivity).\nThis transform takes a mask as input and produces a new, cleaned-up mask as output.\n\n\n\n\n\nCalcium Imaging Cleanup: In two-photon or wide-field calcium imaging, masks representing active neurons or regions of interest (ROIs) can be noisy. This transform can remove small, spurious signals that are unlikely to be true neuronal activity, ensuring that only significant ROIs are kept for analysis.\nBehavioral Tracking: When tracking an animal’s position or posture using video, the resulting binary masks can contain noise from lighting changes or background artifacts. Removing small connected components can clean up the mask of the animal, leading to more accurate tracking of its centroid or body shape.\nHistology and Morphology: When analyzing images of tissue slices, automated segmentation can produce small, artifactual regions. This transform can be used to filter out these artifacts, leaving only the larger, more relevant structures like cell bodies or labeled tracts.",
    "crumbs": [
      "Data Transformations",
      "Remove Small Connected Components"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/remove_small_connected_components.html#overview",
    "href": "user_guide/data_transformations/remove_small_connected_components.html#overview",
    "title": "Remove Small Connected Components",
    "section": "",
    "text": "This transform removes small, isolated groups of pixels (connected components) from a binary mask, which is useful for cleaning up noise or irrelevant objects.\n\n\nIn image analysis, particularly with binary masks, it’s common to have small, spurious regions of interest that are not relevant to the main analysis. This transform uses a connected components algorithm to identify all distinct, contiguous regions of pixels in the mask. It then filters out any region whose total number of pixels is below a specified threshold.\nThis process helps to isolate and preserve only the most significant structures in the mask, making subsequent analysis, such as object tracking or morphological measurement, more robust. The connectivity analysis considers pixels to be connected if they touch at their edges or corners (8-connectivity).\nThis transform takes a mask as input and produces a new, cleaned-up mask as output.\n\n\n\n\n\nCalcium Imaging Cleanup: In two-photon or wide-field calcium imaging, masks representing active neurons or regions of interest (ROIs) can be noisy. This transform can remove small, spurious signals that are unlikely to be true neuronal activity, ensuring that only significant ROIs are kept for analysis.\nBehavioral Tracking: When tracking an animal’s position or posture using video, the resulting binary masks can contain noise from lighting changes or background artifacts. Removing small connected components can clean up the mask of the animal, leading to more accurate tracking of its centroid or body shape.\nHistology and Morphology: When analyzing images of tissue slices, automated segmentation can produce small, artifactual regions. This transform can be used to filter out these artifacts, leaving only the larger, more relevant structures like cell bodies or labeled tracts.",
    "crumbs": [
      "Data Transformations",
      "Remove Small Connected Components"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/remove_small_connected_components.html#parameters",
    "href": "user_guide/data_transformations/remove_small_connected_components.html#parameters",
    "title": "Remove Small Connected Components",
    "section": "Parameters",
    "text": "Parameters\nThis transform has a single parameter:\n\nthreshold: An integer value that specifies the minimum number of pixels a connected component must have to be preserved. Any component with a pixel count smaller than this value will be removed from the mask.",
    "crumbs": [
      "Data Transformations",
      "Remove Small Connected Components"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/remove_small_connected_components.html#example-configuration",
    "href": "user_guide/data_transformations/remove_small_connected_components.html#example-configuration",
    "title": "Remove Small Connected Components",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example loads a mask, removes all connected components with fewer than 3 pixels, and saves the result.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Mask Connected Component Pipeline\",\n            \"description\": \"Test connected component analysis on mask data\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Remove Small Connected Components\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_mask\",\n                \"output_key\": \"filtered_mask\",\n                \"parameters\": {\n                    \"threshold\": 3\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Remove Small Connected Components"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_centroid.html",
    "href": "user_guide/data_transformations/mask_centroid.html",
    "title": "Calculate Mask Centroid",
    "section": "",
    "text": "This transform calculates the centroid (geometric center) for each mask in a dataset.\n\n\nThis transformation processes mask data, which represents regions of interest, and for each individual mask at each point in time, it computes the center of mass. The output is a series of points, where each point represents the calculated centroid of a corresponding mask. This is a common operation for summarizing a spatial distribution of pixels with a single representative point.\nThe calculation is purely geometric; it finds the average of the x and y coordinates of all the points within the mask. This means that for some non-convex (hollow or U-shaped) masks, the centroid can fall outside the mask itself, as shown in the examples below.\n\n\n\n\n\nTracking Cell Bodies: If a mask outlines a neuron’s soma, its centroid provides a consistent point for tracking the cell’s position over time, even if the cell’s shape changes slightly.\nAnalyzing Animal Behavior: When tracking an animal in a video, a mask might be generated around the animal’s body. The centroid of this mask can serve as a simplified representation of the animal’s location for trajectory analysis.\nQuantifying Limb Position: For masks drawn around an animal’s limbs, the centroid can be used to approximate the position of the limb for kinematic studies.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Centroid"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_centroid.html#overview",
    "href": "user_guide/data_transformations/mask_centroid.html#overview",
    "title": "Calculate Mask Centroid",
    "section": "",
    "text": "This transform calculates the centroid (geometric center) for each mask in a dataset.\n\n\nThis transformation processes mask data, which represents regions of interest, and for each individual mask at each point in time, it computes the center of mass. The output is a series of points, where each point represents the calculated centroid of a corresponding mask. This is a common operation for summarizing a spatial distribution of pixels with a single representative point.\nThe calculation is purely geometric; it finds the average of the x and y coordinates of all the points within the mask. This means that for some non-convex (hollow or U-shaped) masks, the centroid can fall outside the mask itself, as shown in the examples below.\n\n\n\n\n\nTracking Cell Bodies: If a mask outlines a neuron’s soma, its centroid provides a consistent point for tracking the cell’s position over time, even if the cell’s shape changes slightly.\nAnalyzing Animal Behavior: When tracking an animal in a video, a mask might be generated around the animal’s body. The centroid of this mask can serve as a simplified representation of the animal’s location for trajectory analysis.\nQuantifying Limb Position: For masks drawn around an animal’s limbs, the centroid can be used to approximate the position of the limb for kinematic studies.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Centroid"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_centroid.html#parameters",
    "href": "user_guide/data_transformations/mask_centroid.html#parameters",
    "title": "Calculate Mask Centroid",
    "section": "Parameters",
    "text": "Parameters\nThis transform does not have any configurable parameters. It calculates the standard geometric centroid for each mask.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Centroid"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_centroid.html#example-configuration",
    "href": "user_guide/data_transformations/mask_centroid.html#example-configuration",
    "title": "Calculate Mask Centroid",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Mask Centroid Pipeline\",\n            \"description\": \"Test mask centroid calculation on mask data\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Calculate Mask Centroid\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_masks\",\n                \"output_key\": \"mask_centroids\",\n                \"parameters\": {}\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Centroid"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_resample.html",
    "href": "user_guide/data_transformations/line_resample.html",
    "title": "Line Resample",
    "section": "",
    "text": "This transform resamples a line to either simplify its geometry or to ensure it has points that are evenly spaced.\n\n\nLine resampling is a common operation in data processing, used to either reduce the number of points in a line (simplification) or to create a new line with a specific number of points or spacing. This transform provides two different algorithms for resampling:\n\nFixed Spacing: This algorithm creates a new line with points that are a fixed distance apart. This is useful for standardizing the number of points in a line, or for preparing a line for further analysis that requires a uniform distribution of points.\nDouglas-Peucker: This algorithm simplifies a line by removing points that are close to the line segment. This is useful for reducing the complexity of a line while preserving its overall shape.\n\n\n\n\n\nIn neuroscience, line resampling can be used in a variety of applications:\n\nWhisker Tracking: When tracking the movement of a whisker, the output is often a line with a variable number of points. Resampling the line to a fixed number of points can be useful for standardizing the data for further analysis, such as calculating the curvature or angle of the whisker over time.\nDendrite Tracing: When tracing the path of a dendrite from a neuron, the resulting line can be very complex. The Douglas-Peucker algorithm can be used to simplify the line, making it easier to visualize and analyze the overall shape of the dendrite.\nAnimal Tracking: When tracking the path of an animal in an open field, the resulting line can be very long and detailed. Resampling the line can help to smooth out the path and reduce the amount of data that needs to be processed.",
    "crumbs": [
      "Data Transformations",
      "Line Resample"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_resample.html#overview",
    "href": "user_guide/data_transformations/line_resample.html#overview",
    "title": "Line Resample",
    "section": "",
    "text": "This transform resamples a line to either simplify its geometry or to ensure it has points that are evenly spaced.\n\n\nLine resampling is a common operation in data processing, used to either reduce the number of points in a line (simplification) or to create a new line with a specific number of points or spacing. This transform provides two different algorithms for resampling:\n\nFixed Spacing: This algorithm creates a new line with points that are a fixed distance apart. This is useful for standardizing the number of points in a line, or for preparing a line for further analysis that requires a uniform distribution of points.\nDouglas-Peucker: This algorithm simplifies a line by removing points that are close to the line segment. This is useful for reducing the complexity of a line while preserving its overall shape.\n\n\n\n\n\nIn neuroscience, line resampling can be used in a variety of applications:\n\nWhisker Tracking: When tracking the movement of a whisker, the output is often a line with a variable number of points. Resampling the line to a fixed number of points can be useful for standardizing the data for further analysis, such as calculating the curvature or angle of the whisker over time.\nDendrite Tracing: When tracing the path of a dendrite from a neuron, the resulting line can be very complex. The Douglas-Peucker algorithm can be used to simplify the line, making it easier to visualize and analyze the overall shape of the dendrite.\nAnimal Tracking: When tracking the path of an animal in an open field, the resulting line can be very long and detailed. Resampling the line can help to smooth out the path and reduce the amount of data that needs to be processed.",
    "crumbs": [
      "Data Transformations",
      "Line Resample"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_resample.html#parameters",
    "href": "user_guide/data_transformations/line_resample.html#parameters",
    "title": "Line Resample",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nalgorithm: The resampling algorithm to use. This can be one of the following:\n\nFixedSpacing: Resamples the line to have points that are a fixed distance apart.\nDouglasPeucker: Simplifies the line using the Douglas-Peucker algorithm.\n\ntarget_spacing: The desired distance between points in the resampled line. This parameter is only used when the algorithm is set to FixedSpacing.\nepsilon: The tolerance for the Douglas-Peucker algorithm. This parameter is only used when the algorithm is set to DouglasPeucker. A larger value of epsilon will result in a more simplified line.",
    "crumbs": [
      "Data Transformations",
      "Line Resample"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_resample.html#example-configuration",
    "href": "user_guide/data_transformations/line_resample.html#example-configuration",
    "title": "Line Resample",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example resamples a line to have points that are 15 pixels apart.\n[\n    {\n        \"transformations\": {\n            \"metadata\": {\n                \"name\": \"Line Resample Pipeline\",\n                \"description\": \"Test line resampling on line data\",\n                \"version\": \"1.0\"\n            },\n            \"steps\": [\n                {\n                    \"step_id\": \"1\",\n                    \"transform_name\": \"Resample Line\",\n                    \"phase\": \"analysis\",\n                    \"input_key\": \"test_lines\",\n                    \"output_key\": \"resampled_lines\",\n                    \"parameters\": {\n                        \"algorithm\": \"FixedSpacing\",\n                        \"target_spacing\": 15.0,\n                        \"epsilon\": 2.0\n                    }\n                }\n            ]\n        }\n    }\n]",
    "crumbs": [
      "Data Transformations",
      "Line Resample"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_clip.html",
    "href": "user_guide/data_transformations/line_clip.html",
    "title": "Line Clip by Reference Line",
    "section": "",
    "text": "This transform clips a set of lines based on where they intersect with a single, user-defined reference line.\n\n\nThis transformation is used to shorten or trim lines at the point where they cross a specified reference line. For each line in the input data, the algorithm finds the first point of intersection with the reference line. Based on the clip_side parameter, it then keeps either the segment of the line from its start to the intersection point (KeepBase) or the segment from the intersection point to its end (KeepDistal).\nIf a line does not intersect the reference line, it remains unchanged in the output. This is useful for standardizing line data, for example, by trimming all lines to a common boundary.\n\n\n\n\n\nStandardizing Whisker Tracking Data: In neuroscience experiments involving whisker tracking, it’s often necessary to analyze the whisker’s movement relative to a fixed point, like the edge of a pole or object. This transform can clip all tracked whisker lines at the boundary of that object, ensuring that only the relevant segment of the whisker is considered for analysis.\nAnalyzing Neurite Outgrowth: When studying neurite or axon growth in microscopy images, researchers might want to measure growth only up to a certain landmark or boundary. This transform can be used to clip the traced neurites at that boundary.\nPath Analysis in Behavioral Studies: In studies of animal behavior, if an animal’s path is tracked, this transform could be used to clip the path at the entrance to a specific zone or maze arm, isolating the behavior within or outside that area.",
    "crumbs": [
      "Data Transformations",
      "Line Clip by Reference Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_clip.html#overview",
    "href": "user_guide/data_transformations/line_clip.html#overview",
    "title": "Line Clip by Reference Line",
    "section": "",
    "text": "This transform clips a set of lines based on where they intersect with a single, user-defined reference line.\n\n\nThis transformation is used to shorten or trim lines at the point where they cross a specified reference line. For each line in the input data, the algorithm finds the first point of intersection with the reference line. Based on the clip_side parameter, it then keeps either the segment of the line from its start to the intersection point (KeepBase) or the segment from the intersection point to its end (KeepDistal).\nIf a line does not intersect the reference line, it remains unchanged in the output. This is useful for standardizing line data, for example, by trimming all lines to a common boundary.\n\n\n\n\n\nStandardizing Whisker Tracking Data: In neuroscience experiments involving whisker tracking, it’s often necessary to analyze the whisker’s movement relative to a fixed point, like the edge of a pole or object. This transform can clip all tracked whisker lines at the boundary of that object, ensuring that only the relevant segment of the whisker is considered for analysis.\nAnalyzing Neurite Outgrowth: When studying neurite or axon growth in microscopy images, researchers might want to measure growth only up to a certain landmark or boundary. This transform can be used to clip the traced neurites at that boundary.\nPath Analysis in Behavioral Studies: In studies of animal behavior, if an animal’s path is tracked, this transform could be used to clip the path at the entrance to a specific zone or maze arm, isolating the behavior within or outside that area.",
    "crumbs": [
      "Data Transformations",
      "Line Clip by Reference Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_clip.html#parameters",
    "href": "user_guide/data_transformations/line_clip.html#parameters",
    "title": "Line Clip by Reference Line",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nreference_line_data_name: The key identifying the LineData object in the data manager that will be used as the reference for clipping.\nreference_frame: The specific time frame within the reference line data to use for clipping. All lines in the input data will be clipped against this single frame of the reference line.\nclip_side: Determines which part of the line to keep after clipping.\n\nKeepBase: Keeps the portion of the line from its starting point to the intersection point.\nKeepDistal: Keeps the portion of the line from the intersection point to its end point.",
    "crumbs": [
      "Data Transformations",
      "Line Clip by Reference Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_clip.html#example-configuration",
    "href": "user_guide/data_transformations/line_clip.html#example-configuration",
    "title": "Line Clip by Reference Line",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example clips a set of lines against a reference line, keeping the base segment of each line.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Line Clip Pipeline\",\n            \"description\": \"Test line clipping on line data\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Clip Line by Reference Line\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_line\",\n                \"output_key\": \"clipped_lines\",\n                \"parameters\": {\n                    \"reference_line_data_name\": \"reference_line\",\n                    \"reference_frame\": 0,\n                    \"clip_side\": \"KeepBase\"\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Line Clip by Reference Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/fill_mask_holes.html",
    "href": "user_guide/data_transformations/fill_mask_holes.html",
    "title": "Fill Mask Holes",
    "section": "",
    "text": "This transform fills any enclosed holes within binary mask regions, which is useful for creating solid,contiguous masks from outlines or noisy data.\n\n\nIn image analysis, particularly with masks derived from object detection or segmentation, it’s common to have “holes” – background areas completely surrounded by foreground. This transform identifies and fills these holes.\nThe process works by identifying all background regions that are not connected to the edges of the image. Any such isolated background region is considered a hole and is filled in, effectively becoming part of the foreground mask. This is useful for cleaning up masks to ensure that objects are represented as solid shapes, which is often a prerequisite for accurate area, centroid, or morphological analysis.\n\n\n\n\n\nCell Body Segmentation: When segmenting neurons from imaging data, the detected outline of a cell might not be perfectly filled. This transform can be used to fill the interior of the cell body, ensuring it’s a solid mask for subsequent analysis like measuring cell size or fluorescence intensity.\nReceptive Field Mapping: In studies mapping receptive fields, a mask might be generated that outlines the field but contains small gaps or holes. Filling these holes can create a more accurate and solid representation of the receptive field.\nLesion Analysis: When analyzing images of tissue lesions, this transform can help create solid masks of the lesion areas, even if the initial segmentation is imperfect, allowing for more accurate measurement of lesion volume or area.",
    "crumbs": [
      "Data Transformations",
      "Fill Mask Holes"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/fill_mask_holes.html#overview",
    "href": "user_guide/data_transformations/fill_mask_holes.html#overview",
    "title": "Fill Mask Holes",
    "section": "",
    "text": "This transform fills any enclosed holes within binary mask regions, which is useful for creating solid,contiguous masks from outlines or noisy data.\n\n\nIn image analysis, particularly with masks derived from object detection or segmentation, it’s common to have “holes” – background areas completely surrounded by foreground. This transform identifies and fills these holes.\nThe process works by identifying all background regions that are not connected to the edges of the image. Any such isolated background region is considered a hole and is filled in, effectively becoming part of the foreground mask. This is useful for cleaning up masks to ensure that objects are represented as solid shapes, which is often a prerequisite for accurate area, centroid, or morphological analysis.\n\n\n\n\n\nCell Body Segmentation: When segmenting neurons from imaging data, the detected outline of a cell might not be perfectly filled. This transform can be used to fill the interior of the cell body, ensuring it’s a solid mask for subsequent analysis like measuring cell size or fluorescence intensity.\nReceptive Field Mapping: In studies mapping receptive fields, a mask might be generated that outlines the field but contains small gaps or holes. Filling these holes can create a more accurate and solid representation of the receptive field.\nLesion Analysis: When analyzing images of tissue lesions, this transform can help create solid masks of the lesion areas, even if the initial segmentation is imperfect, allowing for more accurate measurement of lesion volume or area.",
    "crumbs": [
      "Data Transformations",
      "Fill Mask Holes"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/fill_mask_holes.html#parameters",
    "href": "user_guide/data_transformations/fill_mask_holes.html#parameters",
    "title": "Fill Mask Holes",
    "section": "Parameters",
    "text": "Parameters\nThis transform has no parameters. It will automatically fill all enclosed holes in the provided masks.",
    "crumbs": [
      "Data Transformations",
      "Fill Mask Holes"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/fill_mask_holes.html#example-configuration",
    "href": "user_guide/data_transformations/fill_mask_holes.html#example-configuration",
    "title": "Fill Mask Holes",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Mask Hole Filling Pipeline\",\n            \"description\": \"Test mask hole filling on hollow rectangle\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Fill Mask Holes\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_mask\",\n                \"output_key\": \"filled_mask\",\n                \"parameters\": {}\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Fill Mask Holes"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_line_subsegment.html",
    "href": "user_guide/data_transformations/extract_line_subsegment.html",
    "title": "Extract Line Subsegment",
    "section": "",
    "text": "This transform extracts a portion of a line (a subsegment) based on specified start and end points.\n\n\nThis transformation allows for the precise extraction of a subsegment from a series of lines, such as those representing the movement of a tracked object over time. You can define the start and end of the subsegment as a fraction of the total line length. For example, you can extract the middle 50% of a line.\nThis is particularly useful for focusing analysis on a specific part of a trajectory or shape. For instance, in neuroscience, you might use this to analyze the movement of an animal’s limb, focusing only on the part of the movement where the limb is closest to a target.\n\nThe extraction can be done in two ways:\n\nDirect: This method pulls the original points from the line that fall within the specified subsegment. This is fast and preserves the original data points.\nParametric: This method fits a polynomial to the line and then generates a new, smooth subsegment with a specified number of points. This is useful for standardizing the length and spacing of the subsegment for further analysis, such as comparing shapes across different trials.\n\n\n\n\n\nWhisker Tracking: When analyzing whisker movements, you might want to extract only the portion of the whisker closest to an object it’s exploring.\nLimb Tracking: In studies of motor control, you could extract the segment of a limb’s trajectory during a specific phase of a reach-to-grasp movement.\nNeural Arborization: When analyzing the structure of neurons, this transform could be used to isolate specific dendritic or axonal segments for morphological analysis.",
    "crumbs": [
      "Data Transformations",
      "Extract Line Subsegment"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_line_subsegment.html#overview",
    "href": "user_guide/data_transformations/extract_line_subsegment.html#overview",
    "title": "Extract Line Subsegment",
    "section": "",
    "text": "This transform extracts a portion of a line (a subsegment) based on specified start and end points.\n\n\nThis transformation allows for the precise extraction of a subsegment from a series of lines, such as those representing the movement of a tracked object over time. You can define the start and end of the subsegment as a fraction of the total line length. For example, you can extract the middle 50% of a line.\nThis is particularly useful for focusing analysis on a specific part of a trajectory or shape. For instance, in neuroscience, you might use this to analyze the movement of an animal’s limb, focusing only on the part of the movement where the limb is closest to a target.\n\nThe extraction can be done in two ways:\n\nDirect: This method pulls the original points from the line that fall within the specified subsegment. This is fast and preserves the original data points.\nParametric: This method fits a polynomial to the line and then generates a new, smooth subsegment with a specified number of points. This is useful for standardizing the length and spacing of the subsegment for further analysis, such as comparing shapes across different trials.\n\n\n\n\n\nWhisker Tracking: When analyzing whisker movements, you might want to extract only the portion of the whisker closest to an object it’s exploring.\nLimb Tracking: In studies of motor control, you could extract the segment of a limb’s trajectory during a specific phase of a reach-to-grasp movement.\nNeural Arborization: When analyzing the structure of neurons, this transform could be used to isolate specific dendritic or axonal segments for morphological analysis.",
    "crumbs": [
      "Data Transformations",
      "Extract Line Subsegment"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_line_subsegment.html#parameters",
    "href": "user_guide/data_transformations/extract_line_subsegment.html#parameters",
    "title": "Extract Line Subsegment",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nstart_position: The starting point of the subsegment, as a fraction of the total line length (from 0.0 to 1.0).\nend_position: The ending point of the subsegment, as a fraction of the total line length (from 0.0 to 1.0).\nmethod: The extraction method to use. This can be:\n\nDirect: Directly extracts the original points within the subsegment range.\nParametric: Uses polynomial interpolation to create a new, smooth subsegment.\n\npolynomial_order: (Parametric method only) The order of the polynomial to fit to the line. A higher order can capture more complex curves but may also be more sensitive to noise.\noutput_points: (Parametric method only) The number of points to generate in the output subsegment.\npreserve_original_spacing: (Direct method only) If true, the subsegment will consist of the original points from the line that fall within the specified range. If false, the subsegment will be resampled to have evenly spaced points.",
    "crumbs": [
      "Data Transformations",
      "Extract Line Subsegment"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_line_subsegment.html#example-configuration",
    "href": "user_guide/data_transformations/extract_line_subsegment.html#example-configuration",
    "title": "Extract Line Subsegment",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example extracts the subsegment from 20% to 80% of the line’s length using the Direct method.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Line Subsegment Extraction Pipeline\",\n            \"description\": \"Test line subsegment extraction\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Extract Line Subsegment\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_line\",\n                \"output_key\": \"extracted_subsegments\",\n                \"parameters\": {\n                    \"start_position\": 0.2,\n                    \"end_position\": 0.8,\n                    \"method\": \"Direct\",\n                    \"preserve_original_spacing\": true\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Extract Line Subsegment"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_mask_principal_axis.html",
    "href": "user_guide/data_transformations/calculate_mask_principal_axis.html",
    "title": "Calculate Mask Principal Axis",
    "section": "",
    "text": "This transform calculates the principal axis of a mask, which is a line that represents the primary orientation of the shape defined by the mask’s pixels.\n\n\nThis [Data Transform Operation] is used to determine the orientation of objects within a mask. It analyzes the distribution of pixels in a mask to find its principal axes. The major axis represents the longest dimension of the shape, indicating its primary orientation, while the minor axis is perpendicular to the major axis and represents the shortest dimension.\nThis is useful for quantifying the orientation of elongated objects. The output is a line segment that passes through the centroid of the mask and aligns with either the major or minor axis of pixel variance.\n\n\n\n\n\nCellular Morphology: In microscopy images, this can be used to quantify the orientation of cells, such as neurons or glia, which can be important for understanding tissue organization and cell migration.\nBehavioral Analysis: When tracking the posture of an animal, the principal axis of a body part’s mask (like a limb or tail) can provide a quantitative measure of its angle and movement over time.\nObject Interaction: It can be used to determine how an animal is oriented relative to an object in its environment, for example, by calculating the principal axis of the animal’s body mask.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Principal Axis"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_mask_principal_axis.html#overview",
    "href": "user_guide/data_transformations/calculate_mask_principal_axis.html#overview",
    "title": "Calculate Mask Principal Axis",
    "section": "",
    "text": "This transform calculates the principal axis of a mask, which is a line that represents the primary orientation of the shape defined by the mask’s pixels.\n\n\nThis [Data Transform Operation] is used to determine the orientation of objects within a mask. It analyzes the distribution of pixels in a mask to find its principal axes. The major axis represents the longest dimension of the shape, indicating its primary orientation, while the minor axis is perpendicular to the major axis and represents the shortest dimension.\nThis is useful for quantifying the orientation of elongated objects. The output is a line segment that passes through the centroid of the mask and aligns with either the major or minor axis of pixel variance.\n\n\n\n\n\nCellular Morphology: In microscopy images, this can be used to quantify the orientation of cells, such as neurons or glia, which can be important for understanding tissue organization and cell migration.\nBehavioral Analysis: When tracking the posture of an animal, the principal axis of a body part’s mask (like a limb or tail) can provide a quantitative measure of its angle and movement over time.\nObject Interaction: It can be used to determine how an animal is oriented relative to an object in its environment, for example, by calculating the principal axis of the animal’s body mask.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Principal Axis"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_mask_principal_axis.html#parameters",
    "href": "user_guide/data_transformations/calculate_mask_principal_axis.html#parameters",
    "title": "Calculate Mask Principal Axis",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameter:\n\naxis_type: Specifies which principal axis to calculate. This can be:\n\nMajor: The axis of greatest variance (the “longest” direction of the shape).\nMinor: The axis of least variance (the “shortest” direction of the shape), perpendicular to the major axis.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Principal Axis"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_mask_principal_axis.html#example-configuration",
    "href": "user_guide/data_transformations/calculate_mask_principal_axis.html#example-configuration",
    "title": "Calculate Mask Principal Axis",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example calculates the major principal axis for a mask.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Mask Principal Axis Pipeline\",\n            \"description\": \"Test mask principal axis calculation on mask data\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Calculate Mask Principal Axis\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_mask\",\n                \"output_key\": \"principal_axes\",\n                \"parameters\": {\n                    \"axis_type\": \"Major\"\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Principal Axis"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/apply_median_filter.html",
    "href": "user_guide/data_transformations/apply_median_filter.html",
    "title": "Apply Median Filter",
    "section": "",
    "text": "This transform applies a median filter to a binary mask, which is effective at removing “salt-and-pepper” noise and smoothing object boundaries.\n\n\nThe median filter is a non-linear digital filtering technique used to remove noise from an image or signal. For each pixel in the mask, the filter considers a square neighborhood of a specified size around it, finds the median of all the pixel values in that neighborhood, and replaces the central pixel with that median value.\nThis process is highly effective at removing isolated pixels of one color surrounded by pixels of another color (i.e., “salt-and-pepper” noise). It is also good at preserving sharp edges in the image while smoothing out noise, making it preferable to a standard blurring (mean) filter in many cases.\nThe figure below illustrates how the median filter removes isolated black and white pixels from a binary mask. Notice how a larger window size results in more aggressive smoothing.\n\n\n\n\nIn neuroscience, image processing is often a critical step in data analysis. The median filter is a valuable tool for pre-processing image-based data.\n\nCalcium Imaging Analysis: When segmenting active neurons from calcium imaging videos, the resulting binary masks can be noisy. A median filter can clean up these masks before further analysis, such as calculating cell centroids or areas, leading to more accurate results.\nMorphological Reconstruction: After tracing neurons from microscopy images, digital reconstructions can have small artifacts. Applying a median filter to a 2D projection (mask) of the reconstruction can help remove these artifacts before measuring morphological properties.\nWhisker Tracking: In behavioral neuroscience, tracking the position of an animal’s whiskers often produces binary images of the whisker. A median filter can smooth the whisker’s shape and remove spurious pixels from the background, improving the accuracy of tracking algorithms.",
    "crumbs": [
      "Data Transformations",
      "Apply Median Filter"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/apply_median_filter.html#overview",
    "href": "user_guide/data_transformations/apply_median_filter.html#overview",
    "title": "Apply Median Filter",
    "section": "",
    "text": "This transform applies a median filter to a binary mask, which is effective at removing “salt-and-pepper” noise and smoothing object boundaries.\n\n\nThe median filter is a non-linear digital filtering technique used to remove noise from an image or signal. For each pixel in the mask, the filter considers a square neighborhood of a specified size around it, finds the median of all the pixel values in that neighborhood, and replaces the central pixel with that median value.\nThis process is highly effective at removing isolated pixels of one color surrounded by pixels of another color (i.e., “salt-and-pepper” noise). It is also good at preserving sharp edges in the image while smoothing out noise, making it preferable to a standard blurring (mean) filter in many cases.\nThe figure below illustrates how the median filter removes isolated black and white pixels from a binary mask. Notice how a larger window size results in more aggressive smoothing.\n\n\n\n\nIn neuroscience, image processing is often a critical step in data analysis. The median filter is a valuable tool for pre-processing image-based data.\n\nCalcium Imaging Analysis: When segmenting active neurons from calcium imaging videos, the resulting binary masks can be noisy. A median filter can clean up these masks before further analysis, such as calculating cell centroids or areas, leading to more accurate results.\nMorphological Reconstruction: After tracing neurons from microscopy images, digital reconstructions can have small artifacts. Applying a median filter to a 2D projection (mask) of the reconstruction can help remove these artifacts before measuring morphological properties.\nWhisker Tracking: In behavioral neuroscience, tracking the position of an animal’s whiskers often produces binary images of the whisker. A median filter can smooth the whisker’s shape and remove spurious pixels from the background, improving the accuracy of tracking algorithms.",
    "crumbs": [
      "Data Transformations",
      "Apply Median Filter"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/apply_median_filter.html#parameters",
    "href": "user_guide/data_transformations/apply_median_filter.html#parameters",
    "title": "Apply Median Filter",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameter:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nwindow_size\nThe size of the square kernel used for filtering. The value must be a positive, odd integer (e.g., 3, 5, 7). A larger window size results in more aggressive smoothing but may also remove fine details from the mask.",
    "crumbs": [
      "Data Transformations",
      "Apply Median Filter"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/apply_median_filter.html#example-configuration",
    "href": "user_guide/data_transformations/apply_median_filter.html#example-configuration",
    "title": "Apply Median Filter",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to load data and process it with a transformation pipeline that includes the “Apply Median Filter” step.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Median Filter Pipeline\",\n            \"description\": \"Test median filtering on mask data\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Apply Median Filter\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_mask\",\n                \"output_key\": \"filtered_mask\",\n                \"parameters\": {\n                    \"window_size\": 3\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Apply Median Filter"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html",
    "title": "Analog Hilbert Phase",
    "section": "",
    "text": "This transform calculates the instantaneous phase of an analog signal, which is useful for analyzing oscillations.\n\n\nThe Hilbert transform is a mathematical operation that shifts the phase of all positive frequency components of a signal by -90 degrees and all negative frequency components by +90 degrees. When this phase-shifted signal (the imaginary part) is combined with the original signal (the real part), it forms a complex-valued “analytic signal”.\nThe instantaneous phase is the angle of this complex number at each point in time. It provides a way to represent an oscillating signal in terms of its phase, which progresses from -π to +π for each cycle of the oscillation. This transform uses an efficient FFT-based method to compute the Hilbert transform.\n\n\n\n\n\nIn neuroscience, analyzing the phase of neural signals is crucial for understanding brain function. Some common applications include:\n\nRhythmic Behaviors: The phase of signals from sensors tracking rhythmic behaviors like whisking, sniffing, or licking can be extracted to correlate them with neural activity.",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html#overview",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html#overview",
    "title": "Analog Hilbert Phase",
    "section": "",
    "text": "This transform calculates the instantaneous phase of an analog signal, which is useful for analyzing oscillations.\n\n\nThe Hilbert transform is a mathematical operation that shifts the phase of all positive frequency components of a signal by -90 degrees and all negative frequency components by +90 degrees. When this phase-shifted signal (the imaginary part) is combined with the original signal (the real part), it forms a complex-valued “analytic signal”.\nThe instantaneous phase is the angle of this complex number at each point in time. It provides a way to represent an oscillating signal in terms of its phase, which progresses from -π to +π for each cycle of the oscillation. This transform uses an efficient FFT-based method to compute the Hilbert transform.\n\n\n\n\n\nIn neuroscience, analyzing the phase of neural signals is crucial for understanding brain function. Some common applications include:\n\nRhythmic Behaviors: The phase of signals from sensors tracking rhythmic behaviors like whisking, sniffing, or licking can be extracted to correlate them with neural activity.",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html#parameters",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html#parameters",
    "title": "Analog Hilbert Phase",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nlow_frequency: The low-cut frequency for the bandpass filter, in Hertz. This determines the lower end of the frequency range you want to analyze.\nhigh_frequency: The high-cut frequency for the bandpass filter, in Hertz. This determines the upper end of the frequency range you want to analyze.\ndiscontinuity_threshold: A time gap, in samples, above which the signal is considered to have a break. The transform will process the continuous segments separately. This is useful for data with missing samples or interruptions.",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_hilbert_phase.html#example-configuration",
    "href": "user_guide/data_transformations/analog_hilbert_phase.html#example-configuration",
    "title": "Analog Hilbert Phase",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that loads data and applies the Analog Hilbert Phase transform.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Hilbert Phase Pipeline\",\n            \"description\": \"Test Hilbert phase calculation on analog signal\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Hilbert Phase\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_signal\",\n                \"output_key\": \"phase_signal\",\n                \"parameters\": {\n                    \"low_frequency\": 5.0,\n                    \"high_frequency\": 15.0,\n                    \"discontinuity_threshold\": 1000\n                }\n            }\n        ]\n    }\n}\n]\n\nReferences\nHill, D.N., Curtis, J.C., Moore, J.D., Kleinfeld, D., 2011. Primary motor cortex reports efferent control of vibrissa motion on multiple timescales. Neuron 72, 344–356. https://doi.org/10.1016/j.neuron.2011.09.020",
    "crumbs": [
      "Data Transformations",
      "Analog Hilbert Phase"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_event_threshold.html",
    "href": "user_guide/data_transformations/analog_event_threshold.html",
    "title": "Analog Event Threshold",
    "section": "",
    "text": "This transform detects moments in time (events) when an analog signal crosses a specified value, which is useful for identifying significant occurrences in the data.\n\n\nThresholding is a fundamental method for event detection in time-series data. This operation identifies every point in time where the signal’s amplitude crosses a defined threshold in a specific direction (either rising above it, falling below it, or crossing it in either direction).\nA “lockout” period can also be set. After an event is detected, the algorithm will ignore any further threshold crossings for the duration of this period, which helps prevent a single, noisy event from being counted multiple times.\nThis transform takes an analog time series as input and produces a digital event series as output, where each event corresponds to a single point in time when a threshold crossing was detected.\n\n\n\n\n\nIn neuroscience, thresholding is a common technique for identifying meaningful events in continuous data streams:\n\nSpike Detection: In extracellular recordings, thresholding can be used to identify action potentials (spikes) that stand out from the background noise. A lockout time is critical here to avoid detecting the same spike multiple times as it repolarizes.\nBehavioral Event Marking: For data from sensors like accelerometers or force transducers, thresholding can mark the onset of specific behaviors, such as a mouse starting to run on a wheel or a bird pecking a key.\nCalcium Imaging Analysis: In calcium imaging data, which reflects neural activity, thresholding can be used to detect significant calcium transients that indicate a neuron or a group of neurons are firing.",
    "crumbs": [
      "Data Transformations",
      "Analog Event Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_event_threshold.html#overview",
    "href": "user_guide/data_transformations/analog_event_threshold.html#overview",
    "title": "Analog Event Threshold",
    "section": "",
    "text": "This transform detects moments in time (events) when an analog signal crosses a specified value, which is useful for identifying significant occurrences in the data.\n\n\nThresholding is a fundamental method for event detection in time-series data. This operation identifies every point in time where the signal’s amplitude crosses a defined threshold in a specific direction (either rising above it, falling below it, or crossing it in either direction).\nA “lockout” period can also be set. After an event is detected, the algorithm will ignore any further threshold crossings for the duration of this period, which helps prevent a single, noisy event from being counted multiple times.\nThis transform takes an analog time series as input and produces a digital event series as output, where each event corresponds to a single point in time when a threshold crossing was detected.\n\n\n\n\n\nIn neuroscience, thresholding is a common technique for identifying meaningful events in continuous data streams:\n\nSpike Detection: In extracellular recordings, thresholding can be used to identify action potentials (spikes) that stand out from the background noise. A lockout time is critical here to avoid detecting the same spike multiple times as it repolarizes.\nBehavioral Event Marking: For data from sensors like accelerometers or force transducers, thresholding can mark the onset of specific behaviors, such as a mouse starting to run on a wheel or a bird pecking a key.\nCalcium Imaging Analysis: In calcium imaging data, which reflects neural activity, thresholding can be used to detect significant calcium transients that indicate a neuron or a group of neurons are firing.",
    "crumbs": [
      "Data Transformations",
      "Analog Event Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_event_threshold.html#parameters",
    "href": "user_guide/data_transformations/analog_event_threshold.html#parameters",
    "title": "Analog Event Threshold",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nthreshold_value: The amplitude value the signal must cross to be considered an event.\ndirection: Specifies the direction of the crossing required to trigger an event. This can be:\n\nPositive (Rising): An event is detected when the signal crosses the threshold from a lower to a higher value.\nNegative (Falling): An event is detected when the signal crosses the threshold from a higher to a lower value.\nAbsolute: An event is detected whenever the absolute value of the signal crosses the threshold value (useful for detecting deviations from baseline in either direction).\n\nlockout_time: A duration (in the same time units as the data, e.g., seconds) after each detected event during which no new events will be registered. This is useful for preventing multiple detections of a single, noisy event.",
    "crumbs": [
      "Data Transformations",
      "Analog Event Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_event_threshold.html#example-configuration",
    "href": "user_guide/data_transformations/analog_event_threshold.html#example-configuration",
    "title": "Analog Event Threshold",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example detects events where the signal rises above an amplitude of 1.0, with no lockout period.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Threshold Detection Pipeline\",\n            \"description\": \"Test threshold event detection on analog signal\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Analog Event Threshold\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_signal\",\n                \"output_key\": \"detected_events\",\n                \"parameters\": {\n                    \"threshold_value\": 1.0,\n                    \"direction\": \"Positive\",\n                    \"lockout_time\": 0.0\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Analog Event Threshold"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html",
    "href": "user_guide/behaviors/tongue.html",
    "title": "Tongue Tracking",
    "section": "",
    "text": "The Tongue Tracking widget deals with operations related to the tongue.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html#loading",
    "href": "user_guide/behaviors/tongue.html#loading",
    "title": "Tongue Tracking",
    "section": "Loading",
    "text": "Loading\nTongue masks can be loaded through the sparse HDF5 format or binary images (where white is part of the mask, black is not).\nJaw keypoint tracking can also be loaded through CSV format. The first column should indicate frame number, the next indicating \\(x\\) position and the next \\(y\\) position.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html#grabcut-tool",
    "href": "user_guide/behaviors/tongue.html#grabcut-tool",
    "title": "Tongue Tracking",
    "section": "GrabCut Tool",
    "text": "GrabCut Tool\nDocumentation on the GrabCut tool can be found here.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "examples/keypoint_labeling.html",
    "href": "examples/keypoint_labeling.html",
    "title": "Keypoint Video Labeling",
    "section": "",
    "text": "Keypoint labeling is a powerful method to systematically study motion, behavior, and anatomical positioning over time. By marking and tracking specific locations on subjects across frames in a video, you can extract detailed data about the dynamics of movement.\nFor example, keypoint data can be used to:\nThis tutorial will guide you through the process of labeling keypoints on a video and saving the keypoint data."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-1-load-video-or-existing-labels",
    "href": "examples/keypoint_labeling.html#step-1-load-video-or-existing-labels",
    "title": "Keypoint Video Labeling",
    "section": "Step 1: Load Video or Existing Labels",
    "text": "Step 1: Load Video or Existing Labels\n\n\n\n\n\nTo begin labeling:\n\nOpen the application and navigate to File &gt; Load Data.\nSelect the video you intend to annotate.\n\nAlternatively, if you’ve already created keypoints and want to review or update them:\n\nSelect File &gt; Load JSON Configuration and choose your .json file.\n\n\n\n\n\n\n\nTip\n\n\n\nSee: Setting up a JSON File for more on configuration options."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-2-open-the-data-manager",
    "href": "examples/keypoint_labeling.html#step-2-open-the-data-manager",
    "title": "Keypoint Video Labeling",
    "section": "Step 2: Open the Data Manager",
    "text": "Step 2: Open the Data Manager\n\n\n\n\n\nTo create or manage point data:\n\nNavigate to Modules &gt; Data Manager to open the Data Manager Module."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-3-create-a-new-keypoint-feature",
    "href": "examples/keypoint_labeling.html#step-3-create-a-new-keypoint-feature",
    "title": "Keypoint Video Labeling",
    "section": "Step 3: Create a New Keypoint Feature",
    "text": "Step 3: Create a New Keypoint Feature\n\n\n\n\n\nIn the Data Manager:\n\nSet the Output Directory: This is where your labeled data (CSV) will be saved.\n\nExample: C:/Users/wanglab/Desktop/Cartoon_Mouse\n\nCreate New Data:\n\nType: point\nTime Frame: Select based on your analysis requirements (e.g., time for temporal tracking).\nName: Choose a descriptive name (e.g., jaw, nose, or paw).\n\n\nClick Create New Data to add the new feature."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-4-edit-and-add-keypoints-in-the-media-widget",
    "href": "examples/keypoint_labeling.html#step-4-edit-and-add-keypoints-in-the-media-widget",
    "title": "Keypoint Video Labeling",
    "section": "Step 4: Edit and Add Keypoints in the Media Widget",
    "text": "Step 4: Edit and Add Keypoints in the Media Widget\n\n\n\n\n\nOnce the point feature is created:\n\nEnable the Feature: Select the feature in the media widget to activate the keypoint editor.\nCustomize Appearance:\n\nChoose a color using the hex selector.\nAdjust the opacity (Alpha) for better contrast against the video.\nModify the size and shape of the marker if needed.\n\nBegin Labeling:\n\nSwitch mouse mode to “Select Point”.\nClick anywhere in the video frame to add a point.\nClicking on an existing point will move it.\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote: The keypoint is recorded as a single-pixel coordinate. Visual shape and style are only for display and are not saved."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-5-review-and-export-your-labels",
    "href": "examples/keypoint_labeling.html#step-5-review-and-export-your-labels",
    "title": "Keypoint Video Labeling",
    "section": "Step 5: Review and Export Your Labels",
    "text": "Step 5: Review and Export Your Labels\n\n\n\n\n\nAfter labeling:\n\nSelect your feature (e.g., jaw) in the Data Manager.\nYou’ll see a spreadsheet with:\n\nFrame indices\n(x, y) coordinates for each labeled keypoint\n\n\n\nExport Options:\n\nFile Name: Choose a meaningful name for your CSV.\nDelimiter: (e.g., comma , or tab \\t) – separates columns.\nLine Ending: Select line break style (\\n, \\r\\n, etc.).\nHeader: Optionally include a header row with custom labels."
  },
  {
    "objectID": "examples/keypoint_labeling.html#optional-export-matching-media-frames",
    "href": "examples/keypoint_labeling.html#optional-export-matching-media-frames",
    "title": "Keypoint Video Labeling",
    "section": "Optional: Export Matching Media Frames",
    "text": "Optional: Export Matching Media Frames\n\n\n\n\n\nYou can export the video frames where keypoints were labeled:\n\nCheck Export Matching Media Frames.\nChoose from export options:\n\nSave by Frame Name\nFrame ID Padding\nImage Name Prefix\nSubfolder for Images\nOverwrite Existing Files\n\n\n\n\n\n\n\n\nWarning\n\n\n\nKeypoints will not be drawn on these frames. Only raw frames are exported."
  },
  {
    "objectID": "examples/keypoint_labeling.html#step-6-save-to-csv",
    "href": "examples/keypoint_labeling.html#step-6-save-to-csv",
    "title": "Keypoint Video Labeling",
    "section": "Step 6: Save to CSV",
    "text": "Step 6: Save to CSV\n\n\n\n\n\nOnce you’ve reviewed your labels:\n\nClick “Save to CSV”\n\nA confirmation will appear showing the file path.\nYour labeled keypoint data is now saved and ready for further analysis.\n\n\nSummary\nThis tutorial demonstrated how to:\n\nLoad a video or keypoint configuration\nCreate and configure keypoint data\nAnnotate frames with feature coordinates\nExport label data and relevant frames"
  },
  {
    "objectID": "developer/test_fixture_porting_guide.html",
    "href": "developer/test_fixture_porting_guide.html",
    "title": "Test Fixture Porting Guide: V1 to V2 Transform Testing",
    "section": "",
    "text": "This guide describes the process for creating reusable test fixtures that enable consistent testing between V1 and V2 transform implementations. By decoupling test data creation from the transform-specific API, you can verify that both V1 and V2 produce identical results for the same input data.\n\n\nThe goal is to: 1. Decouple test data from transformation logic 2. Reuse the same test data across V1 and V2 tests 3. Ensure algorithmic consistency between implementations 4. Keep parameter/JSON testing in their respective test files\n\n\n\ntests/\n└── DataManager/\n    └── fixtures/\n        ├── MaskAreaTestFixture.hpp      # Shared test data\n        ├── AnalogEventThresholdTestFixture.hpp\n        └── ...\n\nsrc/DataManager/\n├── transforms/\n│   └── Masks/\n│       └── Mask_Area/\n│           └── mask_area.test.cpp       # V1 tests (uses fixture)\n└── transforms/v2/\n    └── algorithms/\n        └── MaskArea/\n            └── MaskArea.test.cpp        # V2 tests (uses same fixture)\n\n\n\n\n\nIdentify the test data patterns in your V1 tests:\n// Before: Data creation scattered in V1 tests\nTEST_CASE(\"Mask area calculation\", \"[mask][area]\") {\n    auto mask_data = std::make_shared&lt;MaskData&gt;();\n    \n    // Data creation mixed with test logic\n    std::vector&lt;uint32_t&gt; x1 = {1, 2, 3};\n    std::vector&lt;uint32_t&gt; y1 = {1, 2, 3};\n    mask_data-&gt;addAtTime(TimeFrameIndex(100), Mask2D(x1, y1), NotifyObservers::No);\n    \n    auto result = area(mask_data.get());\n    // ... assertions\n}\n\n\n\nCreate a fixture header in tests/DataManager/fixtures/:\n// tests/DataManager/fixtures/MyTransformTestFixture.hpp\n#ifndef MY_TRANSFORM_TEST_FIXTURE_HPP\n#define MY_TRANSFORM_TEST_FIXTURE_HPP\n\n#include \"catch2/catch_test_macros.hpp\"\n#include \"DataManager.hpp\"\n#include \"MyDataType.hpp\"\n#include \"TimeFrame/TimeFrame.hpp\"\n#include \"TimeFrame/StrongTimeTypes.hpp\"\n\n#include &lt;memory&gt;\n#include &lt;map&gt;\n#include &lt;string&gt;\n\nclass MyTransformTestFixture {\nprotected:\n    MyTransformTestFixture() {\n        m_data_manager = std::make_unique&lt;DataManager&gt;();\n        m_time_frame = std::make_shared&lt;TimeFrame&gt;();\n        m_data_manager-&gt;setTime(TimeKey(\"default\"), m_time_frame);\n        populateTestData();\n    }\n\n    ~MyTransformTestFixture() = default;\n\n    DataManager* getDataManager() {\n        return m_data_manager.get();\n    }\n\n    // Map of named test data objects\n    std::map&lt;std::string, std::shared_ptr&lt;MyDataType&gt;&gt; m_test_data;\n\nprivate:\n    void populateTestData() {\n        // Create each test scenario with a descriptive key\n        createData(\"empty_data\", /* ... */);\n        createData(\"single_element\", /* ... */);\n        createData(\"multiple_elements\", /* ... */);\n        createData(\"edge_case_scenario\", /* ... */);\n    }\n\n    void createData(const std::string& key, /* ... */) {\n        auto data = std::make_shared&lt;MyDataType&gt;();\n        // ... populate data ...\n        m_data_manager-&gt;setData(key, data, TimeKey(\"default\"));\n        m_test_data[key] = data;\n    }\n\n    std::unique_ptr&lt;DataManager&gt; m_data_manager;\n    std::shared_ptr&lt;TimeFrame&gt; m_time_frame;\n};\n\n#endif\nKey Design Principles:\n\nUse descriptive keys - Name test data by the scenario, not the expected result\nStore both in map and DataManager - Allows direct access and key-based lookup\nDocument expected results in comments - But don’t encode them in the fixture\nKeep fixture minimal - Only data creation, no transform logic\n\n\n\n\nUpdate the V1 test file to use TEST_CASE_METHOD:\n// src/DataManager/transforms/.../my_transform.test.cpp\n\n#include \"my_transform.hpp\"\n#include \"fixtures/MyTransformTestFixture.hpp\"\n\n// Tests using fixture data\nTEST_CASE_METHOD(MyTransformTestFixture, \n                 \"Transform - Scenario Name\", \n                 \"[transforms][v1][my_transform]\") {\n    \n    auto input = m_test_data[\"single_element\"];\n    auto result = my_transform(input.get());\n    \n    REQUIRE(result != nullptr);\n    // ... assertions specific to this scenario\n}\n\n// Keep non-fixture tests for simple cases\nTEST_CASE(\"Transform - Simple inline test\", \"[transforms][v1][my_transform]\") {\n    // Small inline tests that don't benefit from fixture\n}\n\n// Keep parameter/JSON tests separate\nTEST_CASE(\"Transform - JSON pipeline\", \"[transforms][v1][json]\") {\n    // JSON/parameter-specific tests stay in this file\n}\n\n\n\nIn the V2 test file, use the same fixture:\n// src/DataManager/transforms/v2/algorithms/MyTransform/MyTransform.test.cpp\n\n#include \"MyTransform.hpp\"\n#include \"fixtures/MyTransformTestFixture.hpp\"\n#include \"transforms/v2/core/DataManagerIntegration.hpp\"\n\nusing namespace WhiskerToolbox::Transforms::V2;\n\n// V2 tests using same fixture data\nTEST_CASE_METHOD(MyTransformTestFixture,\n                 \"TransformsV2 - Scenario Name\",\n                 \"[transforms][v2][my_transform]\") {\n    \n    auto input = m_test_data[\"single_element\"];\n    \n    // Use V2 registry\n    auto& registry = ElementRegistry::instance();\n    MyTransformParams params;\n    \n    auto result = registry.execute&lt;ElementType, OutputType, MyTransformParams&gt;(\n        \"MyTransform\", *input, params);\n    \n    // Same assertions as V1 (verify algorithmic parity)\n    REQUIRE(result == expected_value);\n}\n\n// V2 JSON/DataManager integration tests\nTEST_CASE_METHOD(MyTransformTestFixture,\n                 \"TransformsV2 - DataManager Integration via load_data_from_json_config_v2\",\n                 \"[transforms][v2][datamanager]\") {\n    \n    // Fixture pre-populates data in DataManager\n    auto dm = getDataManager();\n    \n    // V2 JSON format\n    std::string json_config = R\"({\n        \"name\": \"Test Pipeline\",\n        \"steps\": [{\n            \"name\": \"MyTransform\",\n            \"input_key\": \"single_element\",\n            \"output_key\": \"result\",\n            \"parameters\": {}\n        }]\n    })\";\n    \n    auto result = load_data_from_json_config_v2(dm, json_config);\n    REQUIRE(result.success);\n    \n    auto output = dm-&gt;getData&lt;OutputType&gt;(\"result\");\n    REQUIRE(output != nullptr);\n    // ... verify results match V1\n}\n\n\n\nCreate explicit parity tests:\nTEST_CASE_METHOD(MyTransformTestFixture,\n                 \"V1/V2 Parity - Single element\",\n                 \"[transforms][parity]\") {\n    \n    auto input = m_test_data[\"single_element\"];\n    \n    // V1 result\n    auto v1_result = v1_transform(input.get());\n    \n    // V2 result\n    auto& registry = ElementRegistry::instance();\n    auto v2_result = registry.execute&lt;...&gt;(\"Transform\", *input, {});\n    \n    // Compare (accounting for V1-&gt;AnalogTimeSeries vs V2-&gt;RaggedAnalogTimeSeries)\n    REQUIRE(/* results equivalent */);\n}\n\n\n\n\n\n\nFor transforms that work on different input types:\nclass MultiInputTestFixture {\nprotected:\n    std::map&lt;std::string, std::shared_ptr&lt;TypeA&gt;&gt; m_type_a_data;\n    std::map&lt;std::string, std::shared_ptr&lt;TypeB&gt;&gt; m_type_b_data;\n};\n\n\n\nDocument expected results without encoding them:\nvoid populateTestData() {\n    // Scenario: Single mask at timestamp 100 with 3 pixels\n    // V1 Expected: AnalogTimeSeries with {100: 3.0}\n    // V2 Expected: RaggedAnalogTimeSeries with {100: [3.0]}\n    createMask(\"single_mask_3px\", \n        /*timestamp=*/100, \n        /*pixels=*/{Point2D{1,1}, Point2D{1,2}, Point2D{2,1}});\n}\n\n\n\nFor testing parameter variations:\nvoid populateTestData() {\n    // Base data for parameter testing\n    createSignal(\"param_test_base\", {1.0, 2.0, 3.0}, {0, 10, 20});\n    \n    // The parameters themselves stay in the test files,\n    // only the data is shared via fixture\n}\n\n\n\n\nKeep these in the respective V1/V2 test files:\n\nParameter struct tests - JSON loading, validation\nTransform-specific API tests - V1 TransformOperation vs V2 ElementRegistry\nJSON pipeline format tests - V1 format vs V2 format\nRegistry-specific tests - Metadata, type mapping\nSimple inline tests - Trivial cases that don’t need fixture\n\n\n\n\n\nIdentify reusable test scenarios in V1 tests\nCreate fixture in tests/DataManager/fixtures/\nAdd fixture to V1 test’s CMakeLists.txt includes\nRefactor V1 tests to use TEST_CASE_METHOD\nVerify V1 tests still pass\nAdd fixture include to V2 test file\nCreate V2 tests using same data keys\nCreate DataManager integration tests using load_data_from_json_config_v2\nVerify V2 tests pass with same expected results\n(Optional) Add explicit V1/V2 parity tests\n\n\n\n\nSee the complete implementation: - Fixture: tests/DataManager/fixtures/AnalogEventThresholdTestFixture.hpp - V2 Tests: src/DataManager/transforms/v2/algorithms/AnalogEventThreshold/AnalogEventThreshold.test.cpp\nThe fixture creates named test signals like \"positive_no_lockout\", \"negative_with_lockout\", etc., which are then used in both V1 and V2 tests with their respective APIs."
  },
  {
    "objectID": "developer/test_fixture_porting_guide.html#overview",
    "href": "developer/test_fixture_porting_guide.html#overview",
    "title": "Test Fixture Porting Guide: V1 to V2 Transform Testing",
    "section": "",
    "text": "The goal is to: 1. Decouple test data from transformation logic 2. Reuse the same test data across V1 and V2 tests 3. Ensure algorithmic consistency between implementations 4. Keep parameter/JSON testing in their respective test files"
  },
  {
    "objectID": "developer/test_fixture_porting_guide.html#directory-structure",
    "href": "developer/test_fixture_porting_guide.html#directory-structure",
    "title": "Test Fixture Porting Guide: V1 to V2 Transform Testing",
    "section": "",
    "text": "tests/\n└── DataManager/\n    └── fixtures/\n        ├── MaskAreaTestFixture.hpp      # Shared test data\n        ├── AnalogEventThresholdTestFixture.hpp\n        └── ...\n\nsrc/DataManager/\n├── transforms/\n│   └── Masks/\n│       └── Mask_Area/\n│           └── mask_area.test.cpp       # V1 tests (uses fixture)\n└── transforms/v2/\n    └── algorithms/\n        └── MaskArea/\n            └── MaskArea.test.cpp        # V2 tests (uses same fixture)"
  },
  {
    "objectID": "developer/test_fixture_porting_guide.html#step-by-step-process",
    "href": "developer/test_fixture_porting_guide.html#step-by-step-process",
    "title": "Test Fixture Porting Guide: V1 to V2 Transform Testing",
    "section": "",
    "text": "Identify the test data patterns in your V1 tests:\n// Before: Data creation scattered in V1 tests\nTEST_CASE(\"Mask area calculation\", \"[mask][area]\") {\n    auto mask_data = std::make_shared&lt;MaskData&gt;();\n    \n    // Data creation mixed with test logic\n    std::vector&lt;uint32_t&gt; x1 = {1, 2, 3};\n    std::vector&lt;uint32_t&gt; y1 = {1, 2, 3};\n    mask_data-&gt;addAtTime(TimeFrameIndex(100), Mask2D(x1, y1), NotifyObservers::No);\n    \n    auto result = area(mask_data.get());\n    // ... assertions\n}\n\n\n\nCreate a fixture header in tests/DataManager/fixtures/:\n// tests/DataManager/fixtures/MyTransformTestFixture.hpp\n#ifndef MY_TRANSFORM_TEST_FIXTURE_HPP\n#define MY_TRANSFORM_TEST_FIXTURE_HPP\n\n#include \"catch2/catch_test_macros.hpp\"\n#include \"DataManager.hpp\"\n#include \"MyDataType.hpp\"\n#include \"TimeFrame/TimeFrame.hpp\"\n#include \"TimeFrame/StrongTimeTypes.hpp\"\n\n#include &lt;memory&gt;\n#include &lt;map&gt;\n#include &lt;string&gt;\n\nclass MyTransformTestFixture {\nprotected:\n    MyTransformTestFixture() {\n        m_data_manager = std::make_unique&lt;DataManager&gt;();\n        m_time_frame = std::make_shared&lt;TimeFrame&gt;();\n        m_data_manager-&gt;setTime(TimeKey(\"default\"), m_time_frame);\n        populateTestData();\n    }\n\n    ~MyTransformTestFixture() = default;\n\n    DataManager* getDataManager() {\n        return m_data_manager.get();\n    }\n\n    // Map of named test data objects\n    std::map&lt;std::string, std::shared_ptr&lt;MyDataType&gt;&gt; m_test_data;\n\nprivate:\n    void populateTestData() {\n        // Create each test scenario with a descriptive key\n        createData(\"empty_data\", /* ... */);\n        createData(\"single_element\", /* ... */);\n        createData(\"multiple_elements\", /* ... */);\n        createData(\"edge_case_scenario\", /* ... */);\n    }\n\n    void createData(const std::string& key, /* ... */) {\n        auto data = std::make_shared&lt;MyDataType&gt;();\n        // ... populate data ...\n        m_data_manager-&gt;setData(key, data, TimeKey(\"default\"));\n        m_test_data[key] = data;\n    }\n\n    std::unique_ptr&lt;DataManager&gt; m_data_manager;\n    std::shared_ptr&lt;TimeFrame&gt; m_time_frame;\n};\n\n#endif\nKey Design Principles:\n\nUse descriptive keys - Name test data by the scenario, not the expected result\nStore both in map and DataManager - Allows direct access and key-based lookup\nDocument expected results in comments - But don’t encode them in the fixture\nKeep fixture minimal - Only data creation, no transform logic\n\n\n\n\nUpdate the V1 test file to use TEST_CASE_METHOD:\n// src/DataManager/transforms/.../my_transform.test.cpp\n\n#include \"my_transform.hpp\"\n#include \"fixtures/MyTransformTestFixture.hpp\"\n\n// Tests using fixture data\nTEST_CASE_METHOD(MyTransformTestFixture, \n                 \"Transform - Scenario Name\", \n                 \"[transforms][v1][my_transform]\") {\n    \n    auto input = m_test_data[\"single_element\"];\n    auto result = my_transform(input.get());\n    \n    REQUIRE(result != nullptr);\n    // ... assertions specific to this scenario\n}\n\n// Keep non-fixture tests for simple cases\nTEST_CASE(\"Transform - Simple inline test\", \"[transforms][v1][my_transform]\") {\n    // Small inline tests that don't benefit from fixture\n}\n\n// Keep parameter/JSON tests separate\nTEST_CASE(\"Transform - JSON pipeline\", \"[transforms][v1][json]\") {\n    // JSON/parameter-specific tests stay in this file\n}\n\n\n\nIn the V2 test file, use the same fixture:\n// src/DataManager/transforms/v2/algorithms/MyTransform/MyTransform.test.cpp\n\n#include \"MyTransform.hpp\"\n#include \"fixtures/MyTransformTestFixture.hpp\"\n#include \"transforms/v2/core/DataManagerIntegration.hpp\"\n\nusing namespace WhiskerToolbox::Transforms::V2;\n\n// V2 tests using same fixture data\nTEST_CASE_METHOD(MyTransformTestFixture,\n                 \"TransformsV2 - Scenario Name\",\n                 \"[transforms][v2][my_transform]\") {\n    \n    auto input = m_test_data[\"single_element\"];\n    \n    // Use V2 registry\n    auto& registry = ElementRegistry::instance();\n    MyTransformParams params;\n    \n    auto result = registry.execute&lt;ElementType, OutputType, MyTransformParams&gt;(\n        \"MyTransform\", *input, params);\n    \n    // Same assertions as V1 (verify algorithmic parity)\n    REQUIRE(result == expected_value);\n}\n\n// V2 JSON/DataManager integration tests\nTEST_CASE_METHOD(MyTransformTestFixture,\n                 \"TransformsV2 - DataManager Integration via load_data_from_json_config_v2\",\n                 \"[transforms][v2][datamanager]\") {\n    \n    // Fixture pre-populates data in DataManager\n    auto dm = getDataManager();\n    \n    // V2 JSON format\n    std::string json_config = R\"({\n        \"name\": \"Test Pipeline\",\n        \"steps\": [{\n            \"name\": \"MyTransform\",\n            \"input_key\": \"single_element\",\n            \"output_key\": \"result\",\n            \"parameters\": {}\n        }]\n    })\";\n    \n    auto result = load_data_from_json_config_v2(dm, json_config);\n    REQUIRE(result.success);\n    \n    auto output = dm-&gt;getData&lt;OutputType&gt;(\"result\");\n    REQUIRE(output != nullptr);\n    // ... verify results match V1\n}\n\n\n\nCreate explicit parity tests:\nTEST_CASE_METHOD(MyTransformTestFixture,\n                 \"V1/V2 Parity - Single element\",\n                 \"[transforms][parity]\") {\n    \n    auto input = m_test_data[\"single_element\"];\n    \n    // V1 result\n    auto v1_result = v1_transform(input.get());\n    \n    // V2 result\n    auto& registry = ElementRegistry::instance();\n    auto v2_result = registry.execute&lt;...&gt;(\"Transform\", *input, {});\n    \n    // Compare (accounting for V1-&gt;AnalogTimeSeries vs V2-&gt;RaggedAnalogTimeSeries)\n    REQUIRE(/* results equivalent */);\n}"
  },
  {
    "objectID": "developer/test_fixture_porting_guide.html#fixture-patterns",
    "href": "developer/test_fixture_porting_guide.html#fixture-patterns",
    "title": "Test Fixture Porting Guide: V1 to V2 Transform Testing",
    "section": "",
    "text": "For transforms that work on different input types:\nclass MultiInputTestFixture {\nprotected:\n    std::map&lt;std::string, std::shared_ptr&lt;TypeA&gt;&gt; m_type_a_data;\n    std::map&lt;std::string, std::shared_ptr&lt;TypeB&gt;&gt; m_type_b_data;\n};\n\n\n\nDocument expected results without encoding them:\nvoid populateTestData() {\n    // Scenario: Single mask at timestamp 100 with 3 pixels\n    // V1 Expected: AnalogTimeSeries with {100: 3.0}\n    // V2 Expected: RaggedAnalogTimeSeries with {100: [3.0]}\n    createMask(\"single_mask_3px\", \n        /*timestamp=*/100, \n        /*pixels=*/{Point2D{1,1}, Point2D{1,2}, Point2D{2,1}});\n}\n\n\n\nFor testing parameter variations:\nvoid populateTestData() {\n    // Base data for parameter testing\n    createSignal(\"param_test_base\", {1.0, 2.0, 3.0}, {0, 10, 20});\n    \n    // The parameters themselves stay in the test files,\n    // only the data is shared via fixture\n}"
  },
  {
    "objectID": "developer/test_fixture_porting_guide.html#what-stays-in-test-files",
    "href": "developer/test_fixture_porting_guide.html#what-stays-in-test-files",
    "title": "Test Fixture Porting Guide: V1 to V2 Transform Testing",
    "section": "",
    "text": "Keep these in the respective V1/V2 test files:\n\nParameter struct tests - JSON loading, validation\nTransform-specific API tests - V1 TransformOperation vs V2 ElementRegistry\nJSON pipeline format tests - V1 format vs V2 format\nRegistry-specific tests - Metadata, type mapping\nSimple inline tests - Trivial cases that don’t need fixture"
  },
  {
    "objectID": "developer/test_fixture_porting_guide.html#checklist-for-porting",
    "href": "developer/test_fixture_porting_guide.html#checklist-for-porting",
    "title": "Test Fixture Porting Guide: V1 to V2 Transform Testing",
    "section": "",
    "text": "Identify reusable test scenarios in V1 tests\nCreate fixture in tests/DataManager/fixtures/\nAdd fixture to V1 test’s CMakeLists.txt includes\nRefactor V1 tests to use TEST_CASE_METHOD\nVerify V1 tests still pass\nAdd fixture include to V2 test file\nCreate V2 tests using same data keys\nCreate DataManager integration tests using load_data_from_json_config_v2\nVerify V2 tests pass with same expected results\n(Optional) Add explicit V1/V2 parity tests"
  },
  {
    "objectID": "developer/test_fixture_porting_guide.html#example-analogeventthreshold",
    "href": "developer/test_fixture_porting_guide.html#example-analogeventthreshold",
    "title": "Test Fixture Porting Guide: V1 to V2 Transform Testing",
    "section": "",
    "text": "See the complete implementation: - Fixture: tests/DataManager/fixtures/AnalogEventThresholdTestFixture.hpp - V2 Tests: src/DataManager/transforms/v2/algorithms/AnalogEventThreshold/AnalogEventThreshold.test.cpp\nThe fixture creates named test signals like \"positive_no_lockout\", \"negative_with_lockout\", etc., which are then used in both V1 and V2 tests with their respective APIs."
  },
  {
    "objectID": "developer/table_view.html",
    "href": "developer/table_view.html",
    "title": "Table View",
    "section": "",
    "text": "The TableView class is able to take heterogeneous data from the Data Manager and structure it like a spreadsheet.\n\n\nThe columns may represent simple double or floating-point values, but also could take the form of different types like Booleans, counting numbers, or even arrays of values. Columns also may be data that is not directly in the Data Manager, but come as the result of some processing similar to the transformation infrastructure in the Data Manager. Some of these transformations may be redundant. I imagine transformations done for the TableView may be out of convenience or for data that you don’t necessarily care about saving as its own type, but just need for some additional processing.\nThe transformations done on data for a column need not necessarily come from the Data Manager, but also could come from other columns in the TableView. For instance, if we compute some kind of transformation and hold it in a column, we could then choose to z-score it.\n\n\n\n\n\n\n\n\n\n\nColumn Computer Name\nDescription\nTest?\nDocumentation?\nBenchmarking?\n\n\n\n\nAnalog Slice Gatherer\nGets analog data within intervals\nYes\n\n\n\n\nAnalog Timestamp Offsets\ncolumn where rows are offset by some value from timestamp selector\nYes\n\n\n\n\nEvent In Interval\n\nYes\n\n\n\n\nInterval Overlap\n\nYes\n\n\n\n\nInterval Property\n\nYes\n\n\n\n\nInterval Reduction\n\nYes\n\n\n\n\nLine Sampling\n\nYes\n\n\n\n\nStandardize\n\nNo\n\n\n\n\nTimestamp In Interval\n\nYes\n\n\n\n\nTimestamp Value\n\nYes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe other difference from transformations comes from the concept of a row in the TableView structure. A row in the TableView could be a timestamp, just like how data is stored in the Manager, but also could be something that is an aggregator. For instance, a digital interview interval in the Data Manager can serve as a row. In this scheme, a double value in a column may not correspond to some value at an instant time, but rather one that is the average over an interval that corresponds to the row. It also could be the minimum value, the maximum value, or it could represent something like a standard deviation.\n\n\n\nTo create a TableView, the user must define what will make up the rows and what will make up the columns.\nRows are defined by selector objects that currently can be supported by intervals or timestamps. Note that these timestamps could correspond to the different timeframes in the Data Manager class.\nColumns are created by different computer objects. These computer objects work on different data sources that are placed into the TableView, and usually perform some kind of transformation. These data sources are things like analog sources, event sources, or interval sources. Adapter objects exist in the TableView to allow types in the Data Manager to be converted to these forms. Some of these transformations are straightforward. For instance, an analog source can be simply converted into a column of doubles. But something like point data would be broken up into two columns of x and y values for each timestamp.\nThe computers for a column that are available depend on the input data type as well as what row selector is being used (for example, intervals versus timestamps). A factory and registry is available to list the types of computers available for these combinations and can output the computer object that is available from the factory.\nWith the rows, analog sources defined, and computers, a builder object can be used to create an actual TableView.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#columns",
    "href": "developer/table_view.html#columns",
    "title": "Table View",
    "section": "",
    "text": "The columns may represent simple double or floating-point values, but also could take the form of different types like Booleans, counting numbers, or even arrays of values. Columns also may be data that is not directly in the Data Manager, but come as the result of some processing similar to the transformation infrastructure in the Data Manager. Some of these transformations may be redundant. I imagine transformations done for the TableView may be out of convenience or for data that you don’t necessarily care about saving as its own type, but just need for some additional processing.\nThe transformations done on data for a column need not necessarily come from the Data Manager, but also could come from other columns in the TableView. For instance, if we compute some kind of transformation and hold it in a column, we could then choose to z-score it.\n\n\n\n\n\n\n\n\n\n\nColumn Computer Name\nDescription\nTest?\nDocumentation?\nBenchmarking?\n\n\n\n\nAnalog Slice Gatherer\nGets analog data within intervals\nYes\n\n\n\n\nAnalog Timestamp Offsets\ncolumn where rows are offset by some value from timestamp selector\nYes\n\n\n\n\nEvent In Interval\n\nYes\n\n\n\n\nInterval Overlap\n\nYes\n\n\n\n\nInterval Property\n\nYes\n\n\n\n\nInterval Reduction\n\nYes\n\n\n\n\nLine Sampling\n\nYes\n\n\n\n\nStandardize\n\nNo\n\n\n\n\nTimestamp In Interval\n\nYes\n\n\n\n\nTimestamp Value\n\nYes",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#rows",
    "href": "developer/table_view.html#rows",
    "title": "Table View",
    "section": "",
    "text": "The other difference from transformations comes from the concept of a row in the TableView structure. A row in the TableView could be a timestamp, just like how data is stored in the Manager, but also could be something that is an aggregator. For instance, a digital interview interval in the Data Manager can serve as a row. In this scheme, a double value in a column may not correspond to some value at an instant time, but rather one that is the average over an interval that corresponds to the row. It also could be the minimum value, the maximum value, or it could represent something like a standard deviation.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#constructing-a-tableview",
    "href": "developer/table_view.html#constructing-a-tableview",
    "title": "Table View",
    "section": "",
    "text": "To create a TableView, the user must define what will make up the rows and what will make up the columns.\nRows are defined by selector objects that currently can be supported by intervals or timestamps. Note that these timestamps could correspond to the different timeframes in the Data Manager class.\nColumns are created by different computer objects. These computer objects work on different data sources that are placed into the TableView, and usually perform some kind of transformation. These data sources are things like analog sources, event sources, or interval sources. Adapter objects exist in the TableView to allow types in the Data Manager to be converted to these forms. Some of these transformations are straightforward. For instance, an analog source can be simply converted into a column of doubles. But something like point data would be broken up into two columns of x and y values for each timestamp.\nThe computers for a column that are available depend on the input data type as well as what row selector is being used (for example, intervals versus timestamps). A factory and registry is available to list the types of computers available for these combinations and can output the computer object that is available from the factory.\nWith the rows, analog sources defined, and computers, a builder object can be used to create an actual TableView.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#csv-export",
    "href": "developer/table_view.html#csv-export",
    "title": "Table View",
    "section": "CSV Export",
    "text": "CSV Export\nThe first, and most straightforward, use case is simple CSV export. For instance, imagine an experiment where an animal is licking. The intervals are calculated when licks occur. We would like to have some output for each lick where we are aware of the start time of the lick, the end time of the lick, and the duration. Within each of these licks, we would like to know the maximum surface area of the tongue that is stored in an analog data source at each timestamp from a tracked video.\nWe may also wish to know if some other digital events are co-occurring. For instance, licks are often organized in sequences of so-called bouts, so there are multiple intervals of licks within a single bout. That digital interval may be contained within the data manager. We can include that as a column, or each lick can be identified by an integer that represents which bout it belongs to. So there may be three licks within the first bout that all have an ID of 0, then licks four and five have a bout ID of 1, et cetera.\nThis can also be a convenient interface to get time conversion information from data that is in a different timeframe. For instance, imagine if we are also considering the ID of a laser digital interval that is coincident with a lick. This laser may be acquired in a different timeframe with a different sampling rate. In addition to getting the ID of the coincident laser interval, we can also get its start time in units of camera frames, like the lick. In this way, we’ll be finding what is the closest camera frame to the beginning of the laser start time that was around this lick. This can be useful for aligning to different event types.\nThis data can be serialized to a CSV. In this way, we can also include data that would be array-like, like gathering all of the events within some interval. This could be useful for getting, say, the spike times that occur within some meaningful experimental time block.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#analysis-dashboard",
    "href": "developer/table_view.html#analysis-dashboard",
    "title": "Table View",
    "section": "Analysis Dashboard",
    "text": "Analysis Dashboard\nThe next use of the TableView is in the analysis dashboard. This is one of the main plotting interfaces for Neuralyzer. In this widget, we are able to plot different kinds of data that are in the data manager. The TableView then allows us to also plot different transformations of data that is in the data manager. For instance, if we wanted to visualize a raster plot from a spiking neuron, we could use the TableView to use a row selector that is an interval around some event. Let’s imagine aligning to lick onset. We could pick one second on either side of the lick onset and call that an interval, and then gather the event times for each of those. This would give us a data structure that is essentially an array of arrays, and we can also normalize these to that event time. This gives us the exact data structure necessary for creating a raster plot.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#feature-extraction-and-dimensionality-reduction",
    "href": "developer/table_view.html#feature-extraction-and-dimensionality-reduction",
    "title": "Table View",
    "section": "Feature Extraction and Dimensionality Reduction",
    "text": "Feature Extraction and Dimensionality Reduction\nThe TableView architecture also provides a convenient interface to get something that looks like a 2D array. Imagine if we wanted to try to figure out features of a two-dimensional line to determine its identity. We could calculate with computers to make columns for the curvature of the line, and perhaps we would want to get the x and y positions at different fractional line lengths (e.g., where is the x position at 20% along the line, 40% along the line, 60% along the line, etc.). Then we could also include the angle of the line, the follicular position, etc.\nI probably don’t want to have to go through the data transform widget and extract all of these and keep them hanging around, because what I really would want is to have a 2D array and then perform some kind of dimensionality reduction on it. Then, with those arbitrary features, I might be able to look for clusters that ideally would correspond to the same whisker, compared to multiple whiskers that would have differences in the first or second principal component.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/table_view.html#machine-learning-interface",
    "href": "developer/table_view.html#machine-learning-interface",
    "title": "Table View",
    "section": "Machine Learning Interface",
    "text": "Machine Learning Interface\nSimilarly, the TableView provides a convenient interface to try to gather data to feed into some kind of machine learning algorithm. We can use the TableView to get our feature matrix for training some kind of model. In that case, if we are trying to classify specific frames of a movie that correspond to some behavior, we could use time as our row selector. Then we could use different calculated quantities from the video, like, say, the 2D positions of key points as the columns, and feed this to some kind of machine learning classification model if we also have accompanying labels. For instance, a label could specify that this frame corresponds to some behavior and this one does not. That could even be one of the columns of our TableView that we then separate and treat differently.",
    "crumbs": [
      "Table View"
    ]
  },
  {
    "objectID": "developer/point_display_options.html",
    "href": "developer/point_display_options.html",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options.\n\n\n\n\n\n\nRange: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives\n\n\n\n\n\n\n\n\nRange: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots\n\n\n\n\n\n\n\nstruct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - All configurations support real-time updates without data loss\n\n\n\n\n\n\n\nSelect the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nDocument new features in this file"
  },
  {
    "objectID": "developer/point_display_options.html#overview",
    "href": "developer/point_display_options.html#overview",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options."
  },
  {
    "objectID": "developer/point_display_options.html#point-display-options",
    "href": "developer/point_display_options.html#point-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives"
  },
  {
    "objectID": "developer/point_display_options.html#line-display-options",
    "href": "developer/point_display_options.html#line-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots"
  },
  {
    "objectID": "developer/point_display_options.html#technical-architecture",
    "href": "developer/point_display_options.html#technical-architecture",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "struct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - All configurations support real-time updates without data loss"
  },
  {
    "objectID": "developer/point_display_options.html#usage-guidelines",
    "href": "developer/point_display_options.html#usage-guidelines",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Select the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nDocument new features in this file"
  },
  {
    "objectID": "developer/intro_to_c.html",
    "href": "developer/intro_to_c.html",
    "title": "Introduction to C++",
    "section": "",
    "text": "C++ is a compiled, systems programming language. It has a long history and is feature-rich. You can do just about anything with C++ code, but it also includes many foot guns.\n\nBooks\n\n\nVideo Tutorials\nMike Shah (Lecturer at Yale, formerly Northeastern University) has an excellent modern C++ design series on youtube. Episodes are a nice ~20 minute length:\nModern Cpp series by Mike Shah\nHe also has a series on modern OpenGL on youtube:\nIntroduction to OpenGL\nThe flagship C++ conference has a “Back to Basics” track every year which ~10 hours of teaching on a range of introductory topics. I highly recommend these, and the various instructors tend to be professional trainers who have additional resources\n\nCppCon 2023 Back to Basics Playlist\nCppCon 2022 Back to Basics Playlist",
    "crumbs": [
      "Introduction to C++"
    ]
  },
  {
    "objectID": "developer/file_io.html",
    "href": "developer/file_io.html",
    "title": "File IO",
    "section": "",
    "text": "Data Type\nFormat\nDependency\nRaw Data for Testing?\nIO Widget Testing\nDatamanager IO Testing\nJSON Validation Testing\nUser Guide\n\n\n\n\nAnalog\nBinary\n\n\n\n\n\n\n\n\nAnalog\nCSV\n\nSome\n\n\n\n\n\n\nDigitalEvent\nCSV\n\nYes\n\n\n\n\n\n\nDigitalInterval\nBinary\n\n\n\n\n\n\n\n\nDigitalInteral\nCSV\n\n\n\n\n\n\n\n\nLine Data\nBinary\nCap’n Proto\n\n\n\n\n\n\n\nLine Data\nCSV\n\n\n\n\n\n\n\n\nMask Data\nHDF5\nHDF5\n\n\n\n\n\n\n\nMask Data\nImage\nOpenCV\n\n\n\n\n\n\n\nMedia Data\nVideo\nffmpeg\nYes\n\n\n\n\n\n\nMedia Data\nImages\nOpenCV\n\n\n\n\n\n\n\nMedia Data\nHDF5\nHDF5\n\n\n\n\n\n\n\nPoint Data\nCSV\n\nYes\n\n\n\n\n\n\nTensor\nNumpy\nNumpy",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#overview-of-file-io-requirements",
    "href": "developer/file_io.html#overview-of-file-io-requirements",
    "title": "File IO",
    "section": "Overview of File IO Requirements",
    "text": "Overview of File IO Requirements\nBecause Neuralizer is compatible with a wide range of data, its file IO needs to support a wide range of file formats. Some of these file formats will be pre-subscribed, such as data acquisition-specific formats for electrophysiology. Others are more specific to this package, such as for line data that varies over time. We must also be aware from the start that some electrophysiology file sizes can be expected to be larger than easy to work with on a regular desktop machine. For instance, Neuropixel probes record almost 400 channels at 20 kHz and are becoming more routine for use. Consequently, multiple file formats should have the ability to be easily memory-mapped and loaded from disk rather than loading the entire file into RAM.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#incremental-data-adjustments-and-key-value-stores",
    "href": "developer/file_io.html#incremental-data-adjustments-and-key-value-stores",
    "title": "File IO",
    "section": "Incremental Data Adjustments and Key-Value Stores",
    "text": "Incremental Data Adjustments and Key-Value Stores\nAnother important consideration from the start is that many of these data types will be incrementally adjusted. For instance, processing a video sequence frame by frame or manually adjusting the output of an automated algorithm requires changing some subset of data in a much larger scheme. For some of these file types that are greater than hundreds of megabytes, resaving an entire file for point manipulations could be onerous and time-consuming. Consequently, for some file types, it would be advantageous to use a key-value database type structure that allows us to easily make incremental adjustments and only save those changes, consequently saving data much more quickly. To my knowledge, this process is quite common in the wild but uncommon in neuroscience.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#serialization-with-captain-proto",
    "href": "developer/file_io.html#serialization-with-captain-proto",
    "title": "File IO",
    "section": "Serialization with Captain Proto",
    "text": "Serialization with Captain Proto\nThe process of data serialization and deserialization will be accomplished with the Captain Proto library. This is able to easily create binary files from more complex data structures, such as those that hold multiple lines of varying size per unit time. This library can define output file structures that are purely binary and would be saved and loaded as entire objects. Alternatively, different definitions can be used to save objects as key-value pairs; for instance, each time point can be the key and the data at that time can be the value.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#memory-mapping-implementation",
    "href": "developer/file_io.html#memory-mapping-implementation",
    "title": "File IO",
    "section": "Memory Mapping Implementation",
    "text": "Memory Mapping Implementation\nCaptain Proto does not perform memory mapping itself. Memory mapping is different across different types of file systems, so to maintain cross-platform use, we will need to either specify different interfaces for interacting with Captain Proto or we could use another third-party library that does this for us. Boost.Interprocess is commonly cited for this purpose, but there are others.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#key-value-database-libraries",
    "href": "developer/file_io.html#key-value-database-libraries",
    "title": "File IO",
    "section": "Key-Value Database Libraries",
    "text": "Key-Value Database Libraries\nFor key-value database structures, there are multiple libraries available. Many of these are mature and designed to work with servers of extremely large datasets, and some have been developed by very large companies. One of the most popular is called LMDB, but in my test cases, this appears to be not as great with Windows. Using this library requires specifying some maximum file size; on Linux, the resulting file size is only the size of the data, but with my testing on Windows, this large maximum file size seems to persist. An alternative database is RocksDB. This software is developed by Facebook, seems to be quite mature, and should also be able to save as a key-value pair across platforms.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/file_io.html#prioritization-and-future-considerations",
    "href": "developer/file_io.html#prioritization-and-future-considerations",
    "title": "File IO",
    "section": "Prioritization and Future Considerations",
    "text": "Prioritization and Future Considerations\nCurrently, the data that would benefit from key-value updates is only theoretical. Having a line for whiskers for each frame in a high-speed video of up to 200,000 frames results in a file size that is almost 300 MB. However, saving a binary file in its entirety when making whisker-specific updates through manual curation still takes less than a second to save and is not noticeable to the user. Consequently, I think prioritizing having binary outputs to save entire files and memory mapping of those binary file types that are read-only, such as for Neuropixel analog traces, may be the most desirable next feature. Key-value pairs with something like RocksDB should be remembered for the future, but I am going to make that less of a priority until I see a clear use case that would benefit.",
    "crumbs": [
      "File IO"
    ]
  },
  {
    "objectID": "developer/Features/Collapsible_Widget.html",
    "href": "developer/Features/Collapsible_Widget.html",
    "title": "Collapsible Widget",
    "section": "",
    "text": "We have a Collapsible Widget called a “Section” available. This kind of widget can expand to display the contents of the widget. This is useful for making an interface appear less cluttered.\n\nGeneral process for to add collapsible Widget:\nQT Designer:\n\nEnsure the parent widget is in a vertical layout\nCreate QWidget inside and promote it to a “Section”\nCreate another QWidget within that QWidget. The inner QWidget will hold all the contents of the section.\nPopulate this inner QWidget with widgets as normal.\n\nCode part:\n\nCall autoSetContentLayout() on the Section QWidget after setupUi.\nOptionally, calling setTitle(\"title\") will give it a title.",
    "crumbs": [
      "Features",
      "Collapsible Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html",
    "href": "developer/data_viewer_widget.html",
    "title": "Data Viewer Widget",
    "section": "",
    "text": "The data viewer widget is designed for visualizing plots of time series data. It can operate on multiple distinct data types, such as analog time series, interval series, and event series. The widget contains a table of compatible data types and their corresponding keys, which can be selected for display in the viewer. It houses options to enable the visualization of different types and also includes an OpenGL canvas responsible for rendering the data according to user specifications.",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#event-viewer-features",
    "href": "developer/data_viewer_widget.html#event-viewer-features",
    "title": "Data Viewer Widget",
    "section": "Event Viewer Features",
    "text": "Event Viewer Features\nThe EventViewer_Widget provides controls for digital event series visualization with two main display modes:\n\nDisplay Modes\n\nStacked Mode (Default): Events are positioned in separate horizontal lanes with configurable spacing\n\nEach event series gets its own horizontal “lane”\nConfigurable vertical spacing between lanes\nConfigurable event line height\nAuto-calculation of optimal spacing when event groups are loaded\n\nFull Canvas Mode: Events stretch from top to bottom of the entire canvas\n\nOriginal behavior maintained for compatibility\nAll events span the full height of the display area\n\n\n\n\nAuto-Spacing for Event Groups\nWhen a tree of digital event series is selected and enabled, the system automatically calculates optimal spacing to fit all events on the canvas:\n\nIf 40 events are enabled on a 400-pixel tall canvas, each event gets approximately 10 pixels of space\nSpacing and height are calculated to ensure visual separation between different event series\nUses 80% of available canvas height, leaving margins at top and bottom\nEvent height is set to 60% of calculated spacing to prevent overlap\n\n\n\nUser Controls\nThe EventViewer_Widget provides: - Display Mode: Combo box to switch between Stacked and Full Canvas modes - Vertical Spacing: Adjustable spacing between stacked event series (0.01-1.0 normalized units) - Event Height: Adjustable height of individual event lines (0.01-0.5 normalized units) - Color Controls: Standard color picker for event line colors and transparency\n\n\nImplementation Details\n\nEvent stacking uses normalized coordinates for consistent spacing across different canvas sizes\nState is preserved when switching between data series\nIntegration with existing TreeWidgetStateManager for persistence\nOptimized rendering with proper OpenGL line thickness and positioning\n\n\n\nTime Frame Synchronization\nThe DataViewer Widget properly handles multi-rate data synchronization:\n\nMaster Time Frame: OpenGLWidget maintains a reference to the master time frame (“master” or “time”) used for X-axis coordinates\nTime Frame Conversion: When data series use different time frames from the master, proper coordinate conversion ensures synchronized display\nCross-Rate Compatibility: Supports simultaneous visualization of data collected at different sampling rates (e.g., 30kHz electrophysiology with 500Hz video)\nConsistent X-Axis: All data types (analog time series, digital events, digital intervals) are rendered with consistent X-axis positioning regardless of their native time frame\nRange Query Optimization: Data range queries are optimized for each time frame to ensure efficient rendering of large datasets",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#interactive-interval-editing",
    "href": "developer/data_viewer_widget.html#interactive-interval-editing",
    "title": "Data Viewer Widget",
    "section": "Interactive Interval Editing",
    "text": "Interactive Interval Editing\nThe DataViewer Widget supports interactive editing of digital interval series through a mouse-based dragging interface that handles multi-timeframe data seamlessly.\n\nInterval Selection and Highlighting\n\nClick-to-Select: Users can click on digital intervals to select them for editing\nVisual Feedback: Selected intervals are highlighted with enhanced borders for clear identification\nPer-Series Selection: Each digital interval series maintains its own independent selection state\n\n\n\nInterval Edge Dragging\nThe widget provides precise interval boundary editing through a sophisticated dragging system:\n\nCore Functionality\n\nEdge Detection: Mouse hover near interval boundaries (within 10 pixels) changes cursor to resize indicator\nDrag Initiation: Click and drag on interval edges to modify interval start or end times\nReal-time Preview: During dragging, both original (dimmed) and new (semi-transparent) interval positions are shown\nCollision Prevention: Automatic constraint enforcement prevents intervals from overlapping with existing intervals\n\n\n\nMulti-Timeframe Support\nThe interval dragging system automatically handles time frame conversion for data collected at different sampling rates:\n\nCoordinate Conversion: Mouse coordinates (in master time frame) are automatically converted to the series’ native time frame indices\nPrecision Handling: Dragged positions are rounded to the nearest valid index in the target time frame, accommodating different sampling resolutions\nConstraint Enforcement: Collision detection and boundary constraints are performed in the series’ native time frame for accuracy\nDisplay Consistency: Visual feedback remains in master time frame coordinates for consistent user experience\n\n\n\nError Handling and Robustness\n\nGraceful Degradation: Failed time frame conversions abort the drag operation while preserving original data\nData Integrity: Invalid interval bounds (e.g., start ≥ end) are rejected without modifying existing data\nState Management: Drag operations can be cancelled (ESC key) to restore original interval boundaries\n\n\n\nExample Use Cases\n\nBehavioral Annotation: Researchers can precisely adjust behavioral event boundaries recorded at video frame rates (30-120 Hz) while viewing synchronized neural data at higher sampling rates (20-30 kHz)\nEvent Refinement: Fine-tune automatically detected events by dragging boundaries to match observed signal characteristics across different data modalities\nCross-Modal Synchronization: Align interval boundaries across different measurement systems with varying temporal resolutions\n\n\n\n\nTechnical Implementation\nThe interval editing system leverages the existing TimeFrame infrastructure:\n\nTimeFrame.getIndexAtTime(): Converts master time coordinates to series-specific indices\nTimeFrame.getTimeAtIndex(): Converts series indices back to master time coordinates for display\nAutomatic Snapping: Ensures all interval boundaries align with valid time points in the target series\nThread Safety: All operations maintain data consistency during concurrent access\n\nThis functionality enables precise temporal analysis workflows while abstracting away the complexity of multi-rate data synchronization from the end user.",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#vertical-space-coordination",
    "href": "developer/data_viewer_widget.html#vertical-space-coordination",
    "title": "Data Viewer Widget",
    "section": "Vertical Space Coordination",
    "text": "Vertical Space Coordination\nThe DataViewer_Widget includes a sophisticated vertical space management system that prevents overlap between different data types and ensures optimal use of screen real estate.\n\nVerticalSpaceManager\nThe VerticalSpaceManager class handles automatic positioning and scaling for all data series:\n\nOrder-preserving: New data positioned below existing data\nType-aware spacing: Different configurations for analog (larger heights), digital events (compact), and intervals (moderate)\nAuto-redistribution: Adding new series triggers recalculation to prevent overlap\nCanvas-independent: Uses normalized coordinates for flexibility\n\n\n\nMVP Matrix Architecture\nThe rendering system uses a systematic Model-View-Projection (MVP) matrix approach for consistent positioning and scaling across all data types:\n\nModel Matrix - Series-Specific Transforms\nHandles individual series positioning and scaling:\n// For VerticalSpaceManager-positioned series:\nModel = glm::translate(Model, glm::vec3(0, series_center_y, 0));  // Position\nModel = glm::scale(Model, glm::vec3(1, series_height * 0.5f, 1)); // Scale\n\n// For analog series, additional amplitude scaling:\nfloat amplitude_scale = 1.0f / (stdDev * scale_factor);\nModel = glm::scale(Model, glm::vec3(1, amplitude_scale, 1));\n\n\nView Matrix - Global Operations\nHandles operations applied to all series equally:\nView = glm::translate(View, glm::vec3(0, _verticalPanOffset, 0)); // Global panning\n\n\nProjection Matrix - Coordinate System Mapping\nMaps world coordinates to screen coordinates:\n// X axis: time range [start_time, end_time] → screen width\n// Y axis: world coordinates [min_y, max_y] → screen height\nProjection = glm::ortho(start_time, end_time, min_y, max_y);\n\n\nVertex Coordinate Systems\nVerticalSpaceManager Mode (recommended): - Vertices use normalized coordinates (-1 to +1 in local space) - Model matrix handles all positioning and scaling - Consistent across all data types\nLegacy Mode (backward compatibility): - Vertices use world coordinates directly - Positioning handled by coordinate calculations - Index-based spacing for events\n\n\nDetection of Positioning Mode\nThe system automatically detects which positioning mode to use:\n\nDigital Events: vertical_spacing == 0.0f signals VerticalSpaceManager mode\nAnalog Series: y_offset != 0.0f signals VerticalSpaceManager mode\n\nDigital Intervals: y_offset != 0.0f signals VerticalSpaceManager mode\n\nThis ensures backward compatibility while enabling the new systematic approach.",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_viewer_widget.html#interval-editing-system",
    "href": "developer/data_viewer_widget.html#interval-editing-system",
    "title": "Data Viewer Widget",
    "section": "Interval Editing System",
    "text": "Interval Editing System",
    "crumbs": [
      "Data Viewer Widget"
    ]
  },
  {
    "objectID": "developer/data_structures.html",
    "href": "developer/data_structures.html",
    "title": "Data Structure Performance",
    "section": "",
    "text": "This document provides profiling information and design rationale for the core data structures used in WhiskerToolbox. Understanding the performance characteristics of these structures is essential for making informed architectural decisions."
  },
  {
    "objectID": "developer/data_structures.html#current-implementation",
    "href": "developer/data_structures.html#current-implementation",
    "title": "Data Structure Performance",
    "section": "Current Implementation",
    "text": "Current Implementation\nThe current implementation uses a map of vectors:\nstd::map&lt;TimeFrameIndex, std::vector&lt;DataEntry&lt;TData&gt;&gt;&gt; _data;\n\ntemplate&lt;typename TData&gt;\nstruct DataEntry {\n    TData data;\n    EntityId entity_id;\n};\n\nCharacteristics\n\n\n\n\n\n\n\n\nOperation\nComplexity\nNotes\n\n\n\n\nLookup by TimeFrameIndex\nO(log n)\nMap lookup\n\n\nLookup by EntityId\nO(n)\nFull scan required\n\n\nInsert at time (append)\nO(log n)\nMap insert + vector push_back\n\n\nInsert at time (middle)\nO(log n + k)\nk = entries at that time\n\n\nIterate all entries\nO(n)\nMust traverse map + vectors\n\n\nMemory layout\nFragmented\nMany small vector allocations"
  },
  {
    "objectID": "developer/data_structures.html#alternative-structure-of-arrays-soa",
    "href": "developer/data_structures.html#alternative-structure-of-arrays-soa",
    "title": "Data Structure Performance",
    "section": "Alternative: Structure of Arrays (SoA)",
    "text": "Alternative: Structure of Arrays (SoA)\nAn alternative storage strategy uses parallel arrays:\ntemplate&lt;typename TData&gt;\nstruct SoAStorage {\n    std::vector&lt;TimeFrameIndex&gt; times;\n    std::vector&lt;LocalIndex&gt; local_indices;\n    std::vector&lt;TData&gt; data;\n    std::vector&lt;EntityId&gt; entity_ids;\n    \n    // Acceleration structures\n    std::map&lt;TimeFrameIndex, std::pair&lt;size_t, size_t&gt;&gt; time_ranges;\n    std::unordered_map&lt;EntityId, size_t&gt; entity_to_index;\n};\n\nCharacteristics\n\n\n\nOperation\nComplexity\nNotes\n\n\n\n\nLookup by TimeFrameIndex\nO(log n)\nVia time_ranges map\n\n\nLookup by EntityId\nO(1)\nVia entity_to_index map\n\n\nAppend\nO(1) amortized\nVector push_back\n\n\nMiddle insert\nO(n)\nMust shift all arrays\n\n\nIterate all entries\nO(n)\nContiguous memory\n\n\nMemory layout\nContiguous\nCache-friendly"
  },
  {
    "objectID": "developer/data_structures.html#profiling-trivially-copyable-vs-non-trivially-copyable-types",
    "href": "developer/data_structures.html#profiling-trivially-copyable-vs-non-trivially-copyable-types",
    "title": "Data Structure Performance",
    "section": "Profiling: Trivially Copyable vs Non-Trivially Copyable Types",
    "text": "Profiling: Trivially Copyable vs Non-Trivially Copyable Types\nA critical performance consideration is whether the stored type is trivially copyable.\n\nWhat is Trivially Copyable?\nA type is trivially copyable if it can be copied with a simple memcpy/memmove:\n// Trivially copyable - just raw bytes\nstruct Point2D {\n    float x, y;  // 8 bytes total\n};\nstatic_assert(std::is_trivially_copyable_v&lt;Point2D&gt;);\n\n// NOT trivially copyable - contains std::vector\nstruct Mask2D {\n    std::vector&lt;uint8_t&gt; data;  // 24 bytes (pointer + size + capacity)\n};\nstatic_assert(!std::is_trivially_copyable_v&lt;Mask2D&gt;);\n\n\nWhy Does It Matter?\nWhen inserting into the middle of a std::vector:\nFor trivially copyable types:\n// Compiler generates a single memmove call\nmemmove(dest, src, num_elements * sizeof(T));\nFor non-trivially copyable types:\n// Compiler must call move constructor for EACH element\nfor (size_t i = num_elements; i &gt; insert_pos; --i) {\n    new (&dest[i]) T(std::move(src[i-1]));\n    src[i-1].~T();\n}\n\n\nDoes Data Size Inside Mask2D Affect Performance?\nNo! The move constructor for std::vector just swaps 3 pointers:\n// std::vector move constructor (simplified)\nvector(vector&& other) noexcept {\n    _data = other._data;           // Steal the pointer\n    _size = other._size;\n    _capacity = other._capacity;\n    other._data = nullptr;         // Leave source empty\n}\nMeasured results with 200,000 elements and 100 middle insertions:\n\n\n\nMask data size\nTime per insert\n\n\n\n\n0 bytes\n~148 µs\n\n\n100 bytes\n~172 µs\n\n\n1 KB\n~145 µs\n\n\n\nThe times are essentially identical regardless of mask content size."
  },
  {
    "objectID": "developer/data_structures.html#profiling-middle-insertion-performance",
    "href": "developer/data_structures.html#profiling-middle-insertion-performance",
    "title": "Data Structure Performance",
    "section": "Profiling: Middle Insertion Performance",
    "text": "Profiling: Middle Insertion Performance\nThe following benchmark compares insertion performance across different data types and storage strategies.\n\nResults\n=============================================================================\nRaggedTimeSeries Storage Benchmark\n=============================================================================\n\nConfiguration:\n  Initial elements: 200000\n  Middle insertions: 1000\n  Average elements moved: 100000\n\n-----------------------------------------------------------------------------\nMIDDLE INSERTION PERFORMANCE\n-----------------------------------------------------------------------------\nType                              Size    Per Insert    Bytes Moved\n-----------------------------------------------------------------------------\n\n-- Trivially Copyable Types (uses memmove) --\nPoint2D&lt;float&gt;                  8 bytes        18.6 µs        800000 bytes\nEntityId                        8 bytes        18.7 µs        800000 bytes\nTimeFrameIndex                  8 bytes        19.2 µs        800000 bytes\nSoAKey (composite)             24 bytes        65.0 µs       2400000 bytes\n\n-- Non-Trivially Copyable Types (move constructor per element) --\nMask2D (empty)                 24 bytes       190.7 µs       2400000 bytes\nMask2D (100 bytes)             24 bytes       210.9 µs       2400000 bytes\nMask2D (1KB)                   24 bytes       155.7 µs       2400000 bytes\nLine2D (empty)                 24 bytes       159.6 µs       2400000 bytes\nLine2D (50 points)             24 bytes       169.0 µs       2400000 bytes\n\n-----------------------------------------------------------------------------\nAPPEND PERFORMANCE (for comparison)\n-----------------------------------------------------------------------------\nType                        Per Append\n-----------------------------------------------------------------------------\nPoint2D&lt;float&gt;                 1.38 ns\nSoAKey (composite)             5.14 ns\nMask2D (empty)                 2.66 ns\nMask2D (1KB)                 245.80 ns\n\n\nAnalysis\nKey findings:\n\nTrivially copyable types are 3-10× faster for middle insertion:\n\nPoint2D (8 bytes): 18.6 µs per insert\nSoAKey (24 bytes): 65.0 µs per insert\n\nMask2D (24 bytes, non-trivial): 155-210 µs per insert\n\nData size inside containers doesn’t matter for moves:\n\nMask2D with 0 bytes: 190.7 µs\nMask2D with 1KB: 155.7 µs (within noise)\n\nThis is because std::vector’s move constructor just swaps 3 pointers.\nAppend is dramatically faster than middle insertion:\n\nPoint2D append: 1.38 ns (13,000× faster than insert)\nMask2D append: 2.66 ns (60,000× faster than insert)\n\nFor SoA with views: Keep metadata (TimeFrameIndex, EntityId) in trivially copyable arrays; keep actual data (Mask2D) in append-only storage."
  },
  {
    "objectID": "developer/data_structures.html#profiling-storage-strategy-comparison",
    "href": "developer/data_structures.html#profiling-storage-strategy-comparison",
    "title": "Data Structure Performance",
    "section": "Profiling: Storage Strategy Comparison",
    "text": "Profiling: Storage Strategy Comparison\nThis benchmark compares the current map-based implementation against the SoA approach.\n\nResults\n=============================================================================\nStorage Strategy Benchmark: Map-Based vs Structure of Arrays\n=============================================================================\n\nConfiguration:\n  Total entries: 200000\n  Time points: 50000\n  Avg entries per time: 4\n  Lookups per test: 10000\n  Filter set size: 1000\n\n-----------------------------------------------------------------------------\nCONSTRUCTION (append 200000 entries in time order)\n-----------------------------------------------------------------------------\nMap-Based:  35.3 ms\nSoA:        85.5 ms\nSpeedup:    0.41x\n\n-----------------------------------------------------------------------------\nLOOKUP BY EntityId (10000 random lookups)\n-----------------------------------------------------------------------------\nMap-Based:  9105.2 ms total, 910.5 µs per lookup\nSoA:        0.7 ms total, 69.4 ns per lookup\nSpeedup:    13120x\n\n-----------------------------------------------------------------------------\nLOOKUP BY TimeFrameIndex (10000 random lookups)\n-----------------------------------------------------------------------------\nMap-Based:  2.9 ms total, 293.6 ns per lookup\nSoA:        4.0 ms total, 403.5 ns per lookup\nSpeedup:    0.73x\n\n-----------------------------------------------------------------------------\nFILTER BY EntityId SET (filter 1000 EntityIds)\n-----------------------------------------------------------------------------\nMap-Based (full copy):    8.1 ms, 998 results\nSoA (scan, indices only): 4.6 ms, 998 results\nSoA (hash, indices only): 0.1 ms, 998 results\nSpeedup (hash vs map):    75x\n\n-----------------------------------------------------------------------------\nITERATE ALL ENTRIES\n-----------------------------------------------------------------------------\nMap-Based:  2.8 ms\nSoA:        0.5 ms\nSpeedup:    5.17x\n\n\nAnalysis\n\n\n\nOperation\nMap-Based\nSoA\nWinner\n\n\n\n\nConstruction\n35.3 ms\n85.5 ms\nMap (2.4×)\n\n\nEntityId Lookup\n910 µs\n69 ns\nSoA (13,120×)\n\n\nTimeFrameIndex Lookup\n294 ns\n404 ns\nMap (1.4×)\n\n\nEntityId Filter (1000 ids)\n8.1 ms\n0.1 ms\nSoA (75×)\n\n\nIterate All\n2.8 ms\n0.5 ms\nSoA (5×)\n\n\n\nKey findings:\n\nEntityId operations are dramatically faster with SoA:\n\nSingle lookup: 13,120× faster (910 µs → 69 ns)\nFiltering 1000 EntityIds: 75× faster (8.1 ms → 0.1 ms)\n\nThis is because SoA maintains an unordered_map&lt;EntityId, size_t&gt; for O(1) lookup, while map-based requires O(n) full scan.\nTimeFrameIndex lookup is slightly faster with Map:\n\nBoth are O(log m) where m = unique time points\nMap wins by ~1.4× due to direct access vs indirection\n\nConstruction favors Map:\n\nMap: 35.3 ms\nSoA: 85.5 ms (2.4× slower)\n\nThis is because SoA maintains additional acceleration structures.\nIteration strongly favors SoA:\n\n5× faster due to cache-friendly sequential memory access\nMap traverses fragmented memory (many small vectors)\n\nFor view-based filtering, the SoA approach returns indices only (8 bytes each), while map-based must copy the entire DataEntry&lt;TData&gt; including the data."
  },
  {
    "objectID": "developer/data_structures.html#implications-for-view-based-filtering",
    "href": "developer/data_structures.html#implications-for-view-based-filtering",
    "title": "Data Structure Performance",
    "section": "Implications for View-Based Filtering",
    "text": "Implications for View-Based Filtering\nOne key use case is creating filtered views of data (e.g., by EntityId set or TimeFrameIndex range).\n\nCurrent Approach: Copy on Filter\n// Creates a new RaggedTimeSeries with copied data\nauto filtered = source.copyByEntityIds(entity_set);\n\n\nAlternative: Index-Based Views\nWith SoA storage, filtering can return a lightweight view:\n// View is just a vector of indices into the source\nstruct RaggedView {\n    SoAStorage const* source;\n    std::vector&lt;size_t&gt; indices;  // Which entries are included\n};\n\n// Creating a filtered view - O(n) scan, O(m) allocation for m matches\n// No data copying!\nauto filtered_view = source.createEntityIdView(entity_set);\n\n\nPerformance Comparison\n\n\n\nOperation\nCopy Approach\nView Approach\n\n\n\n\nCreate filter\nO(n) + copy TData\nO(n) + O(m) indices\n\n\nMemory usage\nFull copy\nJust indices (8 bytes each)\n\n\nAccess element\nO(1)\nO(1) + 1 indirection\n\n\nModify source\nIndependent\nView sees changes"
  },
  {
    "objectID": "developer/data_structures.html#recommendations",
    "href": "developer/data_structures.html#recommendations",
    "title": "Data Structure Performance",
    "section": "Recommendations",
    "text": "Recommendations\nBased on the profiling results:\n\nAppend-only data: SoA storage is highly efficient for data that arrives chronologically. With 200,000 entries, iteration is 5× faster due to cache-friendly memory layout.\nFrequent EntityId lookups: The current map-based implementation is O(n) per lookup (910 µs with 200k entries). Adding an entity_to_index map provides O(1) access (69 ns), a 13,000× improvement.\nView-based filtering: Use index vectors rather than copying data. Filtering 1000 EntityIds takes 8.1 ms with copy vs 0.1 ms with indices only (75× faster).\nMiddle insertions: Avoid when possible; batch operations and sort. A single middle insertion into 200k Mask2D elements costs ~170 µs, while append costs ~3 ns.\nTimeFrameIndex operations: The current map-based approach is already optimal for time-based lookup (O(log m)). SoA provides similar performance.\nHybrid approach: For best overall performance:\n\nKeep Mask2D/Line2D data in append-only storage (arena)\nMaintain parallel vectors of trivially-copyable metadata (TimeFrameIndex, EntityId)\nUse hash maps for O(1) EntityId lookup\nViews hold index vectors referencing the source data"
  },
  {
    "objectID": "developer/data_structures.html#derived-storage-views-performance",
    "href": "developer/data_structures.html#derived-storage-views-performance",
    "title": "Data Structure Performance",
    "section": "Derived Storage (Views) Performance",
    "text": "Derived Storage (Views) Performance\nWhen creating filtered views that reference source data without copying, we measured the overhead of index indirection.\n\nResults\n=============================================================================\nDerived Storage Overhead Benchmark\n=============================================================================\n\nConfiguration:\n  Source entries: 200000\n  View size: 10000 (5% of source)\n\n-----------------------------------------------------------------------------\nSEQUENTIAL ITERATION (all elements)\n-----------------------------------------------------------------------------\nDirect array access:     7438.8 µs  (37.2 ns/element)\nSource (virtual):        6958.1 µs  (34.8 ns/element)\nDerived 'all' view:      6627.0 µs  (33.1 ns/element)\n\nOverhead analysis (vs direct array):\n  Virtual dispatch:      0.94x\n  View indirection:      0.89x\n\n-----------------------------------------------------------------------------\nFILTERED VIEW ITERATION (subset)\n-----------------------------------------------------------------------------\nFiltered view (9742 entries): 1724.7 µs  (177.0 ns/element)\nSource + filter check:   7853.9 µs  (scanning all 200000)\n\nFiltered view speedup:   4.6x faster\n\n-----------------------------------------------------------------------------\nCACHE EFFECTS: SEQUENTIAL vs RANDOM VIEW INDICES\n-----------------------------------------------------------------------------\nSequential indices:      306.9 µs  (30.7 ns/element)\nRandom indices:          1670.0 µs  (171.2 ns/element)\n\nCache penalty:           5.44x slower\n\n-----------------------------------------------------------------------------\nMEMORY OVERHEAD\n-----------------------------------------------------------------------------\nSource storage:\n  Total:                  30468 KB\n\nDerived view (9742 entries):\n  Index vector:           76 KB\n  Local EntityId map:     152 KB\n  Data copied:            0 KB (references source)\n  Total:                  228 KB\n\nMemory savings:          82.9% vs full copy\n\n\nAnalysis\n\n\n\n\n\n\n\n\nMetric\nResult\nNotes\n\n\n\n\nVirtual dispatch overhead\n0.94x (faster!)\nCompiler optimizes well\n\n\nIndex indirection overhead\n0.89x (faster!)\nCPU prefetching helps\n\n\nFiltered iteration\n4.6x faster\nvs scanning all + filter check\n\n\nCache penalty (random indices)\n5.4x slower\n171 ns vs 31 ns per element\n\n\nMemory savings\n83%\n228 KB view vs full copy\n\n\nView creation\n233 ns/EntityId\nOne-time cost\n\n\n\nKey insights:\n\nVirtual dispatch is essentially free - The compiler devirtualizes with -O3\nIndex indirection overhead is negligible for sequential access due to CPU prefetching\nThe REAL cost is cache misses with random access patterns (5.4x penalty)\nViews pay for themselves quickly - Creation cost is amortized in &lt; 1 iteration\nSort view indices when possible for cache-friendly access"
  },
  {
    "objectID": "developer/data_structures.html#crtp-variant-vs-virtual-inheritance",
    "href": "developer/data_structures.html#crtp-variant-vs-virtual-inheritance",
    "title": "Data Structure Performance",
    "section": "CRTP + Variant vs Virtual Inheritance",
    "text": "CRTP + Variant vs Virtual Inheritance\nAn alternative to virtual inheritance for polymorphic storage is the CRTP (Curiously Recurring Template Pattern) combined with std::variant for type erasure. This pattern is already used in AnalogDataStorage.hpp.\n\nThe Two Approaches\nVirtual Inheritance (Classic OOP):\ntemplate&lt;typename TData&gt;\nclass IRaggedStorage {\npublic:\n    virtual ~IRaggedStorage() = default;\n    virtual size_t size() const = 0;\n    virtual TData const& getData(size_t idx) const = 0;\n    // ... other pure virtual methods\n};\n\nclass SourceStorage : public IRaggedStorage&lt;Mask2D&gt; { /* ... */ };\nclass DerivedStorage : public IRaggedStorage&lt;Mask2D&gt; { /* ... */ };\n\n// Usage via base pointer\nstd::shared_ptr&lt;IRaggedStorage&lt;Mask2D&gt;&gt; storage = /*...*/;\nauto& data = storage-&gt;getData(i);  // vtable dispatch\nCRTP + Variant (Modern C++):\ntemplate&lt;typename Derived, typename TData&gt;\nclass StorageBase {\npublic:\n    // Static dispatch via CRTP\n    size_t size() const { \n        return static_cast&lt;Derived const*&gt;(this)-&gt;sizeImpl(); \n    }\n    // ...\n};\n\nclass SourceStorage : public StorageBase&lt;SourceStorage, Mask2D&gt; { /* ... */ };\nclass DerivedStorage : public StorageBase&lt;DerivedStorage, Mask2D&gt; { /* ... */ };\n\n// Type erasure via variant\nusing StorageVariant = std::variant&lt;\n    std::shared_ptr&lt;SourceStorage&gt;,\n    std::shared_ptr&lt;DerivedStorage&gt;\n&gt;;\n\nStorageVariant storage = /*...*/;\nauto& data = std::visit([i](auto& ptr) -&gt; auto& { \n    return ptr-&gt;getData(i); \n}, storage);  // std::visit dispatch\n\n\nResults\n=============================================================================\nCRTP + Variant vs Virtual Inheritance Benchmark\n=============================================================================\n\nConfiguration:\n  Entries: 200000\n  View size: 10000\n  Iterations: 100\n\n=============================================================================\nPART 1: Point2D (trivially copyable) - Isolating Dispatch Overhead\n=============================================================================\n\nPOINT2D: SEQUENTIAL ITERATION (dispatch overhead visible)\nVirtual (base ptr):      691.7 µs  (3.5 ns/elem)\nCRTP (direct):           332.3 µs  (1.7 ns/elem)\nCRTP (variant):          320.8 µs  (1.6 ns/elem)\n\nSpeedup vs Virtual:\n  CRTP direct:  2.08x\n  CRTP variant: 2.16x\n\n=============================================================================\nPART 2: Mask2D (non-trivially copyable) - Data Access Dominates\n=============================================================================\n\nSOURCE STORAGE: SEQUENTIAL ITERATION\nVirtual (base ptr):      7366.7 µs  (36.8 ns/elem)\nCRTP (direct):           6635.9 µs  (33.2 ns/elem)\nCRTP (variant):          7000.2 µs  (35.0 ns/elem)\n\nSpeedup vs Virtual:\n  CRTP direct:  1.11x\n  CRTP variant: 1.05x\n\nENTITY ID LOOKUP (10000 lookups)\nVirtual:                 4406.9 µs  (440.7 ns/lookup)\nCRTP (direct):           3778.3 µs  (377.8 ns/lookup)\nCRTP (variant):          2993.7 µs  (299.4 ns/lookup)\n\nSpeedup vs Virtual:\n  CRTP direct:  1.17x\n  CRTP variant: 1.47x\n\n\nAnalysis Summary\n\n\n\nAccess Pattern\nVirtual\nCRTP Direct\nCRTP Variant\n\n\n\n\nPoint2D iteration\n3.5 ns/elem\n1.7 ns\n1.6 ns\n\n\nMask2D iteration\n36.8 ns/elem\n33.2 ns\n35.0 ns\n\n\nEntityId lookup\n440.7 ns\n377.8 ns\n299.4 ns\n\n\n\nKey insights:\n\nCRTP is 2× faster for lightweight data (Point2D):\n\nDispatch overhead becomes visible when data access is cheap\nBoth direct and variant approaches eliminate vtable lookup\n\nWhen data access dominates, dispatch doesn’t matter:\n\nFor Mask2D (vector-backed), virtual vs CRTP vs variant are within 10%\nThe CPU spends most time chasing pointers, not dispatching\n\nCRTP Variant wins on hash lookups (1.47× faster):\n\nstd::visit can be faster than virtual when compiler can see all types\nBetter inlining opportunities\n\nWhen to use each approach:\n\n\n\nApproach\nBest For\n\n\n\n\nVirtual\nOpen extension (plugins), external libraries\n\n\nCRTP Direct\nMaximum performance, type known at compile time\n\n\nCRTP Variant\nClosed type set, value semantics, pattern matching\n\n\n\nCRTP + Variant advantages:\n\nNo heap allocation for small types (in-place storage)\nExhaustive handling enforced by compiler (std::visit)\nNo virtual destructor overhead\nBetter optimization across type boundaries\n\nCRTP + Variant disadvantages:\n\nMore complex code\nAll types must be known at compile time\nLarger object size (variant stores largest type + discriminant)\nRecompile required to add new types"
  },
  {
    "objectID": "developer/data_manager.html",
    "href": "developer/data_manager.html",
    "title": "Data Manager",
    "section": "",
    "text": "The DataManager has 3 core functionalities",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/data_manager.html#time-frame-structure-for-multi-rate-data-visualization",
    "href": "developer/data_manager.html#time-frame-structure-for-multi-rate-data-visualization",
    "title": "Data Manager",
    "section": "Time Frame Structure for Multi-Rate Data Visualization",
    "text": "Time Frame Structure for Multi-Rate Data Visualization\nAn important feature of the program is being able to simultaneously visualize data that was collected at different sampling rates. The time frame structure is designed to indicate when time events occur at a particular sampling frequency. The data manager is responsible for keeping track of which data is located in what time frame. Multiple data objects can belong to one time frame, but one data object can only belong to a single time frame. However, if the relationship between time frame objects is specified, then data requested in one coordinate system can be converted to another.\n\nIllustrative Example: Electrophysiology and Video Data\nConsider the following example: an NI-DAQ box samples at 30000 Hz for electrophysiology. This results in analog data with 30000 samples per second as well as event data for sorted spikes at 30000 Hz resolution. The experiment may have also had a high-speed video camera collecting frames at 500 Hz. A digital event for each frame was recorded with the same NI-DAQ. The user may have processed the video frames to categorize behavior, which would also be at the 500 Hz resolution and be interval data.\n\n\nDefault Time Frame and Customization\nThe default time frame is simply called “time” and defaults to numbers between 1 and the number of frames in a loaded video. The scroll bar operates in this time frame and consequently sends signals in this time frame. The user can override this default time frame and replace it with an event structure with the same number of samples but where each event corresponds to the 30000 Hz resolution digitized camera frame exposures. The user can then create a new clock called “master” that again counts from 1,2,3,… up to the total number of samples collected by the NI-DAQ.\n\n\nData Indexing within Time Frames\nAll data manager data types have a notion of “index,” and these correspond to the time frame they are associated with. This index property is important because data may be sparsely labeled and not have the same number of samples as their time frame.\n\n\nWidget Considerations for Data Synchronization\nWidgets that represent different data simultaneously must be aware of accounting for these differences.\n\n\nPerformance Notes\nThe user should also be aware that pointer indirections sample by sample for large vectors will be quite inefficient. If the user needs to find a series of values in a range in a different coordinate system it is most likely important to.",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/data_manager.html#observer-framework",
    "href": "developer/data_manager.html#observer-framework",
    "title": "Data Manager",
    "section": "Observer Framework",
    "text": "Observer Framework",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/data_manager.html#data-types",
    "href": "developer/data_manager.html#data-types",
    "title": "Data Manager",
    "section": "Data Types",
    "text": "Data Types\n\nPoint\nA Point represents a 2 dimensional (x, y) coordinate, which may vary in time. The PointData structure can hold multiple points per unit time.\n\n\n\nExamples of Point Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLine\nA Line represents a 2 dimensional (x,y) collection of points, which may vary with time. The collection of points is ordered, meaning that each point is positioned relative to its neighbors. The LineData structure can hold multiple lines per unit time.\n\n\n\nExamples of Line Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurrently, lines are represented by a raw list of points. One could imagine lines being parameterized in different ways, such as the coefficients of a polynomial fit. For many calculations, the parameterized version of a line may be less memory intensive and more efficient.\n\n\nMask\nA mask represents a 2 dimensional collection of points, which may vary with time. Compared to a Line, the points in a Mask are not ordered. These would correspond to something like a binary segmentation mask over an image. The MaskData structure can hold multiple masks per unit time.\n\n\n\nExamples of Mask Data\n\n\n\n\nBinary Semantic Segmentation Labels\n\n\n\n\n\n\n\n\n\nA mask may just be thought as a raw pixel, by pixel definition of a shape. Shapes could be defined as bounding boxes, circles, polygons, etc. It may one day be useful to describe shapes in other ways compared to the raw pixel-by-pixel definition.\n\n\nTensors\nTensors are N-Dimensional data structures, and consequently very flexible containers for complex data. A concrete use would be to store a Height x Width x Channel array for different timepoints during an experiment. These may be the features output from an encoder neural network that processes a video.\n\n\n\nExamples of Tensor Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalog Time Series\nAn analog time series has values that can vary continuously\n\n\n\nExamples of Analog Time Series Data\n\n\n\n\nVoltage traces from electrode recordings\n\n\nFluorescence intensity traces from Calcium imaging\n\n\n\n\n\n\n\n\nDigital Event Series\nA digital event represents an ordered sequence of events, where each event is represented as a single instance in time. For instance, spike times from electrophysiology\n\n\n\nExamples of Event Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigital Interval Series\n\n\n\nExamples of Interval Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedia\nMedia is a sequence of images.\n\nImage\n\n\n\nExamples of Image Data\n\n\n\n\nImage sequence from two photon calcium imaging experiment\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\nExamples of Video Data\n\n\n\n\nMP4 video from high speed scientific camera of behavior\n\n\n\n\n\n\n\n\n\n\n\n\nTime Frame",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/contributing_code.html",
    "href": "developer/contributing_code.html",
    "title": "Contributing Code",
    "section": "",
    "text": "Header files should use the hpp suffix, and source files should use the cpp suffix\nIncludes should follow the “Lakos” include order, that is\n\nThe prototype/interface header for this implementation (ie, the .h/.hh file that corresponds to this .cpp/.cc file).\nOther headers from the same project, as needed.\nHeaders from other non-standard, non-system libraries (for example, Qt, Eigen, etc).\nHeaders from other “almost-standard” libraries (for example, Boost)\nStandard C++ headers (for example, iostream, functional, etc.)\nStandard C headers (for example, cstdint, dirent.h, etc.)\n\nPrefer returning std::optional as a mechanism of error handling\nThis is a scientific computing library. Performance is critical. Helping the user to understand where errors have occurred is helpful, but keeping the program alive after an error is not critical. Functions should fail gracefully and provide informative error messages when they do. Logging should use spdlog.\nPrefer free functions as much as possible. Ideally, class member functions will be simple and pass member variables to free functions.\nPrefer standard library algorithms where possible\nPublic member functions and free function declarations should include doxygen comments above them. Private member function definitions should include doxygen comments above them.\nPrefer forward declarations in header files\nDocument pre-conditions and post-conditions in doxygen comments uses the @pre and @post tags.\nThis is a C++20 project. Prefer standard library algorithms and std::ranges where possible.\nPrivate member variables and functions in classes should be prefaced with an underscore operator (e.g. calculateMean). In a struct with public facing member variables, they should be prefaced with m followed by an underscore (e.g. m_height).",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#design-guidelines",
    "href": "developer/contributing_code.html#design-guidelines",
    "title": "Contributing Code",
    "section": "",
    "text": "Header files should use the hpp suffix, and source files should use the cpp suffix\nIncludes should follow the “Lakos” include order, that is\n\nThe prototype/interface header for this implementation (ie, the .h/.hh file that corresponds to this .cpp/.cc file).\nOther headers from the same project, as needed.\nHeaders from other non-standard, non-system libraries (for example, Qt, Eigen, etc).\nHeaders from other “almost-standard” libraries (for example, Boost)\nStandard C++ headers (for example, iostream, functional, etc.)\nStandard C headers (for example, cstdint, dirent.h, etc.)\n\nPrefer returning std::optional as a mechanism of error handling\nThis is a scientific computing library. Performance is critical. Helping the user to understand where errors have occurred is helpful, but keeping the program alive after an error is not critical. Functions should fail gracefully and provide informative error messages when they do. Logging should use spdlog.\nPrefer free functions as much as possible. Ideally, class member functions will be simple and pass member variables to free functions.\nPrefer standard library algorithms where possible\nPublic member functions and free function declarations should include doxygen comments above them. Private member function definitions should include doxygen comments above them.\nPrefer forward declarations in header files\nDocument pre-conditions and post-conditions in doxygen comments uses the @pre and @post tags.\nThis is a C++20 project. Prefer standard library algorithms and std::ranges where possible.\nPrivate member variables and functions in classes should be prefaced with an underscore operator (e.g. calculateMean). In a struct with public facing member variables, they should be prefaced with m followed by an underscore (e.g. m_height).",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#before-you-submit-a-pull-request",
    "href": "developer/contributing_code.html#before-you-submit-a-pull-request",
    "title": "Contributing Code",
    "section": "Before You Submit a Pull Request",
    "text": "Before You Submit a Pull Request\n\nClang Format\nPlease make sure to run clang-format on all of your submitted files with the style guidelines in the base directory. A CI check will ensure that this happens upon pull request. You can read more about clang-format here:\nhttps://clang.llvm.org/docs/ClangFormat.html\n\n\nClang Tidy\nPlease make sure to run clang-tidy on all of your submitted files with the style guidelines in the base directory. A CI check will ensure that this happens upon pull request. You can read more about clang-tidy here:\nhttps://clang.llvm.org/extra/clang-tidy/",
    "crumbs": [
      "Contributing Code"
    ]
  },
  {
    "objectID": "developer/code_quality.html",
    "href": "developer/code_quality.html",
    "title": "Code Quality, Linters, and Analyzers",
    "section": "",
    "text": "Clang Format\n\n\nClang Tidy\n\n\nInclude What you Use (IWYU)\n\n\nCode Checker\n\n\nOptview2\n\n\nGamma Ray\n\n\nHeapTrack\n\n\nHotSpot\n\n\nTracy Profiler\nhttps://github.com/wolfpld/tracy\n\n\nInfer",
    "crumbs": [
      "Code Quality, Linters, and Analyzers"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html",
    "href": "developer/benchmark/benchmarking.html",
    "title": "Profiling and Performance",
    "section": "",
    "text": "This document describes the benchmarking infrastructure for Neuralyzer, including how to create benchmarks, run them, and analyze performance using various profiling tools.",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#quick-start",
    "href": "developer/benchmark/benchmarking.html#quick-start",
    "title": "Profiling and Performance",
    "section": "Quick Start",
    "text": "Quick Start\n# Build with benchmarks enabled (they're on by default)\ncmake --preset linux-clang-release -DENABLE_BENCHMARK=ON\ncmake --build --preset linux-clang-release\n\n# Run all benchmarks\ncd out/build/Clang/Release/benchmark\n./benchmark_MaskArea\n\n# Run specific benchmarks with filtering\n./benchmark_MaskArea --benchmark_filter=Pipeline\n\n# Run with different output formats\n./benchmark_MaskArea --benchmark_format=json &gt; results.json\n./benchmark_MaskArea --benchmark_format=csv &gt; results.csv",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#architecture",
    "href": "developer/benchmark/benchmarking.html#architecture",
    "title": "Profiling and Performance",
    "section": "Architecture",
    "text": "Architecture\n\nCMake Infrastructure\nThe benchmarking system is built on cmake/BenchmarkUtils.cmake, which provides:\n\nadd_selective_benchmark() - Create individual benchmark executables\nconfigure_benchmark_for_profiling() - Add profiling tool support\nprint_benchmark_summary() - Display configuration summary\n\nEach benchmark can be individually enabled/disabled via CMake options:\n# Disable a specific benchmark\ncmake -DBENCHMARK_MASK_AREA=OFF ..\n\n# Only build specific benchmarks\ncmake -DBENCHMARK_MASK_AREA=ON -DBENCHMARK_OTHER=OFF ..\n\n\nTest Data Fixtures\nbenchmark/fixtures/BenchmarkFixtures.hpp provides fixtures for generating realistic test data:\n\nMaskDataFixture - Generate MaskData with configurable properties\nLineDataFixture - Generate LineData for curve/line benchmarks\nPointDataFixture - Generate PointData for point-based benchmarks\n\nPresets are available for common scenarios: - Presets::SmallMaskData() - Quick iteration (10 frames) - Presets::MediumMaskData() - Realistic testing (100 frames) - Presets::LargeMaskData() - Stress testing (1000 frames) - Presets::SparseMaskData() - Few masks, large time gaps - Presets::DenseMaskData() - Many small masks per frame\nExample usage:\n#include \"fixtures/BenchmarkFixtures.hpp\"\n\nBENCHMARK_F(MyBenchmark, TestCase)(benchmark::State& state) {\n    auto fixture = MaskDataFixture(Presets::MediumMaskData());\n    auto mask_data = fixture.generate();\n    \n    for (auto _ : state) {\n        // Benchmark code here\n    }\n}",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#creating-new-benchmarks",
    "href": "developer/benchmark/benchmarking.html#creating-new-benchmarks",
    "title": "Profiling and Performance",
    "section": "Creating New Benchmarks",
    "text": "Creating New Benchmarks\n\n1. Create Benchmark Source File\nCreate benchmark/MyFeature.benchmark.cpp:\n#include \"fixtures/BenchmarkFixtures.hpp\"\n#include &lt;benchmark/benchmark.h&gt;\n\n// Simple function benchmark\nstatic void BM_MyFunction(benchmark::State& state) {\n    // Setup\n    auto data = setupTestData();\n    \n    for (auto _ : state) {\n        auto result = myFunction(data);\n        benchmark::DoNotOptimize(result);\n    }\n}\nBENCHMARK(BM_MyFunction);\n\n// Fixture-based benchmark\nclass MyFeatureBenchmark : public benchmark::Fixture {\npublic:\n    void SetUp(benchmark::State const& state) override {\n        // Generate test data once\n        test_data_ = generateData();\n    }\n    \nprotected:\n    std::shared_ptr&lt;DataType&gt; test_data_;\n};\n\nBENCHMARK_F(MyFeatureBenchmark, TestCase)(benchmark::State& state) {\n    for (auto _ : state) {\n        auto result = process(test_data_);\n        benchmark::DoNotOptimize(result);\n    }\n}\n\nBENCHMARK_MAIN();\n\n\n2. Register in CMake\nAdd to benchmark/CMakeLists.txt:\nadd_selective_benchmark(\n    NAME MyFeature\n    SOURCES \n        MyFeature.benchmark.cpp\n    LINK_LIBRARIES \n        DataManager\n        MyFeatureLib\n    DEFAULT ON\n)\n\n# Optional: Enable profiling support\nif(TARGET benchmark_MyFeature)\n    configure_benchmark_for_profiling(\n        TARGET benchmark_MyFeature\n        ENABLE_PERF ON\n        ENABLE_HEAPTRACK ON\n        GENERATE_ASM ON\n    )\nendif()\n\n\n3. Build and Run\ncmake --build --preset linux-clang-release\n./out/build/Clang/Release/benchmark/benchmark_MyFeature",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#google-benchmark-features",
    "href": "developer/benchmark/benchmarking.html#google-benchmark-features",
    "title": "Profiling and Performance",
    "section": "Google Benchmark Features",
    "text": "Google Benchmark Features\n\nParameterized Benchmarks\nTest with different input sizes:\nBENCHMARK(BM_MyFunction)\n    -&gt;Arg(100)\n    -&gt;Arg(1000)\n    -&gt;Arg(10000)\n    -&gt;Unit(benchmark::kMillisecond);\n\n// Or use ranges\nBENCHMARK(BM_MyFunction)\n    -&gt;Range(8, 8&lt;&lt;10)  // 8 to 8192, powers of 2\n    -&gt;RangeMultiplier(2);\n\n\nFixtures with Parameters\nBENCHMARK_F(MyBenchmark, TestCase)(benchmark::State& state) {\n    size_t size = state.range(0);\n    // Use size parameter\n}\nBENCHMARK_REGISTER_F(MyBenchmark, TestCase)\n    -&gt;DenseRange(0, 4)  // Parameters 0, 1, 2, 3, 4\n    -&gt;Unit(benchmark::kMicrosecond);\n\n\nCustom Counters\nTrack additional metrics:\nfor (auto _ : state) {\n    auto result = myFunction(data);\n    state.counters[\"items_processed\"] = data.size();\n    state.counters[\"bytes_processed\"] = data.size() * sizeof(Item);\n}\n\nstate.SetItemsProcessed(state.iterations() * data.size());\nstate.SetBytesProcessed(state.iterations() * data.size() * sizeof(Item));",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#performance-analysis-tools",
    "href": "developer/benchmark/benchmarking.html#performance-analysis-tools",
    "title": "Profiling and Performance",
    "section": "Performance Analysis Tools",
    "text": "Performance Analysis Tools\n\n1. Perf (CPU Profiling)\nProfile CPU usage and generate call graphs:\n# Record profile data\nperf record -g ./benchmark_MaskArea --benchmark_filter=Pipeline\n\n# View interactive report\nperf report\n\n# Generate flamegraph\nperf script | stackcollapse-perf.pl | flamegraph.pl &gt; flame.svg\n\n# View specific functions\nperf annotate functionName\n\n# Show hot spots\nperf top\nKey perf options: - -g - Enable call-graph (stack trace) recording - -e cycles - Profile CPU cycles - -e cache-misses - Profile cache misses - --call-graph dwarf - Use DWARF for more accurate call graphs\n\n\n2. Heaptrack (Memory Profiling)\nAnalyze memory allocation patterns:\n# Run with heaptrack\nheaptrack ./benchmark_MaskArea\n\n# View results in GUI\nheaptrack_gui heaptrack.benchmark_MaskArea.*.gz\n\n# View results in terminal\nheaptrack_print heaptrack.benchmark_MaskArea.*.gz\nWhat heaptrack shows: - Total memory allocated - Peak memory usage - Number of allocations - Call stacks for allocations - Memory leaks - Temporary allocations\n\n\n3. Valgrind Cachegrind (Cache Analysis)\nAnalyze cache behavior:\n# Run cachegrind\nvalgrind --tool=cachegrind ./benchmark_MaskArea --benchmark_filter=CacheBehavior\n\n# Visualize results\ncg_annotate cachegrind.out.&lt;pid&gt;\n\n# Interactive visualization\nkcachegrind cachegrind.out.&lt;pid&gt;\nMetrics provided: - L1/L2/L3 cache hits/misses - Instruction cache behavior - Data cache behavior - Branch prediction statistics\n\n\n4. Assembly Inspection\nView generated assembly for optimization:\n# Method 1: objdump\nobjdump -d -C -S ./benchmark_MaskArea | less\n\n# Search for specific function\nobjdump -d -C -S ./benchmark_MaskArea | grep -A 50 \"calculateMaskArea\"\n\n# Method 2: During compilation (if GENERATE_ASM=ON)\n# Assembly files (.s) generated alongside object files\nfind . -name \"*.s\" | xargs less\nWhat to look for: - Loop vectorization (SIMD instructions) - Unnecessary branches - Memory access patterns - Inlining decisions - Register usage\n\n\n5. Time Command (Quick Stats)\nGet quick overview of resource usage:\n/usr/bin/time -v ./benchmark_MaskArea\n\n# Key metrics:\n# - Maximum resident set size (peak memory)\n# - Page faults (major = disk I/O, minor = memory)\n# - Context switches\n# - CPU percentage\n\n\n6. Clang Build Time Analysis\nIf built with -DENABLE_TIME_TRACE=ON:\n# View compilation time breakdown\nClangBuildAnalyzer --all build_trace build_results.bin\nClangBuildAnalyzer --analyze build_results.bin",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#example-workflows",
    "href": "developer/benchmark/benchmarking.html#example-workflows",
    "title": "Profiling and Performance",
    "section": "Example Workflows",
    "text": "Example Workflows\n\nWorkflow 1: Optimize a Hot Function\n# 1. Identify hot spots\nperf record -g ./benchmark_MaskArea\nperf report  # Identify slowest function\n\n# 2. View assembly\nobjdump -d -C -S ./benchmark_MaskArea | grep -A 100 \"mySlowFunction\"\n\n# 3. Check cache behavior\nvalgrind --tool=cachegrind ./benchmark_MaskArea --benchmark_filter=MyFunction\n\n# 4. Make changes and compare\n./benchmark_MaskArea --benchmark_filter=MyFunction &gt; before.txt\n# ... make changes ...\n./benchmark_MaskArea --benchmark_filter=MyFunction &gt; after.txt\ncompare.py compare before.txt after.txt  # From Google Benchmark tools\n\n\nWorkflow 2: Fix Memory Issues\n# 1. Identify allocation hot spots\nheaptrack ./benchmark_MaskArea\n\n# 2. View in GUI\nheaptrack_gui heaptrack.benchmark_MaskArea.*.gz\n# Look for:\n# - Temporary allocations in loops\n# - Large peak allocations\n# - Memory leaks\n\n# 3. Check for leaks with valgrind\nvalgrind --leak-check=full ./benchmark_MaskArea --benchmark_filter=MyTest\n\n# 4. Verify fix\nheaptrack ./benchmark_MaskArea\n# Compare before/after total allocations and peak memory\n\n\nWorkflow 3: Compare Algorithms\n# Benchmark different implementations\n./benchmark_MaskArea --benchmark_filter=\"Algorithm_v1|Algorithm_v2\" \\\n    --benchmark_format=json &gt; comparison.json\n\n# Use benchmark tools to compare\ntools/compare.py benchmarks comparison.json baseline.json\n\n# Or export to CSV for plotting\n./benchmark_MaskArea --benchmark_format=csv &gt; data.csv\n# Import into spreadsheet/plotting tool",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#best-practices",
    "href": "developer/benchmark/benchmarking.html#best-practices",
    "title": "Profiling and Performance",
    "section": "Best Practices",
    "text": "Best Practices\n\nDo’s\n\nUse Fixtures - Set up expensive test data once\nDoNotOptimize - Prevent compiler from optimizing away benchmarks\nTest Multiple Sizes - Use parameter ranges to test scaling\nAdd Counters - Track domain-specific metrics (items/sec, throughput)\nDisable Benchmarks - Turn off unused benchmarks to speed up builds\nRun Multiple Times - Benchmarks automatically run multiple iterations\nProfile Before Optimizing - Use profiling tools to find actual bottlenecks\n\n\n\nDon’ts\n\nDon’t Measure Setup - Put expensive setup in SetUp(), not in loop\nDon’t Ignore Variability - Check standard deviation in results\nDon’t Benchmark Debug Builds - Always use Release or RelWithDebInfo\nDon’t Optimize Without Data - Profile first, optimize second\nDon’t Test Only One Size - Real data varies, test across ranges\n\n\n\nCode Patterns\n// ✅ Good: Setup outside loop\nvoid SetUp(benchmark::State const& state) override {\n    test_data_ = generateExpensiveData();\n}\n\nBENCHMARK_F(MyBench, Test)(benchmark::State& state) {\n    for (auto _ : state) {\n        auto result = process(test_data_);\n        benchmark::DoNotOptimize(result);\n    }\n}\n\n// ❌ Bad: Setup in loop\nBENCHMARK(MyBench)(benchmark::State& state) {\n    for (auto _ : state) {\n        auto data = generateExpensiveData();  // Measured every time!\n        auto result = process(data);\n    }\n}\n\n// ✅ Good: Prevent optimization\nauto result = expensiveComputation();\nbenchmark::DoNotOptimize(result);\n\n// ❌ Bad: Result might be optimized away\nauto result = expensiveComputation();\n// Compiler might remove entire computation if result unused",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#interpreting-results",
    "href": "developer/benchmark/benchmarking.html#interpreting-results",
    "title": "Profiling and Performance",
    "section": "Interpreting Results",
    "text": "Interpreting Results\n\nBenchmark Output\n-------------------------------------------------------------------\nBenchmark                         Time             CPU   Iterations\n-------------------------------------------------------------------\nBM_Pipeline/0                  1.23 ms         1.23 ms          569\nBM_Pipeline/1                 12.45 ms        12.43 ms           56\nBM_Pipeline/2                124.56 ms       124.12 ms            6\n\nTime: Wall-clock time (includes I/O, system time)\nCPU: CPU time (excludes I/O wait)\nIterations: How many times the benchmark ran\n\n\n\nWhat’s Fast Enough?\n\n&lt; 1 microsecond: Excellent for element operations\n&lt; 1 millisecond: Good for frame-level processing\n&lt; 100 milliseconds: Acceptable for batch operations\n&gt; 1 second: Consider optimization or progress reporting\n\n\n\nProfiling Result Interpretation\nPerf Report: - Focus on functions with &gt;5% of total time - Check for unexpected function calls - Look for optimization opportunities in hot paths\nHeaptrack: - Peak memory &lt; 2x working set is good - Allocation count should be O(1) for inner loops - Look for unnecessary temporary allocations\nCachegrind: - L1 miss rate &lt; 5% is excellent - L2 miss rate &lt; 1% is good - Look for cache-unfriendly access patterns",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#continuous-performance-monitoring",
    "href": "developer/benchmark/benchmarking.html#continuous-performance-monitoring",
    "title": "Profiling and Performance",
    "section": "Continuous Performance Monitoring",
    "text": "Continuous Performance Monitoring\n\nSave Baseline Results\n# Save current performance as baseline\n./benchmark_MaskArea --benchmark_out=baseline.json \\\n    --benchmark_out_format=json\n\n# After changes, compare\n./benchmark_MaskArea --benchmark_out=current.json \\\n    --benchmark_out_format=json\n\n# Compare results\ntools/compare.py benchmarks baseline.json current.json",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#troubleshooting",
    "href": "developer/benchmark/benchmarking.html#troubleshooting",
    "title": "Profiling and Performance",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nBenchmarks Too Fast\nIf benchmarks complete in &lt; 1 microsecond:\n// Increase work per iteration\nfor (auto _ : state) {\n    for (int i = 0; i &lt; 100; ++i) {  // Amortize overhead\n        auto result = fastFunction();\n        benchmark::DoNotOptimize(result);\n    }\n}\nstate.SetItemsProcessed(state.iterations() * 100);\n\n\nHigh Variability\nIf standard deviation &gt; 10% of mean:\n\nClose other applications\nDisable CPU frequency scaling: sudo cpupower frequency-set --governor performance\nPin to specific CPU: taskset -c 0 ./benchmark_MaskArea\nIncrease minimum iteration time: --benchmark_min_time=5\n\n\n\nBuild Issues\n# Benchmarks not building\ncmake -DENABLE_BENCHMARK=ON ..\n\n# Specific benchmark disabled\ncmake -DBENCHMARK_MASK_AREA=ON ..\n\n# Google Benchmark not found\n# (Should auto-fetch with FetchContent, but can install manually)\nsudo apt install libbenchmark-dev  # Ubuntu/Debian",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "developer/benchmark/benchmarking.html#additional-resources",
    "href": "developer/benchmark/benchmarking.html#additional-resources",
    "title": "Profiling and Performance",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGoogle Benchmark Documentation\nPerf Tutorial\nHeaptrack Documentation\nValgrind Manual\nOptimization Guide",
    "crumbs": [
      "Profiling and Performance"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "api_reference.html",
    "href": "api_reference.html",
    "title": "API Reference",
    "section": "",
    "text": "This section provides automatically generated API documentation for WhiskerToolbox classes, functions, and data structures. The documentation is generated from the Doxygen comments in the source code.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "api_reference.html#overview",
    "href": "api_reference.html#overview",
    "title": "API Reference",
    "section": "",
    "text": "This section provides automatically generated API documentation for WhiskerToolbox classes, functions, and data structures. The documentation is generated from the Doxygen comments in the source code.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "api_reference.html#browse-the-api-documentation",
    "href": "api_reference.html#browse-the-api-documentation",
    "title": "API Reference",
    "section": "Browse the API Documentation",
    "text": "Browse the API Documentation\nThe complete API reference is available through the Doxygen-generated documentation:\n\nOpen API Documentation",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "api_reference.html#key-sections",
    "href": "api_reference.html#key-sections",
    "title": "API Reference",
    "section": "Key Sections",
    "text": "Key Sections\nThe API documentation includes:\n\nClasses: Documentation for all C++ classes including DataManager, various widget classes, and data processing classes\nNamespaces: Organized documentation by namespace\nFiles: Documentation organized by source file location\nFunctions: All public functions and methods with parameter descriptions",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "api_reference.html#building-the-api-documentation",
    "href": "api_reference.html#building-the-api-documentation",
    "title": "API Reference",
    "section": "Building the API Documentation",
    "text": "Building the API Documentation\nTo regenerate the API documentation locally:\ncd /path/to/WhiskerToolbox\ndoxygen Doxyfile\nThis will regenerate the documentation in the docs/api/html/ directory.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "developer/analysis_dashboard.html",
    "href": "developer/analysis_dashboard.html",
    "title": "Analysis Dashboard: Embedding OpenGL Widgets in QGraphicsScene",
    "section": "",
    "text": "Overview\nThis note documents the pattern for embedding a QOpenGLWidget inside a QGraphicsScene via QGraphicsProxyWidget, as used by the Analysis Dashboard plots. The goal is to achieve smooth, interactive pan/zoom and reliable repainting while the plot lives as a QGraphicsItem within a scene.\nThe core pieces:\n\nA plot QGraphicsWidget subclass that owns a QGraphicsProxyWidget hosting the OpenGL widget\nAn OpenGL widget configured for proxy embedding and interactive input\nIntentional event routing so dragging and wheel actions reach the OpenGL widget (not the parent graphics item)\n\n\n\nArchitecture\n\nPlot item: QGraphicsWidget (e.g., SpatialOverlayPlotWidget, ScatterPlotWidget)\n\nDraws the plot frame and title in its paint()\nEmbeds an OpenGL child via QGraphicsProxyWidget\nRoutes mouse events: title area → item movement; content area → OpenGL interactivity\n\nOpenGL child: QOpenGLWidget (e.g., SpatialOverlayOpenGLWidget, ScatterPlotOpenGLWidget)\n\nHolds visualization state and OpenGL resources\nImplements interactive behavior: mouse press/move/release, wheel zoom, tooltips\nEmits signals to trigger lightweight parent/proxy repaints\n\n\n\n\nEmbedding pattern (plot item)\nConfigure the OpenGL widget and proxy to avoid stale frames and input conflicts when embedded in a QGraphicsScene:\n// In PlotWidget::setupOpenGLWidget()\n_opengl_widget = new ScatterPlotOpenGLWidget();\n\n_proxy_widget = new QGraphicsProxyWidget(this);\n_proxy_widget-&gt;setWidget(_opengl_widget);\n\n// OpenGL widget attributes for proxy embedding\n_opengl_widget-&gt;setAttribute(Qt::WA_AlwaysStackOnTop, false);\n_opengl_widget-&gt;setAttribute(Qt::WA_OpaquePaintEvent, true);\n_opengl_widget-&gt;setAttribute(Qt::WA_NoSystemBackground, true);\n_opengl_widget-&gt;setUpdateBehavior(QOpenGLWidget::NoPartialUpdate);\n\n// Prevent the proxy from intercepting movement/selection\n_proxy_widget-&gt;setFlag(QGraphicsItem::ItemIsMovable, false);\n_proxy_widget-&gt;setFlag(QGraphicsItem::ItemIsSelectable, false);\n\n// Make sure we repaint from the child every update\n_proxy_widget-&gt;setCacheMode(QGraphicsItem::NoCache);\n\n// Lay out inside the plot frame (leave room for title/border)\nQRectF rect = boundingRect();\nQRectF content_rect = rect.adjusted(8, 30, -8, -8);\n_proxy_widget-&gt;setGeometry(content_rect);\n_proxy_widget-&gt;widget()-&gt;resize(content_rect.size().toSize());\nEvent routing in the plot item ensures that content-area interaction reaches the OpenGL widget:\n// In PlotWidget::mousePressEvent(QGraphicsSceneMouseEvent* event)\nQRectF title_area = boundingRect().adjusted(0, 0, 0, -boundingRect().height() + 25);\n\nif (title_area.contains(event-&gt;pos())) {\n    // Title: allow moving/selecting the plot item\n    emit plotSelected(getPlotId());\n    setFlag(QGraphicsItem::ItemIsMovable, true);\n    AbstractPlotWidget::mousePressEvent(event);\n} else {\n    // Content: disable item movement so drag goes to the OpenGL child\n    emit plotSelected(getPlotId());\n    setFlag(QGraphicsItem::ItemIsMovable, false);\n    event-&gt;accept();\n}\n\n\nOpenGL widget configuration\nConfigure the QOpenGLWidget so it reliably receives hover/wheel events and repaints well under a proxy:\n// In OpenGLWidget constructor\nsetMouseTracking(true);\nsetFocusPolicy(Qt::StrongFocus);\n\n// Prefer a core profile and multisampling\nQSurfaceFormat fmt;\nfmt.setVersion(4, 1);\nfmt.setProfile(QSurfaceFormat::CoreProfile);\nfmt.setSamples(4);\nsetFormat(fmt);\n\n// Attributes for embedding in a QGraphicsProxyWidget\nsetAttribute(Qt::WA_AlwaysStackOnTop, false);\nsetAttribute(Qt::WA_OpaquePaintEvent, true);\nsetAttribute(Qt::WA_NoSystemBackground, true);\nsetUpdateBehavior(QOpenGLWidget::NoPartialUpdate);\nTypical interactive handlers (pan/zoom):\nvoid OpenGLWidget::mousePressEvent(QMouseEvent* e) {\n    if (e-&gt;button() == Qt::LeftButton) {\n        _is_panning = true;\n        _last_mouse_pos = e-&gt;pos();\n        e-&gt;accept();\n    } else {\n        e-&gt;ignore();\n    }\n}\n\nvoid OpenGLWidget::mouseMoveEvent(QMouseEvent* e) {\n    if (_is_panning && (e-&gt;buttons() & Qt::LeftButton)) {\n        QPoint delta = e-&gt;pos() - _last_mouse_pos;\n        float world_scale = 2.0f / (_zoom_level * std::min(width(), height()));\n        setPanOffset(_pan_offset_x + delta.x() * world_scale,\n                     _pan_offset_y - delta.y() * world_scale);\n        _last_mouse_pos = e-&gt;pos();\n        e-&gt;accept();\n    } else {\n        e-&gt;accept();\n    }\n}\n\nvoid OpenGLWidget::wheelEvent(QWheelEvent* e) {\n    float zoom_factor = 1.0f + (e-&gt;angleDelta().y() / 1200.0f);\n    setZoomLevel(_zoom_level * zoom_factor);\n    e-&gt;accept();\n}\n\n\nRepaint strategy\n\nThe OpenGL widget calls update() (optionally throttled) on interaction; the proxy and parent item listen to signals like zoomLevelChanged / panOffsetChanged to call update() on themselves too.\nDisable caching (QGraphicsItem::NoCache) on the proxy so frames are not reused while the GL child is animating.\n\n\n\nTroubleshooting\n\nSymptom: plot only updates after resize or clicking away\n\nEnsure the GL child has Qt::WA_OpaquePaintEvent, Qt::WA_NoSystemBackground, and NoPartialUpdate\nEnsure the proxy uses NoCache\nVerify event routing: in content area, disable ItemIsMovable and accept the event so drag/wheel reach the GL widget\nEnable setMouseTracking(true) and setFocusPolicy(Qt::StrongFocus) on the GL widget\n\n\n\n\nReferences\n\nPlot items: src/WhiskerToolbox/Analysis_Dashboard/Widgets/SpatialOverlayPlotWidget/SpatialOverlayPlotWidget.cpp, src/WhiskerToolbox/Analysis_Dashboard/Widgets/ScatterPlotWidget/ScatterPlotWidget.cpp\nOpenGL widgets: src/WhiskerToolbox/Analysis_Dashboard/Widgets/SpatialOverlayPlotWidget/SpatialOverlayOpenGLWidget.cpp, src/WhiskerToolbox/Analysis_Dashboard/Widgets/ScatterPlotWidget/ScatterPlotOpenGLWidget.cpp"
  },
  {
    "objectID": "developer/building.html",
    "href": "developer/building.html",
    "title": "Building",
    "section": "",
    "text": "For building any large c++ project, we are going to need to have 1) a compiler; and 2) a build system. Multiple compilers are supported by this project (GCC, Clang and MSVC), and the one you select varies by platform. The build system is CMake. You are also going to need to use the dependency management system for c++, vcpkg. The first time you install all of the dependencies, it is going to take a while\n\nIDE Specific Instructions:\n\nCLion\nAt the top to configure build properties, select More Action -&gt; Configuration -&gt; Edit…\nUnder Environment variables, adding this on windows works for me:\nPATH=bin\\;C:\\Qt\\6.7.2\\msvc2019_64\\bin\\;_deps\\torch-src\\lib\\;_deps\\iir-build\\;$PATH$",
    "crumbs": [
      "Building"
    ]
  },
  {
    "objectID": "developer/compute_outline.html",
    "href": "developer/compute_outline.html",
    "title": "compute_outline",
    "section": "",
    "text": "Data Types\nAnalogTimeSeries -&gt; float for each time\nDigitalEventSeries\nDigitalIntervalSeries\nLineData\nMaskData\nPointData\n\n\nData Transforms\n\n\n\n\n\n\n\n\n\nName\nInput\nOutput\nParams\n\n\n\n\nAnalog Event Threshold\nAnalogTimeSeries\nDigitalEventSeries\n\n\n\nAnalog Interval Peak\nAnalogTimeSeries\nDigitalIntervalSeries\nDigitalEventSeries\n\n\n\nAnalog Scaling\nAnalogTimeSeries\nAnalogTimeSeries\n\n\n\nAnalog Filter\nAnalogTimeSeries\nAnalogTimeSeries\n\n\n\nAnalogHilbertPhase\nAnalogTimeSeries\nAnalogTimeSeries\n\n\n\nBoolean\nDigitalIntervalSeries\nDigitalIntervalSeries\nDigitalIntervalSeries\n\n\n\nDigital interval Group\nDigitalIntervalSeries\nDigitalIntervalSeries\n\n\n\nLineAlignment\nLineData (Single)\nMediaData\nLineData (Ragged)\n\n\n\nLine Angle\nLineData (T)\nAnalogTimeSeries (T)\n\n\n\nLine Base Flip\nLineData (T)\nPointData (just single point)\nLineData (T)\n\n\n\nLine Clip\nLineData (T)\nLineData (S)\nLineData (T)\n\n\n\nLine Curvature\nLineData (T)\nAnalogTimeSeries (T)\n\n\n\nLine Group to Intervals\nLineData\nDigitalIntervalSeries\nEntityGroupManager\n\n\nLine Index Grouping\nLineData\nNothing\nEntityGroupManager\n\n\nLine Kalman Grouping\nLineData\nNothing\nEntityGroupManager\n\n\nLine Min Point Dist\nLineData\nPointData\nAnalogTimeSeries\n\n\n\nLine Outlier Detection\nLineData\nNothing\nEntityGroupManager\n\n\nLine Point Extraction\nLineData\nPointData\n\n\n\nLine Proximity Grouping\nLineData\nNothing\nEntityGroupManager\n\n\nLine Resampling\nLineData\nLineData\n\n\n\nLine Subsegment\nLineData\nLineData\n\n\n\nMask Area\nMaskData\nAnalogTimeSeries\n\n\n\nMask Centroid\nMaskData\nPointData\n\n\n\nMask Connected Component\nMaskData\nMaskData\n\n\n\nMask Hole Filling\nMaskData\nMaskData\n\n\n\nMedian Filter\nMaskData\nMaskData\n\n\n\nMask Principal Axis\nMaskData\nLineData\n\n\n\nMask Skeletonize\nMaskData\nMaskData\n\n\n\nMask to Line\nMaskData\nLineData\n\n\n\nWhisker Tracing\nMediaData\nMaskData (optional)\nLineData\n\n\n\nPoint Particle Filter\nPointData\nMaskData\nPointData\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTableview Computers\n\n\n\n\n\n\n\n\n\n\nName\nInput\nTime\nOutput\nParams\n\n\n\n\nAnalog Slice Gatherer\nAnalogTimeSeries\nDigitalIntervalSeries\nTimeFrameIndex\nstd::vector&lt;std::vector&lt;float&gt;&gt;\nstd::vector&lt;&gt; within digital interval series\n\n\n\nAnalogTimestampOffsetMultiComputer\nAnalogTimeSeries\nTimeFrameIndex\nstd::vector&lt;FixedSizeArray&gt;\n\n\n\nEventInIntervalComputer\nDigitalEventSeries\nDigitalIntervalSeries\nPresence - bool\nCount - int\nGather - std::vector&lt;float&gt;\nGatherCenter - std::vector&lt;float&gt;\nall of these also provide std::Vector&lt;EntityIDs&gt;\n\n\n\nIntervalOverlapComputer\nDigitalIntervalSeries\nDigitalIntervalSeries\nAssignID - int\nCounterOverlaps - int\nAssignID_Start - int\nAssignID_End - int\nall of these also provide std::Vector&lt;EntityIDs&gt;\n\n\n\nIntervalPropertyComputer\nDigitalIntervalSeries\nDigitalIntervalSeries\n\n\n\n\nIntervalReductionComputer\nAnalogTimeSeries\nDigitalIntervalSeries\n\n\n\n\nLineLengthComputer\nLineData\nTimeFrameIndex\nAnalogTimeSeries (Ragged)\n\n\n\nLIneSamplingMultiComputer\nLineData\nTimeFrameIndex\nPointData?\nAnalogTimeSeries(FixedSize)\n\n\n\nLineTimestampComputer\n\n\n\n\n\n\nTimestampInIntervalComputer\n\n\n\n\n\n\nTimestampValueComputer"
  },
  {
    "objectID": "developer/copilot.html",
    "href": "developer/copilot.html",
    "title": "C++ Development Tools Reference",
    "section": "",
    "text": "This document serves as a reference for the C++ development tools available in the WhiskerToolbox development environment. These tools help with code quality, performance analysis, and debugging.",
    "crumbs": [
      "C++ Development Tools Reference"
    ]
  },
  {
    "objectID": "developer/copilot.html#iwyu-include-what-you-use",
    "href": "developer/copilot.html#iwyu-include-what-you-use",
    "title": "C++ Development Tools Reference",
    "section": "IWYU (Include What You Use)",
    "text": "IWYU (Include What You Use)\nVersion: 0.21 (based on Ubuntu clang version 17.0.6)\nPurpose: IWYU analyzes C++ source files to ensure that they directly include all headers they use and do not include unnecessary headers. This helps reduce compilation times and maintain clean dependencies.\nLocation: /usr/bin/include-what-you-use\nCommand Line Usage:\nBasic usage on a single file (requires compiler flags):\ninclude-what-you-use -I/path/to/includes src/file.cpp\nUsing with compile_commands.json (recommended):\n# Extract compilation command for a specific file from compile_commands.json\n# and pass those flags to IWYU\ninclude-what-you-use -Xiwyu --verbose=3 $(jq -r '.[] | select(.file | contains(\"main.cpp\")) | .arguments[]' out/build/Clang/Release/compile_commands.json | grep -v \"^clang\" | tr '\\n' ' ')\nCommon IWYU options: - -Xiwyu --verbose=3: Increase verbosity - -Xiwyu --check_also=&lt;glob&gt;: Check additional files matching pattern - -Xiwyu --mapping_file=&lt;file&gt;: Use custom mapping file - -Xiwyu --no_default_mappings: Disable default mappings\nValidation:\n# Verified IWYU is installed and accessible\ninclude-what-you-use --version\n# Output: include-what-you-use 0.21 based on Ubuntu clang version 17.0.6\nUse Cases: - Reduce compilation times by removing unnecessary includes - Identify missing direct includes - Maintain clean header dependencies - Prevent header inclusion bloat",
    "crumbs": [
      "C++ Development Tools Reference"
    ]
  },
  {
    "objectID": "developer/copilot.html#cppcheck",
    "href": "developer/copilot.html#cppcheck",
    "title": "C++ Development Tools Reference",
    "section": "cppcheck",
    "text": "cppcheck\nVersion: 2.13.0\nPurpose: Static analysis tool for C/C++ code that detects bugs, undefined behavior, dangerous coding constructs, and stylistic issues without executing the program.\nLocation: /usr/bin/cppcheck\nCommand Line Usage:\nAnalyze a single file:\ncppcheck --enable=warning,performance --suppress=missingIncludeSystem src/file.cpp\nAnalyze entire source directory:\ncppcheck --enable=all --suppress=missingIncludeSystem --project=compile_commands.json\nAnalyze with specific checks:\n# Enable specific check categories\ncppcheck --enable=warning,performance,portability,information src/\n\n# Full analysis (slower but more thorough)\ncppcheck --enable=all --inconclusive src/\nCommon options: - --enable=&lt;checks&gt;: Enable specific checks (all, warning, style, performance, portability, information) - --suppress=&lt;id&gt;: Suppress specific warnings - --project=&lt;file&gt;: Use compilation database - --inconclusive: Show inconclusive results - --quiet: Only show errors - --verbose: Show verbose output\nValidation:\n# Verified cppcheck is installed\ncppcheck --version\n# Output: Cppcheck 2.13.0\n\n# Test on project file\ncppcheck --enable=warning,performance --suppress=missingIncludeSystem src/WhiskerToolbox/main.cpp\n# Successfully executed and reported issues\nUse Cases: - Static bug detection (null pointer dereferences, memory leaks, etc.) - Code quality checks - Performance issue detection - Portability checks - Pre-commit hook integration",
    "crumbs": [
      "C++ Development Tools Reference"
    ]
  },
  {
    "objectID": "developer/copilot.html#infer",
    "href": "developer/copilot.html#infer",
    "title": "C++ Development Tools Reference",
    "section": "Infer",
    "text": "Infer\nVersion: 1.2.0\nPurpose: Static analyzer developed by Facebook/Meta for detecting bugs in C, C++, Java, and Objective-C code. Focuses on serious issues like null pointer dereferences, memory leaks, and concurrency bugs.\nLocation: /usr/local/bin/infer\nCommand Line Usage:\nAnalyze using compilation database:\ninfer run --compilation-database out/build/Clang/Release/compile_commands.json\nAnalyze with specific build command:\ninfer run -- cmake --build out/build/Clang/Release\nView results:\n# View analysis results\ninfer explore\n\n# Generate report\ninfer report --issues-txt report.txt\nCommon options: - --compilation-database &lt;file&gt;: Use compile_commands.json - --report-console-limit &lt;n&gt;: Limit console output - --project-root &lt;dir&gt;: Set project root directory - --keep-going: Continue analysis despite errors - --results-dir &lt;dir&gt;: Specify output directory (default: infer-out)\nValidation:\n# Verified infer is installed\ninfer --version\n# Output: Infer version v1.2.0\n\n# Test analysis (note: full analysis takes time)\ninfer run --compilation-database out/build/Clang/Release/compile_commands.json --report-console-limit 5\n# Successfully started analysis of 671 translation units\nUse Cases: - Find null pointer dereferences - Detect memory leaks and resource leaks - Identify race conditions - Find logic errors - CI/CD integration for automated bug detection",
    "crumbs": [
      "C++ Development Tools Reference"
    ]
  },
  {
    "objectID": "developer/copilot.html#hotspot",
    "href": "developer/copilot.html#hotspot",
    "title": "C++ Development Tools Reference",
    "section": "Hotspot",
    "text": "Hotspot\nVersion: 1.3.0\nPurpose: GUI for Linux perf profiler. Visualizes performance data collected by perf, making it easy to identify performance bottlenecks, hot functions, and call graphs.\nLocation: /usr/bin/hotspot\nCommand Line Usage:\nLaunch GUI with existing perf data:\nhotspot perf.data\nRecord and analyze:\n# First, record performance data with perf\nperf record -g ./out/build/Clang/Release/WhiskerToolbox\n\n# Then analyze with hotspot\nhotspot perf.data\nWith custom sysroot and debug paths:\nhotspot --sysroot /path/to/sysroot --debugPaths /usr/lib/debug perf.data\nCommon options: - --sysroot &lt;path&gt;: Path to sysroot for finding libraries - --kallsyms &lt;path&gt;: Path to kallsyms file for kernel symbols - --debugPaths &lt;paths&gt;: Colon-separated debug info paths - --extraLibPaths &lt;paths&gt;: Additional library paths - --appPath &lt;path&gt;: Application executable path\nValidation:\n# Verified hotspot is installed\nhotspot --version\n# Output: hotspot 1.3.0\nUse Cases: - Visualize CPU profiling data - Identify performance hotspots - Analyze call graphs - Compare performance across runs - Flame graph visualization - Top-down and bottom-up analysis\nNote: Hotspot is a GUI application and requires an X server or display to run.",
    "crumbs": [
      "C++ Development Tools Reference"
    ]
  },
  {
    "objectID": "developer/copilot.html#heaptrack",
    "href": "developer/copilot.html#heaptrack",
    "title": "C++ Development Tools Reference",
    "section": "Heaptrack",
    "text": "Heaptrack\nVersion: 1.6.80\nPurpose: Heap memory profiler for Linux that tracks all memory allocations in an application. Helps identify memory leaks, excessive allocations, and memory usage patterns.\nLocation: /home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/heaptrack/build/bin/heaptrack\nCommand Line Usage:\nProfile an application:\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/heaptrack/build/bin/heaptrack ./out/build/Clang/Release/WhiskerToolbox [args...]\nAttach to running process:\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/heaptrack/build/bin/heaptrack -p &lt;PID&gt;\nAnalyze recorded data:\n# Print text report\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/heaptrack/build/bin/heaptrack_print heaptrack.APP.PID.gz\n\n# Launch GUI (if available)\nheaptrack_gui heaptrack.APP.PID.gz\nCommon options: - -p, --pid &lt;PID&gt;: Attach to running process - -r, --raw: Record raw data only (faster) - -d, --debug: Run with GDB debugger - --use-inject: Use symbol interception mechanism - --asan: Enable for ASAN-built binaries - --record-only: Record and interpret, don’t analyze - -q, --quiet: Suppress output except errors\nValidation:\n# Verified heaptrack is installed\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/heaptrack/build/bin/heaptrack --version\n# Output: heaptrack 1.6.80\nUse Cases: - Find memory leaks - Identify excessive allocations - Track peak memory usage - Analyze allocation patterns - Optimize memory consumption - Debug memory issues in production\nNote: Heaptrack uses LD_PRELOAD to intercept allocation functions. Results are saved in .gz files.",
    "crumbs": [
      "C++ Development Tools Reference"
    ]
  },
  {
    "objectID": "developer/copilot.html#clangbuildanalyzer",
    "href": "developer/copilot.html#clangbuildanalyzer",
    "title": "C++ Development Tools Reference",
    "section": "ClangBuildAnalyzer",
    "text": "ClangBuildAnalyzer\nVersion: 1.6.0\nPurpose: Tool to analyze and visualize Clang build times. Helps identify slow compilation units, expensive headers, and optimize build performance.\nLocation: /home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/ClangBuildAnalyzer/build/ClangBuildAnalyzer\nCommand Line Usage:\nThe workflow involves three steps:\n\nStart tracing before build:\n\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/ClangBuildAnalyzer/build/ClangBuildAnalyzer --start out/build/Clang/Release\n\nBuild with time-trace enabled:\n\ncmake --build out/build/Clang/Release -- -ftime-trace\n\nAnalyze the results:\n\n# Stop tracing and capture data\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/ClangBuildAnalyzer/build/ClangBuildAnalyzer --stop out/build/Clang/Release build_trace.bin\n\n# Generate analysis report\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/ClangBuildAnalyzer/build/ClangBuildAnalyzer --analyze build_trace.bin\nAlternative all-in-one command:\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/ClangBuildAnalyzer/build/ClangBuildAnalyzer --all out/build/Clang/Release build_trace.bin\nValidation:\n# Verified ClangBuildAnalyzer is built and accessible\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/ClangBuildAnalyzer/build/ClangBuildAnalyzer\n# Output: ClangBuildAnalyzer 1.6.0, USAGE: one of...\nUse Cases: - Identify slowest compilation units - Find expensive headers to optimize or forward-declare - Analyze template instantiation costs - Optimize build times - Track build performance over time - Understand what’s slowing down incremental builds\nRequirements: - Clang compiler with -ftime-trace support - Must run –start before building - Build must include -ftime-trace flag",
    "crumbs": [
      "C++ Development Tools Reference"
    ]
  },
  {
    "objectID": "developer/copilot.html#summary",
    "href": "developer/copilot.html#summary",
    "title": "C++ Development Tools Reference",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\n\nTool\nPurpose\nType\nOutput\n\n\n\n\nIWYU\nInclude dependency analysis\nStatic analysis\nTerminal\n\n\ncppcheck\nBug & quality static analysis\nStatic analysis\nTerminal\n\n\nInfer\nAdvanced bug detection\nStatic analysis\nTerminal + HTML\n\n\nHotspot\nCPU profiling visualization\nRuntime profiling\nGUI\n\n\nHeaptrack\nMemory profiling\nRuntime profiling\nTerminal + GUI\n\n\nClangBuildAnalyzer\nBuild time analysis\nBuild tool\nTerminal",
    "crumbs": [
      "C++ Development Tools Reference"
    ]
  },
  {
    "objectID": "developer/copilot.html#quick-reference",
    "href": "developer/copilot.html#quick-reference",
    "title": "C++ Development Tools Reference",
    "section": "Quick Reference",
    "text": "Quick Reference\nFor code quality before commit:\ncppcheck --enable=warning,performance --suppress=missingIncludeSystem src/\nFor deep static analysis:\ninfer run --compilation-database out/build/Clang/Release/compile_commands.json\nFor include optimization:\ninclude-what-you-use src/file.cpp [compiler flags]\nFor CPU performance profiling:\nperf record -g ./out/build/Clang/Release/WhiskerToolbox\nhotspot perf.data\nFor memory profiling:\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/heaptrack/build/bin/heaptrack ./out/build/Clang/Release/WhiskerToolbox\nFor build time optimization:\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/ClangBuildAnalyzer/build/ClangBuildAnalyzer --start out/build/Clang/Release\ncmake --build out/build/Clang/Release -- -ftime-trace\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/ClangBuildAnalyzer/build/ClangBuildAnalyzer --all out/build/Clang/Release trace.bin\n/home/runner/work/WhiskerToolbox/WhiskerToolbox/tools/ClangBuildAnalyzer/build/ClangBuildAnalyzer --analyze trace.bin",
    "crumbs": [
      "C++ Development Tools Reference"
    ]
  },
  {
    "objectID": "developer/copilot.html#build-location",
    "href": "developer/copilot.html#build-location",
    "title": "C++ Development Tools Reference",
    "section": "Build Location",
    "text": "Build Location\nThe project uses the linux-clang-release CMake preset with build artifacts located at:\nout/build/Clang/Release\nThis directory contains: - compile_commands.json - Compilation database for analysis tools - WhiskerToolbox - Main executable - Various shared libraries (.so files) - Build artifacts and intermediate files",
    "crumbs": [
      "C++ Development Tools Reference"
    ]
  },
  {
    "objectID": "developer/data_manager_widget.html",
    "href": "developer/data_manager_widget.html",
    "title": "Data Manager Widget",
    "section": "",
    "text": "Data Manager Widget Overview\nThe data manager widget serves as the main user interface for interacting with all data currently loaded into the program. At its top, the widget displays all keys currently in the data manager along with their corresponding data types. It enables the user to specify the folder for saving data. Additionally, users can create new, initially blank data sets, which can subsequently be populated through further manipulations and other widgets.\nWhen a user selects an entry in the top table, a data type-specific soft widget is populated. This widget presents the user with various features of the selected data. For example, when dealing with point data, it shows a table listing every available point within that data set and its associated frame. The user has the ability to scroll through this table; clicking an entry causes the time displayed by the rest of the program to jump to that point’s frame. Users can perform several data manipulations via these specific widgets. For instance, using the point widget, a user can delete any specific point. They can also opt to move a specific point to another point data set available in the manager.\n\n\nData Saving Functionality\nThe data manager widget also functions as the primary interface for saving data to disk. It features a sub-widget interface where the user can select the desired file output type, and specific options for that output type are then displayed. For instance, if a user is viewing point data, they can choose to output CSV files and will be presented with various CSV saving options, such as selecting the delimiter and deciding whether a header should be included, etc.",
    "crumbs": [
      "Data Manager Widget"
    ]
  },
  {
    "objectID": "developer/data_transform_widget.html",
    "href": "developer/data_transform_widget.html",
    "title": "Data Transform Widget",
    "section": "",
    "text": "The data transform interface is used for processing different data types. The core idea is to define various data processing operations that can be dynamically discovered and applied to different types of data, with parameters configurable through the UI. The system employs several design patterns, most notably the Strategy pattern for individual transformations and a Registry pattern for managing them, along with Factory Method for creating UI components.",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_transform_widget.html#core-components-and-workflow",
    "href": "developer/data_transform_widget.html#core-components-and-workflow",
    "title": "Data Transform Widget",
    "section": "Core Components and Workflow",
    "text": "Core Components and Workflow\nThe system revolves around a few key components:\n\nTransformOperation (Strategy Pattern): This is an abstract base class that defines the interface for all data transformation operations. Each concrete operation (e.g., EventThresholdOperation, MaskAreaOperation) inherits from TransformOperation and implements methods like getName(), getTargetInputTypeIndex(), canApply(), and execute(). This allows different algorithms (strategies) for data transformation to be used interchangeably.\nTransformParametersBase and derived structs (e.g., ThresholdParams): These structures hold the parameters for specific transformations. TransformParametersBase is a base class, and each operation can define its own derived struct (like ThresholdParams for thresholding operations) to store specific settings.\nTransformRegistry (Registry Pattern): This class acts as a central repository for all available TransformOperation instances. On initialization, it registers various concrete operation objects (e.g., MaskAreaOperation, EventThresholdOperation). It provides methods to find operations by name and to get a list of applicable operations for a given data type.\nTransformParameter_Widget (UI Abstraction): This is an abstract base class for UI widgets that allow users to set parameters for a TransformOperation. Concrete classes like AnalogEventThreshold_Widget inherit from it and provide the specific UI controls (e.g., spinboxes, comboboxes) for an operation.\nDataTransform_Widget (Main UI Controller): This Qt widget orchestrates the user interaction for data transformations.\n\nIt uses a Feature_Table_Widget to display available data items (features) from a DataManager.\nWhen a feature is selected, it queries the TransformRegistry to find applicable operations for that feature’s data type.\nIt populates a QComboBox with the names of these operations.\nWhen an operation is selected, it uses a map of factory functions (_parameterWidgetFactories) to create and display the appropriate TransformParameter_Widget (e.g., AnalogEventThreshold_Widget) for that operation. This is an example of the Factory Method pattern.\nIt has a “Do Transform” button that, when clicked, retrieves the parameters from the current TransformParameter_Widget, gets the selected TransformOperation from the TransformRegistry, and executes the operation on the selected data.",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_transform_widget.html#features",
    "href": "developer/data_transform_widget.html#features",
    "title": "Data Transform Widget",
    "section": "Features",
    "text": "Features\n\nThe ProgressCallback mechanism allows the TransformOperation to notify the DataTransform_Widget about its progress, which then updates the UI. This is a simple form of the Observer pattern.",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/data_transform_widget.html#design-example",
    "href": "developer/data_transform_widget.html#design-example",
    "title": "Data Transform Widget",
    "section": "Design Example",
    "text": "Design Example\nFirst, the user will create the transformation translation unit. These are located in the DataManager/transforms directory. The transformations are organized according to their input type. The designer will first need to specify the transformation operation itself, which should take a pointer to the input and return a std::shared_ptr to the output type. The user can also overload this function to take a ProgressCallback for longer operations. For example:\nstruct MaskConnectedComponentParameters : public TransformParametersBase {\n    /**\n     * @brief Minimum size (in pixels) for a connected component to be preserved\n     * \n     * Connected components smaller than this threshold will be removed from the mask.\n     * Must be greater than 0.\n     */\n    int threshold = 10;\n};\n\n///////////////////////////////////////////////////////////////////////////////\n\n/**\n * @brief Remove small connected components from mask data\n * \n * This function applies connected component analysis to remove small isolated\n * regions from masks. Uses 8-connectivity (considers diagonal neighbors as connected).\n * \n * @param mask_data The MaskData to process\n * @param params The connected component parameters\n * @return A new MaskData with small connected components removed\n */\nstd::shared_ptr&lt;MaskData&gt; remove_small_connected_components(\n        MaskData const * mask_data,\n        MaskConnectedComponentParameters const * params = nullptr);\n\n/**\n * @brief Remove small connected components from mask data with progress reporting\n * \n * @param mask_data The MaskData to process\n * @param params The connected component parameters\n * @param progressCallback Progress reporting callback\n * @return A new MaskData with small connected components removed\n */\nstd::shared_ptr&lt;MaskData&gt; remove_small_connected_components(\n        MaskData const * mask_data,\n        MaskConnectedComponentParameters const * params,\n        ProgressCallback progressCallback);\nThe user will also need to define a TransformationOperation interface class that uses this function. This is defined in DataManager/transforms/data_transforms.hpp and this is an example for the MaskConnectedComponentOperation:\n\nclass MaskConnectedComponentOperation final : public TransformOperation {\npublic:\n    [[nodiscard]] std::string getName() const override;\n    [[nodiscard]] std::type_index getTargetInputTypeIndex() const override;\n    [[nodiscard]] bool canApply(DataTypeVariant const & dataVariant) const override;\n    [[nodiscard]] std::unique_ptr&lt;TransformParametersBase&gt; getDefaultParameters() const override;\n    \n    DataTypeVariant execute(DataTypeVariant const & dataVariant,\n                           TransformParametersBase const * transformParameters) override;\n                           \n    DataTypeVariant execute(DataTypeVariant const & dataVariant,\n                           TransformParametersBase const * transformParameters,\n                           ProgressCallback progressCallback) override;\n};\nThe body of these functions is mostly boilerplate that will be verbatim between operations.\n\n\n\n\n\n\nImportant\n\n\n\nThe value return by the getName function will need to be used later in the User Interface exactly. If you do not use the name here to identify your transformation, it may not appear in the UI.\n\n\nAfter you have designed your tranformation, add the files to the CMakeLists.txt for DataManager listed in DataManager/CMakeLists.txt. Then include your header in DataManager/transforms/TransformRegistry.cpp and add your type with the _registerOperation function.\nNow you can create the user interface for your transformation operation. The user interfaces are kept in DataTransform_Widget under folders for the specific input type (same as the transformation). Create a hpp/cpp/ui triplet for your transformation. The purpose of this UI should be to populate parameters structure you created with the tranformation. If you have no options structure, this widget can simply be a label the describes the transformation. See MaskArea_Widget for an example fo a blank UI and LineResample_Widget for a more complex example. Your widget will need to inherit from TransformParameter_Widget as a base class.\nOnce you have completed your widget triplet, add these files to the main CMakeLists.txt for WhiskerToolbox. Then you will modify DataTransform_Widget.cpp to include the header to your UI, and populate _parameterWidgetFactories with the name of your transformation. For example:\n\n_parameterWidgetFactories[\"Remove Small Connected Components\"] = [](QWidget * parent) -&gt; TransformParameter_Widget * {\n        return new MaskConnectedComponent_Widget(parent);\n    };\nNote that the name in this map (e.g. “Remove Small Connected Components”) must match the name that is returned by your transformation operation!\nAfter this, compile and your transformation should appear in the data transformation widget!",
    "crumbs": [
      "Data Transform Widget"
    ]
  },
  {
    "objectID": "developer/display_options.html",
    "href": "developer/display_options.html",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options.\n\n\n\n\n\n\nRange: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives\n\n\n\n\n\n\n\n\nRange: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nPosition Marker: Display a marker at a specified percentage distance along the line (off by default)\n\nshow_position_marker (bool): Enable/disable position marker display\nposition_percentage (int, 0-100%): Position along line where marker appears\n\nLine Segment: Display only a portion of the line between two percentage points (off by default)\n\nshow_segment (bool): Enable/disable segment-only display mode\nsegment_start_percentage (int, 0-100%): Start percentage for line segment\nsegment_end_percentage (int, 0-100%): End percentage for line segment\n\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nRange: 0-100% along the cumulative length of the line\nUI Controls:\n\nCheckbox to enable/disable the feature (off by default)\nHorizontal slider for quick adjustments\nSpin box for precise numeric input with % suffix\n\nDefault Value: 20% along the line\nVisual Appearance: Distinctive filled circle with white border, same color as the line\nCalculation: Based on cumulative distance along line segments, not point indices\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nPosition marker calculated using cumulative distance along line segments\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots\n\n\n\nThe segment feature allows displaying only a portion of the line between two percentage points along its cumulative distance:\n\nget_segment_between_percentages(): Utility function in lines.hpp that extracts a continuous segment\n\nCalculates cumulative distances along the original line\nPerforms linear interpolation for precise start/end points\nReturns a new Line2D containing only the specified segment\nHandles edge cases (empty lines, invalid percentages, zero-length segments)\n\nUI Validation: Start percentage cannot exceed end percentage (enforced in UI)\nRendering Logic: When enabled, replaces the full line with the extracted segment in all rendering operations\nPosition Marker Compatibility: Position markers work correctly with segments (percentage relative to segment, not original line)\n\n\n\n\nThe bounding box feature provides a visual outline around mask regions:\n\nget_bounding_box(): Utility function in masks.hpp that calculates min/max coordinates\n\nIterates through all mask points to find extrema\nReturns pair of Point2D representing opposite corners\nHandles single-point masks correctly\n\nRendering Logic: Draws unfilled rectangles using Qt’s addRect() with Qt::NoBrush\nCoordinate Scaling: Applies same aspect ratio scaling as mask data\nMultiple Masks: Each mask gets its own bounding box when feature is enabled\nTime Handling: Bounding boxes are drawn for both current time and time -1 masks\nContainer Management: Uses separate _mask_bounding_boxes container (similar to digital intervals) to avoid type conflicts\n\n\n\n\n\n\n\n\n\n\n\nstruct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n    bool show_position_marker{false};\n    int position_percentage{20};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - Position markers use get_position_at_percentage() for accurate placement - All configurations support real-time updates without data loss\n\n\n\n\n\n\n\nSelect the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\nEnable position marker to highlight specific locations along lines\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nUse get_position_at_percentage() for accurate line position calculations\nDocument new features in this file\n\n\n\n\nMask data represents 2D regions or shapes that can be overlaid on the media canvas:\n\nColor and Alpha: Configurable mask color and transparency\nBounding Box: Display rectangular outline around the mask extent (off by default)\n\nshow_bounding_box (bool): Enable/disable bounding box display\n\n\n\n\n\n\n\n\nThe mask display system includes:\n\nColor and Alpha Control: Masks can be displayed with configurable colors and transparency levels\nBounding Box: Option to display a rectangular outline around each mask showing its bounds\nOutline Drawing: Option to display the mask boundary as a thick line by connecting extremal points\n\n\n\nWhen enabled, a bounding box renders a rectangle outline around each mask. The implementation: - Uses the existing get_bounding_box() utility function from masks.hpp - Scales coordinates properly with aspect ratios - Draws unfilled rectangles using Qt::NoBrush for outline-only appearance - Handles both current time and time -1 masks\n\n\n\nWhen enabled, displays the mask boundary as a thick line connecting extremal points. The algorithm: - For each unique x coordinate, finds the maximum y value - For each unique y coordinate, finds the maximum x value\n- Collects all extremal points and sorts them by angle from centroid - Connects points to form a closed boundary outline - Renders as a thick 4-pixel wide line using QPainterPath\nThe outline feature uses the get_mask_outline() function to compute boundary points by finding extremal coordinates, providing a visual representation of the mask’s outer boundary."
  },
  {
    "objectID": "developer/display_options.html#overview",
    "href": "developer/display_options.html#overview",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "The display system in WhiskerToolbox allows users to customize the visual appearance of various data types including points and lines through configurable display options."
  },
  {
    "objectID": "developer/display_options.html#point-display-options",
    "href": "developer/display_options.html#point-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-50 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 5 pixels\n\n\n\n\nThe system supports six different marker shapes:\n\nCircle (default) - Filled circular marker\nSquare - Filled rectangular marker\n\nTriangle - Filled triangular marker pointing upward\nCross - Plus sign (+) shaped marker\nX - X-shaped marker (×)\nDiamond - Diamond shaped marker (rotated square)\n\n\n\n\n\nPoint configuration is stored in the PointDisplayOptions structure\nUI controls are synchronized to prevent conflicts\nReal-time updates are supported via Qt signals/slots\nDifferent marker shapes are rendered using appropriate Qt drawing primitives"
  },
  {
    "objectID": "developer/display_options.html#line-display-options",
    "href": "developer/display_options.html#line-display-options",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Range: 1-20 pixels\nUI Controls:\n\nHorizontal slider for quick adjustments\nSpin box for precise numeric input\nSynchronized controls (changing one updates the other)\n\nDefault Value: 2 pixels\n\n\n\n\n\nShow Points: Option to display open circles at each data point along the line\nEdge Snapping: Enable automatic snapping to detected edges when adding points\nPosition Marker: Display a marker at a specified percentage distance along the line (off by default)\n\nshow_position_marker (bool): Enable/disable position marker display\nposition_percentage (int, 0-100%): Position along line where marker appears\n\nLine Segment: Display only a portion of the line between two percentage points (off by default)\n\nshow_segment (bool): Enable/disable segment-only display mode\nsegment_start_percentage (int, 0-100%): Start percentage for line segment\nsegment_end_percentage (int, 0-100%): End percentage for line segment\n\nColor and Alpha: Configurable line color and transparency\n\n\n\n\n\nRange: 0-100% along the cumulative length of the line\nUI Controls:\n\nCheckbox to enable/disable the feature (off by default)\nHorizontal slider for quick adjustments\nSpin box for precise numeric input with % suffix\n\nDefault Value: 20% along the line\nVisual Appearance: Distinctive filled circle with white border, same color as the line\nCalculation: Based on cumulative distance along line segments, not point indices\n\n\n\n\n\nLine configuration is stored in the LineDisplayOptions structure\n\nLine thickness is applied via QPen::setWidth() during rendering\nPosition marker calculated using cumulative distance along line segments\nUI controls follow the same synchronization pattern as point controls\nReal-time updates are supported via Qt signals/slots\n\n\n\nThe segment feature allows displaying only a portion of the line between two percentage points along its cumulative distance:\n\nget_segment_between_percentages(): Utility function in lines.hpp that extracts a continuous segment\n\nCalculates cumulative distances along the original line\nPerforms linear interpolation for precise start/end points\nReturns a new Line2D containing only the specified segment\nHandles edge cases (empty lines, invalid percentages, zero-length segments)\n\nUI Validation: Start percentage cannot exceed end percentage (enforced in UI)\nRendering Logic: When enabled, replaces the full line with the extracted segment in all rendering operations\nPosition Marker Compatibility: Position markers work correctly with segments (percentage relative to segment, not original line)\n\n\n\n\nThe bounding box feature provides a visual outline around mask regions:\n\nget_bounding_box(): Utility function in masks.hpp that calculates min/max coordinates\n\nIterates through all mask points to find extrema\nReturns pair of Point2D representing opposite corners\nHandles single-point masks correctly\n\nRendering Logic: Draws unfilled rectangles using Qt’s addRect() with Qt::NoBrush\nCoordinate Scaling: Applies same aspect ratio scaling as mask data\nMultiple Masks: Each mask gets its own bounding box when feature is enabled\nTime Handling: Bounding boxes are drawn for both current time and time -1 masks\nContainer Management: Uses separate _mask_bounding_boxes container (similar to digital intervals) to avoid type conflicts"
  },
  {
    "objectID": "developer/display_options.html#technical-architecture",
    "href": "developer/display_options.html#technical-architecture",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "struct PointDisplayOptions : public BaseDisplayOptions {\n    int point_size{DefaultDisplayValues::POINT_SIZE};\n    PointMarkerShape marker_shape{DefaultDisplayValues::POINT_MARKER_SHAPE};\n};\n\nstruct LineDisplayOptions : public BaseDisplayOptions {\n    int line_thickness{DefaultDisplayValues::LINE_THICKNESS};\n    bool show_points{DefaultDisplayValues::SHOW_POINTS};\n    bool edge_snapping{false};\n    bool show_position_marker{false};\n    int position_percentage{20};\n};\n\n\n\n\nMediaPoint_Widget handles point-specific controls\nMediaLine_Widget handles line-specific controls\nMedia_Window applies the configurations during rendering\nSynchronized UI controls prevent user confusion\n\n\n\n\nThe rendering system in Media_Window applies the display options during the plotting phase: - Point markers are drawn using the configured size and shape - Lines are drawn using the configured thickness via QPen - Position markers use get_position_at_percentage() for accurate placement - All configurations support real-time updates without data loss"
  },
  {
    "objectID": "developer/display_options.html#usage-guidelines",
    "href": "developer/display_options.html#usage-guidelines",
    "title": "Display Options for Media Widgets",
    "section": "",
    "text": "Select the data item from the feature table\nAdjust display options in the widget panel\nChanges are applied immediately to the visualization\nUse sliders for quick adjustments or spinboxes for precise values\nEnable position marker to highlight specific locations along lines\n\n\n\n\n\nAll display options inherit from BaseDisplayOptions\nUI controls should use blockSignals() to prevent recursive updates\nFollow the established naming convention for slot functions\nAdd corresponding test cases for new display options\nUse get_position_at_percentage() for accurate line position calculations\nDocument new features in this file\n\n\n\n\nMask data represents 2D regions or shapes that can be overlaid on the media canvas:\n\nColor and Alpha: Configurable mask color and transparency\nBounding Box: Display rectangular outline around the mask extent (off by default)\n\nshow_bounding_box (bool): Enable/disable bounding box display\n\n\n\n\n\n\n\n\nThe mask display system includes:\n\nColor and Alpha Control: Masks can be displayed with configurable colors and transparency levels\nBounding Box: Option to display a rectangular outline around each mask showing its bounds\nOutline Drawing: Option to display the mask boundary as a thick line by connecting extremal points\n\n\n\nWhen enabled, a bounding box renders a rectangle outline around each mask. The implementation: - Uses the existing get_bounding_box() utility function from masks.hpp - Scales coordinates properly with aspect ratios - Draws unfilled rectangles using Qt::NoBrush for outline-only appearance - Handles both current time and time -1 masks\n\n\n\nWhen enabled, displays the mask boundary as a thick line connecting extremal points. The algorithm: - For each unique x coordinate, finds the maximum y value - For each unique y coordinate, finds the maximum x value\n- Collects all extremal points and sorts them by angle from centroid - Connects points to form a closed boundary outline - Renders as a thick 4-pixel wide line using QPainterPath\nThe outline feature uses the get_mask_outline() function to compute boundary points by finding extremal coordinates, providing a visual representation of the mask’s outer boundary."
  },
  {
    "objectID": "developer/Features/overview.html",
    "href": "developer/Features/overview.html",
    "title": "Features",
    "section": "",
    "text": "Documentation of some of the general widget types in Neuralyzer.",
    "crumbs": [
      "Features"
    ]
  },
  {
    "objectID": "developer/fuzz/fuzzing.html",
    "href": "developer/fuzz/fuzzing.html",
    "title": "Fuzz Testing",
    "section": "",
    "text": "Fuzz testing is a way that we can test programs with random inputs. This is a technique that allows us to test many interesting edge cases very efficiently and really harden our program against failing.",
    "crumbs": [
      "Fuzz Testing"
    ]
  },
  {
    "objectID": "developer/fuzz/fuzzing.html#overview",
    "href": "developer/fuzz/fuzzing.html#overview",
    "title": "Fuzz Testing",
    "section": "",
    "text": "Fuzz testing is a way that we can test programs with random inputs. This is a technique that allows us to test many interesting edge cases very efficiently and really harden our program against failing.",
    "crumbs": [
      "Fuzz Testing"
    ]
  },
  {
    "objectID": "developer/fuzz/fuzzing.html#procedure",
    "href": "developer/fuzz/fuzzing.html#procedure",
    "title": "Fuzz Testing",
    "section": "Procedure",
    "text": "Procedure\nThe procedure might look like this: Imagine we have an input file for some kind of processing. In fuzz testing, we could randomly generate text and feed it to our transformation function, and we could do this many, many times. We consider a success in this case to be if the program doesn’t crash.\nIn our circumstance with Neuralyzer we could imagine that for loading data, we could randomly generate lots and lots of data and see if those loading functions crash whenever we try to load it. Alternatively, we could randomly generate different specification files or configuration files to load that data and see if it crashes. If we forget a semicolon or something that causes a critical failure, does it crash, or does it fail gracefully and give some kind of useful message to the user?",
    "crumbs": [
      "Fuzz Testing"
    ]
  },
  {
    "objectID": "developer/fuzz/fuzzing.html#structured-fuzzing-and-corpora",
    "href": "developer/fuzz/fuzzing.html#structured-fuzzing-and-corpora",
    "title": "Fuzz Testing",
    "section": "Structured Fuzzing and Corpora",
    "text": "Structured Fuzzing and Corpora\nWe also can try to add some structure to our fuzzing in what is referred to as a corpus for our fuzz test. In a corpus, we specify a range of parameter values, and we can vary some parameters within that range and see if the output changes. This can help us catch things that are more specific than just random crashes. By changing values to be within the ranges that we would expect and doing this many times over many different parameters, we may be able to catch things that we wouldn’t see otherwise.",
    "crumbs": [
      "Fuzz Testing"
    ]
  },
  {
    "objectID": "developer/fuzz/fuzzing.html#neuralyzer-and-chaining-transformations",
    "href": "developer/fuzz/fuzzing.html#neuralyzer-and-chaining-transformations",
    "title": "Fuzz Testing",
    "section": "Neuralyzer and Chaining Transformations",
    "text": "Neuralyzer and Chaining Transformations\nWe also might be able to test how we chain together different types of transformations. In Neuralyzer, we have the ability to chain together different types of data transformations. By chaining together data transformations in a random way, we might be able to ensure our processing pipelines are robust in this type of variability.\nThe goal of Neuralyzer is to be able to have as many of the specifications as we can in a format that can be read from JSON files. This is facilitated with a reflection library known as Reflect CPP. The more things that we have in JSON specifications, the easier it is for us to use a corpus to load those and test them with fuzzing.",
    "crumbs": [
      "Fuzz Testing"
    ]
  },
  {
    "objectID": "developer/fuzz/fuzzing.html#implementation",
    "href": "developer/fuzz/fuzzing.html#implementation",
    "title": "Fuzz Testing",
    "section": "Implementation",
    "text": "Implementation\nFuzz testing in this library can be specified with the fuzz test library made by Google: https://github.com/google/fuzztest.\nWhenever we use the fuzz testing library for a translation unit, we should specify that library for that file with the name of the translation unit. We can specify a fuzzing file as .fuzz.cpp in our library, and then we can add it to the fuzz test driver. Again, these should be written in such a way that if expectations are not met, it will flag what the state was from the random state there, help us understand why this test may have failed, and repeat it with those more specific parameters.",
    "crumbs": [
      "Fuzz Testing"
    ]
  },
  {
    "objectID": "developer/index.html",
    "href": "developer/index.html",
    "title": "Developer Documentation",
    "section": "",
    "text": "Welcome to the developer documentation for Neuralyzer. This is a C++ project for data analysis and visualization focused on the multi-modal data found in Systems Neuroscience. While higher level languages like MATLAB or Python are more common in our field, C++ was selected for this package to hide the programming aspects from the average user so that it “just works”. However, if you are here, you most likely want to try to modify Neuralyzer in some way. If you are new to C++, I have collected resouces that helped me to learn in the Intro to C++ section.\nThe Build Instructions section has a guide with getting an operational, developmental version of Neuralyzer on your own machine.\nOnce you have a working version of Neuralyzer, I recommend checking out the design guidelines and code quality section. It contains the design specifications and rules for writing C++ code for this project. It also contains descriptions of some of the tooling that can do most of this for you.\nFor starting projects, I recommend looking into four separate initiatives to ensure Neuralyzer remains performant and robust. These are good starting points to learn more about how Neuralyzer works under the hood:\n\nTesting: Unit tests are critical for ensuring that code changes do not break existing functionality. This section describes the testing framework and guidelines for writing tests.\nProfiling and Performance: Profiling tools can help identify performance bottlenecks in the code. This section describes how to use profiling tools to optimize code performance.\nStatic Analysis: Static analysis tools can help identify potential issues in the code before they become problems. This section describes how to use static analysis tools to improve code quality.\nFuzzing: Fuzzing is a technique for testing software by providing random inputs to the program. This section describes how to use fuzzing to identify bugs in the code or unexpected edge cases in complex algorithms.\n\nTo view how specific parts of Neuralyzer work under the hood, please view the implementation documentation.",
    "crumbs": [
      "Developer Documentation"
    ]
  },
  {
    "objectID": "developer/media_widget.html",
    "href": "developer/media_widget.html",
    "title": "Media Widget",
    "section": "",
    "text": "The media widget is responsible for displaying data that can be visualized on a canvas. The types of data that can be displayed by the data manager include point data, line data, mask data, tensor data, interval data, and media data. Data currently in the data manager matching these types is displayed on the top left side. In this table, the user can click to enable the visibility of particular data in the accompanying media window canvas.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/media_widget.html#overview-and-data-display",
    "href": "developer/media_widget.html#overview-and-data-display",
    "title": "Media Widget",
    "section": "",
    "text": "The media widget is responsible for displaying data that can be visualized on a canvas. The types of data that can be displayed by the data manager include point data, line data, mask data, tensor data, interval data, and media data. Data currently in the data manager matching these types is displayed on the top left side. In this table, the user can click to enable the visibility of particular data in the accompanying media window canvas.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/media_widget.html#data-specific-interactions-and-manipulations",
    "href": "developer/media_widget.html#data-specific-interactions-and-manipulations",
    "title": "Media Widget",
    "section": "Data-Specific Interactions and Manipulations",
    "text": "Data-Specific Interactions and Manipulations\nClicking on a data item also brings up a sub-widget below the table, offering data type-specific manipulations. For example, this sub-widget would be responsible for changing attributes such as color, alpha value, marker type, etc., for point data within the media window. This sub-widget may also handle data-specific manipulations directly within the media window. For instance, the user might wish to click on a position in the media window to set it as the location for the currently displayed point. Alternatively, the user might click and hold to utilize a paintbrush-type function for extending or erasing mask data.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/media_widget.html#shared-components-and-common-interactions",
    "href": "developer/media_widget.html#shared-components-and-common-interactions",
    "title": "Media Widget",
    "section": "Shared Components and Common Interactions",
    "text": "Shared Components and Common Interactions\nThese data-specific sub-widgets may house common sub-widgets that are shared among others. For instance, color selection is a common feature for multiple data types. An interface for click, hover, and release interactions is common to multiple data types but may perform different actions, such as painting a mask or erasing the media foreground.\nNote that there is some overlap here with the data manager widget. For many data types the data manager widget includes the ability to view all data present in the data manager and select particular instances such as a point at a certain time and the user can delete that point. Consequently we will try to keep media window based manipulation to the media widget. \nThe media window is the widget that houses the actual canvas for display. It is also the owner of the specific drawing options and drawing routines onto the canvas. For instance if the user changes the marker size and color of point data the options that are updated are held by the media window. The media window uses qt-based drawing. It is also responsible for receiving mouse events inside its space and emits signals that other widgets can receive.",
    "crumbs": [
      "Media Widget"
    ]
  },
  {
    "objectID": "developer/state_estimation.html",
    "href": "developer/state_estimation.html",
    "title": "State Estimation Library",
    "section": "",
    "text": "The primary goals of this library are to provide a modular framework for state estimation tasks. The high-level objectives include:\n\nGenerate Smooth Estimates: Produce a smooth estimate of a state from raw data.\nCompare and Validate: Generate a smooth estimate and compare it against a pre-existing estimate, with the ability to flag potential outliers.\nInterpolate Missing Data: Generate smoothed predictions that can fill in gaps where data is missing. This is in contrast to simply refining existing measurements.\nAssign Identity to Unlabeled Data: Use predictions to assign identities to measured data points within a time series that has only sparse labels."
  },
  {
    "objectID": "developer/state_estimation.html#core-goals",
    "href": "developer/state_estimation.html#core-goals",
    "title": "State Estimation Library",
    "section": "",
    "text": "The primary goals of this library are to provide a modular framework for state estimation tasks. The high-level objectives include:\n\nGenerate Smooth Estimates: Produce a smooth estimate of a state from raw data.\nCompare and Validate: Generate a smooth estimate and compare it against a pre-existing estimate, with the ability to flag potential outliers.\nInterpolate Missing Data: Generate smoothed predictions that can fill in gaps where data is missing. This is in contrast to simply refining existing measurements.\nAssign Identity to Unlabeled Data: Use predictions to assign identities to measured data points within a time series that has only sparse labels."
  },
  {
    "objectID": "developer/state_estimation.html#design-principles-and-variation-points",
    "href": "developer/state_estimation.html#design-principles-and-variation-points",
    "title": "State Estimation Library",
    "section": "Design Principles and Variation Points",
    "text": "Design Principles and Variation Points\nThe library should be designed in a modular fashion to accommodate several key points of variation in state estimation problems. The interfaces should be flexible enough to handle significant structural differences between algorithms.\n\nData Type\nThe nature of the input data is a primary variation point. The library must handle:\n\nContinuous Data: Analog time series or other continuous state variables.\nDiscrete Data: Data that exists on a discrete grid or set of points. An example would be making a prediction for a point within an image mask where only certain pixels are available.\n\n\n\nFiltering Schemes\nThe design must support different families of filtering algorithms, which may have different assumptions about data distributions and different methodological considerations.\n\nKalman Filters:\n\nStandard Kalman Filter\nExtended Kalman Filter (EKF) for linearized systems\nUnscented Kalman Filter (UKF)\n\nParticle Filters:\n\nBootstrap Particle Filter\nSequential Importance Sampling with Resampling (SIR)\n\n\n\n\nSmoothing Operations\nAlongside forward-in-time filtering, the library must incorporate smoothing operations, which use the full dataset to refine estimates by making a backward pass through the data. The design should recognize that some smoothers are tightly paired with specific filters, while others may be more general.\n\nPaired Smoother: An example is the Rauch-Tung-Striebel (RTS) smoother, which is specifically paired with the Kalman filter.\nComplex Smoothers: For particle filters, smoothing and resampling schemes can be more complicated and may require filter-specific implementations.\n\n\n\nAncillary Tools for Performance\nTo ensure high performance, especially for complex schemes, the library should incorporate specialized data structures and algorithms.\n\nExample: For particle filters operating on spatial data, a k-d tree could be implemented to accelerate the lookup and resampling of large particle fields based on their location.\n\nThe overall goal is to design interfaces that are modular enough to accommodate these major structural variations as new algorithms and techniques are implemented."
  },
  {
    "objectID": "developer/state_estimation.html#design-roadmap",
    "href": "developer/state_estimation.html#design-roadmap",
    "title": "State Estimation Library",
    "section": "Design Roadmap",
    "text": "Design Roadmap\n\nFilter Types\n\n\n\nFilter Type\nCompletion Status\n\n\n\n\nStandard Kalman Filter\nImplemented\n\n\nExtended Kalman Filter (EKF)\nPlanned\n\n\nUnscented Kalman Filter (UKF)\nPlanned\n\n\nBootstrap Particle Filter\nImplemented\n\n\nSequential Importance Resampling\nPlanned\n\n\n\n\n\nSmoothing Types\n\n\n\nSmoothing Type\nCompletion Status\n\n\n\n\nRauch-Tung-Striebel (RTS)\nImplemented\n\n\nParticle Filter Smoother\nPlanned"
  },
  {
    "objectID": "developer/testing/testing.html",
    "href": "developer/testing/testing.html",
    "title": "Testing",
    "section": "",
    "text": "Testing makes sure that our code works as expected, and that future changes do not break existing functionality.\n\nTesting Guidelines\n\nTesting is performed with Catch2. \nThe component being tested generally has two TEST_CASE parts. The first will test the “happy path” to ensure that the computations work as expected. Different SECTIONs will be used for different computations. A second TEST_CASE will handle error handling and edge cases. Each SECTION will be for a different edge case / error.\nUse descriptive names for each test.\nTEST_CASES should also use useful tags.\nUse REQUIRE instead of CHECK\nSimple setup can be performed in the beginning of a TEST_CASE. Fixtures are only necessary for complex setup/teardown.\nPrefer Catch::Matchers::WithinRel to Catch::Approx\nTest files are included in the same folder as a given translation unit. They should have the same name as the header file with the extension “.test.cpp”. For example mask_to_line.hpp and mask_to_line.cpp will have the test file mask_to_line.test.cpp.",
    "crumbs": [
      "Testing"
    ]
  },
  {
    "objectID": "developer/transforms.html",
    "href": "developer/transforms.html",
    "title": "transforms",
    "section": "",
    "text": "Transform Catalog\nThe below table has all of the transformations currently available in the Data Transform widget. Each transform should have a test.cpp file (Column 2). The test.cpp file should include tests to make sure it can be populated by a JSON template (e.g. it can be run by loading data with a JSON file). Some transforms will “just work” because they are parameterless, but transforms with parameters will need to have them also described with a JSON link and put in the transform parameter factory. Documentation for users should be provided to describe the algorithm and its uses. Finally, a benchmark.cpp file should be created and placed in the same directory.\n\n\n\n\n\n\n\n\n\n\n\n\nTransform Name\nHas .test.cpp?\nJSON-based Test?\nParameters in ParameterFactory?\nUser Docs Exist?\nHas benchmark.cpp?\nBenchmark Results\n\n\n\n\nCalculate Mask Centroid\nYes\nYes\nYes\nYes\nNo\n\n\n\nConvert Mask to Line\nYes\nYes\nYes\nNo\nNo\n\n\n\nSkeletonize Mask\nYes\nYes\nYes\nYes\nNo\n\n\n\nRemove Small Connected Components\nYes\nYes\nYes\nYes\nNo\n\n\n\nApply Median Filter\nYes\nYes\nYes\nYes\nNo\n\n\n\nCalculate Area\nYes\nYes\nYes\nYes\nNo\n\n\n\nFill Mask Holes\nYes\nYes\nYes\nYes\nNo\n\n\n\nCalculate Mask Principal Axis\nYes\nYes\nYes\nYes\nNo\n\n\n\nWhisker Tracing\nNo\nN/A\nNo\nNo\nNo\n\n\n\nClip Line by Reference Line\nYes\nYes\nYes\nYes\nNo\n\n\n\nLine Alignment to Bright Features\nYes\nYes\nYes\nNo\nYes\n10 images in 2ms\n\n\nCalculate Line to Point Distance\nYes\nYes\nYes\nYes\nNo\n\n\n\nResample Line\nYes\nYes\nYes\nYes\nNo\n\n\n\nExtract Line Subsegment\nYes\nYes\nYes\nYes\nNo\n\n\n\nCalculate Line Angle\nYes\nYes\nYes\nYes\nNo\n\n\n\nCalculate Line Curvature\nYes\nYes\nYes\nYes\nNo\n\n\n\nExtract Point from Line\nYes\nYes\nYes\nYes\nNo\n\n\n\nFilter\nYes\nNo\nNo\nNo\nNo\n\n\n\nHilbert Phase\nYes\nYes\nYes\nYes\nNo\n\n\n\nThreshold Event Detection\nYes\nYes\nYes\nYes\nNo\n\n\n\nThreshold Interval Detection\nYes\nYes\nYes\nYes\nNo\n\n\n\nScale and Normalize\nYes\nYes\nYes\nNo\nNo\n\n\n\nGroup Intervals\nYes\nYes\nYes\nYes\nNo",
    "crumbs": [
      "transforms"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neuralyzer",
    "section": "",
    "text": "Neuralyzer is a cross-platform software package designed for analyzing and visualizing common forms of data generated during systems neuroscience experiments."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Neuralyzer",
    "section": "Installation",
    "text": "Installation\nThe software comes as compiled .exe, .dmg or tar files and can be immediately used on Windows, Mac and Linux - no install required !\nLatest Release: link\nOld Releases: link"
  },
  {
    "objectID": "index.html#supported-data-types",
    "href": "index.html#supported-data-types",
    "title": "Neuralyzer",
    "section": "Supported Data Types",
    "text": "Supported Data Types\nSome currently supported data types include:\n\nMultimedia - High speed video and collections of images\nArrays of simple 2D shapes - Points, Lines, and masks that vary with time.\nDigital time series data - Events, timestamps,\nAnalog time series data - Continuous movement variables"
  },
  {
    "objectID": "index.html#how-documentation-is-organized",
    "href": "index.html#how-documentation-is-organized",
    "title": "Neuralyzer",
    "section": "How Documentation is Organized",
    "text": "How Documentation is Organized\nThis documentation will follow the quadrants of the Diátaxis documentation authoring framework\n\nTutorials\nHow-to Guides\nConcepts\nReference Guides"
  },
  {
    "objectID": "labeling/mask.html",
    "href": "labeling/mask.html",
    "title": "Creating Mask Labels",
    "section": "",
    "text": "Creating mask labels is done through the GrabCut tool. Currently it can be opened through the Tongue Tracking widget."
  },
  {
    "objectID": "labeling/mask.html#grabcut-tool-operation",
    "href": "labeling/mask.html#grabcut-tool-operation",
    "title": "Creating Mask Labels",
    "section": "GrabCut Tool Operation",
    "text": "GrabCut Tool Operation\n\nSelecting ROI\nTo start creating a mask, left click and drag the mouse over the image to form a green rectangle as the region of interest. The objected intended to be masked should be completely within this rectangle. If a mistake is made the reset button returns the tool to the initial state.\nThe GrabCut algorithm works as such: the algorithm keeps track of the mask and for each iteration, attempts to refine it. In between iterations, the user can tweak the mask to provide feedback to the algorithm, which will be noted through subsequent iterations.\nAfter drawing an ROI the “Iterate Grabcut” button becomes functional. Using it completes one iteration of the GrabCut algorithm. The first usage of this button will show the initial mask created by the algorithm. Ideally the user should press this button throughout the editing process.\n\nOpacity of the displayed mask can be adjusted with the transparency slider\n\n\n\nUser Feedback\nIn between iterations the user has access to a drawing bush whose radius can be adjusted. It is used to paint feedback for the GrabCut algorithm, which it will take into account upon pressing the “Iterate Grabcut” button. The brush has four colors:\n\n“Definite Background”: Tells GrabCut the painted area is definitely not part of the mask.\n“Definite Foreground”: Tells GrabCut the painted area is definitely part of the mask.\n“Probable Background”: Suggests to GrabCut the painted area may not be part of the mask. GrabCut uses this information to create a better mask but may partially/fully disobey it.\n“Probable Foreground”: Suggests to GrabCut the painted area is likely part of the mask. GrabCut uses this information to create a better mask but may partially/fully disobey it.\n\n\n\nSaving and Exporting\nThe GrabCut tool may be exited at any time by pressing the “Exit” button. “Save and Exit” will exit the tool and save the mask into the main application and displayed in the media player. All created masks can be saved to disk using the “Save Drawn Masks” button in the Tongue Tracking widget."
  },
  {
    "objectID": "user_guide/behaviors/whisker.html",
    "href": "user_guide/behaviors/whisker.html",
    "title": "Whisker Tracking",
    "section": "",
    "text": "Load Whiskers\n\nSupported Whisker File Formats\n\n\n\n\n\n\nFile Format\nDescription\n\n\n\n\nJanelia\nBinary format output by janelia whisker tracker\n\n\nCSV\nEach row represents a 2d point (x,y) along the whisker. The points should be arranged from follicle to tip\n\n\nHDF5\n\n\n\n\n\n\nLoad Keypoints\n\n\nTrace Button\n\n\nLength Threshold\nWhisker segments below the length threshold will be discarded. Length threshold is in units of pixels\n\n\nWhisker Pad Selection\nThe whisker pad location in pixel coordinates. Candidate whiskers are ordered so that the base of the whisker is nearest the whisker pad. Whiskers with bases beyond some distance from the whisker pad can also be discarded.\n\n\nHead Orientation\nThe head orientation is the direction that the animal’s nose is pointing in the image. The head orientation is used to determine the identity of the whiskers in the frame (most posterior whisker is 0, next most posterior is 1, etc).\n\n\nNumber of Whiskers Selection\n\n\nExport Image and CSV Button\n\n\nFace Mask\nThe face mask corresponds to the part of the image belonging to the face. This can be used in several ways\n\nWhisker bases can be extended to always connect to the face mask. This eliminates jitter that can occur because of fur\nWhisker bases can be clipped to ensure that the whisker does not enter the face mask.\n\n\n\nJanelia Settings\n\n\nContact Detection",
    "crumbs": [
      "Behavioral Modules",
      "Whisker Tracking"
    ]
  },
  {
    "objectID": "user_guide/data_loading/JSON_loading.html",
    "href": "user_guide/data_loading/JSON_loading.html",
    "title": "JSON_loading",
    "section": "",
    "text": "Digital Event Series\nDigital event series are data represented by an ordered sequence of timestamps. Examples include spike timestamps from extracellular recordings or behavioral events (e.g. go cue, reward given).\n\n\nDigital Interval Series\n\n16 bit binary representation\n{\n  \"filepath\": \"ttl.bin\",\n  \"data_type\": \"digital_interval\",\n  \"name\": \"laser\",\n  \"format\": \"uint16\",\n  \"channel\": 2, // REQUIRED, bit (0 based) for channel of interest\n  \"transition\": \"rising\", //optional, \n  \"clock\": \"master\", //optional, clock signal to assign to these events\n  \"header_size\": 0 //optional, number of bytes to skip at start of file\n}\n\n\n\n\n\n\n\n\n\n\nElement\nDescription\nRequired?\nType\nNotes\n\n\n\n\nfilepath\nPath to binary file, relative to JSON file.\nYes\nstring\n\n\n\ndata_type\n\nYes\nstring\n“digital_interval”\n\n\nname\nName of the data once it is loaded into Neuralyzer\nYes\nstring\n\n\n\nformat\n\nYes\nstring\n“uint16”\n\n\nchannel\nSpecifies which bit in binary representation should be extracted as digital interval\nNo\nnumber\nDefault is 0. Valid values range from 0-15\n\n\nTransition\n“rising” will count a TTL interval as one extending from low-to-high transitions to high-to-low transitions. “falling” will count a TTL interval as one extending from high-to-low to low-to-high transitions.\nNo\nstring\nDefault is “rising”. Valid values are “rising” or “falling”.\n\n\nclock\nClock signal to associate with this digital interval\nNo\nstring\nThe clock string must match the name of a loaded clock signal.\n\n\nheader_size\nThis many bytes will be skipped at the beginning of the file before reading the rest.\nNo\nnumber\nDefault is 0. Accepted values range from 0 to size of file in bytes.\n\n\n\n\n\n\n\n\nCSV\n\n{\n  \"filepath\": \"ttl.bin\",\n  \"data_type\": \"digital_interval\",\n  \"name\": \"laser\",\n  \"format\": \"csv\"\n\n}\n\n\n\n\n\n\n\n\n\n\nElement\nDescription\nRequired?\nType\nNotes\n\n\n\n\nfilepath\nPath to csv file, relative to JSON file.\nYes\nstring\n\n\n\ndata_type\n\nYes\nstring\n“digital_interval”\n\n\nname\nName of the data once it is loaded into Neuralyzer\nYes\nstring\n\n\n\nformat\n\nYes\nstring\n“csv”",
    "crumbs": [
      "Data Loading",
      "JSON_loading"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html",
    "href": "user_guide/data_transformations/analog_filter.html",
    "title": "Analog Filter",
    "section": "",
    "text": "The Analog Filter transform applies a digital filter to an AnalogTimeSeries. This can be used to remove noise, isolate specific frequency bands, or perform other signal processing tasks."
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#purpose",
    "href": "user_guide/data_transformations/analog_filter.html#purpose",
    "title": "Analog Filter",
    "section": "",
    "text": "The Analog Filter transform applies a digital filter to an AnalogTimeSeries. This can be used to remove noise, isolate specific frequency bands, or perform other signal processing tasks."
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#parameters",
    "href": "user_guide/data_transformations/analog_filter.html#parameters",
    "title": "Analog Filter",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilter_type\nstring\nThe type of filter to apply. One of “lowpass”, “highpass”, “bandpass”, “bandstop”.\n“lowpass”\n\n\ncutoff_frequency\ndouble\nThe cutoff frequency for lowpass and highpass filters, or the lower cutoff frequency for bandpass and bandstop filters.\n10.0\n\n\ncutoff_frequency2\ndouble\nThe upper cutoff frequency for bandpass and bandstop filters.\n0.0\n\n\norder\ninteger\nThe order of the filter. Must be between 1 and 8.\n4\n\n\nripple\ndouble\nThe ripple for Chebyshev filters (in dB). Not used for Butterworth filters.\n0.0\n\n\nzero_phase\nboolean\nIf true, applies the filter forward and backward to eliminate phase distortion.\nfalse"
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#json-schema",
    "href": "user_guide/data_transformations/analog_filter.html#json-schema",
    "title": "Analog Filter",
    "section": "JSON Schema",
    "text": "JSON Schema\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"transform\": {\n      \"type\": \"string\",\n      \"const\": \"Analog Filter\"\n    },\n    \"input\": {\n      \"type\": \"string\"\n    },\n    \"output\": {\n      \"type\": \"string\"\n    },\n    \"params\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"filter_type\": {\n          \"type\": \"string\",\n          \"enum\": [\"lowpass\", \"highpass\", \"bandpass\", \"bandstop\"]\n        },\n        \"cutoff_frequency\": {\n          \"type\": \"number\"\n        },\n        \"cutoff_frequency2\": {\n          \"type\": \"number\"\n        },\n        \"order\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"maximum\": 8\n        },\n        \"ripple\": {\n          \"type\": \"number\"\n        },\n        \"zero_phase\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\"filter_type\", \"cutoff_frequency\", \"order\"]\n    }\n  },\n  \"required\": [\"transform\", \"input\", \"output\", \"params\"]\n}"
  },
  {
    "objectID": "user_guide/data_transformations/analog_filter.html#minimal-example",
    "href": "user_guide/data_transformations/analog_filter.html#minimal-example",
    "title": "Analog Filter",
    "section": "Minimal Example",
    "text": "Minimal Example\nThis example applies a 4th order lowpass Butterworth filter at 100 Hz.\n{\n  \"transform\": \"Analog Filter\",\n  \"input\": \"my_analog_series\",\n  \"output\": \"filtered_series\",\n  \"params\": {\n    \"filter_type\": \"lowpass\",\n    \"cutoff_frequency\": 100,\n    \"order\": 4\n  }\n}"
  },
  {
    "objectID": "user_guide/data_transformations/analog_interval_threshold.html",
    "href": "user_guide/data_transformations/analog_interval_threshold.html",
    "title": "Analog Interval Threshold",
    "section": "",
    "text": "This transform detects continuous time intervals during which an analog signal’s value meets a specified threshold condition (e.g., staying above a value).\n\n\nThis transformation is used to identify continuous periods, or “intervals,” where a signal’s amplitude meets certain criteria. Unlike event detection, which marks a single point in time, interval detection finds the start and end times of periods that satisfy the condition.\nThe user can define a threshold and specify whether the signal must be above (Positive), below (Negative), or have its absolute value exceed (Absolute) that threshold.\nAdditionally, two timing parameters help refine the detection: - A lockout_time prevents the detection of new intervals for a set duration after the start of a previous interval. - A min_duration ensures that only intervals lasting longer than a specified duration are included in the final output.\nThis transform is useful for isolating epochs of interest in a continuous signal for further analysis. It takes an analog time series as input and produces a digital interval series as output.\n\n\n\n\nThis transform is particularly useful for identifying specific “states” or “epochs” in neural or behavioral data.\n\nIdentifying Periods of Movement: From accelerometer or video tracking data, one can find intervals of high activity that correspond to an animal moving, versus periods of rest.\nAnalyzing Muscle Activation: In electromyography (EMG), this can be used to determine the duration of muscle contractions by finding intervals where the rectified signal remains above a certain level of activity.",
    "crumbs": [
      "Data Transformations",
      "Analog Interval Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_interval_threshold.html#overview",
    "href": "user_guide/data_transformations/analog_interval_threshold.html#overview",
    "title": "Analog Interval Threshold",
    "section": "",
    "text": "This transform detects continuous time intervals during which an analog signal’s value meets a specified threshold condition (e.g., staying above a value).\n\n\nThis transformation is used to identify continuous periods, or “intervals,” where a signal’s amplitude meets certain criteria. Unlike event detection, which marks a single point in time, interval detection finds the start and end times of periods that satisfy the condition.\nThe user can define a threshold and specify whether the signal must be above (Positive), below (Negative), or have its absolute value exceed (Absolute) that threshold.\nAdditionally, two timing parameters help refine the detection: - A lockout_time prevents the detection of new intervals for a set duration after the start of a previous interval. - A min_duration ensures that only intervals lasting longer than a specified duration are included in the final output.\nThis transform is useful for isolating epochs of interest in a continuous signal for further analysis. It takes an analog time series as input and produces a digital interval series as output.\n\n\n\n\nThis transform is particularly useful for identifying specific “states” or “epochs” in neural or behavioral data.\n\nIdentifying Periods of Movement: From accelerometer or video tracking data, one can find intervals of high activity that correspond to an animal moving, versus periods of rest.\nAnalyzing Muscle Activation: In electromyography (EMG), this can be used to determine the duration of muscle contractions by finding intervals where the rectified signal remains above a certain level of activity.",
    "crumbs": [
      "Data Transformations",
      "Analog Interval Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_interval_threshold.html#parameters",
    "href": "user_guide/data_transformations/analog_interval_threshold.html#parameters",
    "title": "Analog Interval Threshold",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nthreshold_value: The amplitude value the signal must be above/below for the duration of the interval.\ndirection: Specifies the condition the signal must meet. Valid options are: \"Positive\", \"Negative\", or \"Absolute\".\nlockout_time: A duration (in the same time units as the data) after the start of a detected interval, during which no new intervals can begin. This is useful for isolating the onset of distinct episodes of activity.\nmin_duration: The minimum required length of an interval. Any detected period that is shorter than this value will be discarded.\nmissing_data_mode: Defines how to handle non-consecutive time points in the signal.\n\nTreat as Zero: Missing time points are treated as if the signal’s value is zero. This can terminate an interval if zero does not meet the threshold condition.\nIgnore: The algorithm proceeds to the next available data point, effectively ignoring the time gap.",
    "crumbs": [
      "Data Transformations",
      "Analog Interval Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/analog_interval_threshold.html#example-configuration",
    "href": "user_guide/data_transformations/analog_interval_threshold.html#example-configuration",
    "title": "Analog Interval Threshold",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example finds all intervals where a signal named “LFP_channel_1” remains above 1.5 for at least 50 milliseconds.\n[\n  {\n    \"transformations\": {\n      \"metadata\": {\n        \"name\": \"Interval Threshold Detection Pipeline\",\n        \"description\": \"Detects intervals of high activity in an LFP signal.\",\n        \"version\": \"1.0\"\n      },\n      \"steps\": [\n        {\n          \"step_id\": \"1\",\n          \"transform_name\": \"Threshold Interval Detection\",\n          \"phase\": \"analysis\",\n          \"input_key\": \"LFP_channel_1\",\n          \"output_key\": \"detected_activity_intervals\",\n          \"parameters\": {\n            \"threshold_value\": 1.5,\n            \"direction\": \"Positive\",\n            \"lockout_time\": 0.0,\n            \"min_duration\": 0.050,\n            \"missing_data_mode\": \"Treat as Zero\"\n          }\n        }\n      ]\n    }\n  }\n]",
    "crumbs": [
      "Data Transformations",
      "Analog Interval Threshold"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_line_to_point_distance.html",
    "href": "user_guide/data_transformations/calculate_line_to_point_distance.html",
    "title": "Calculate Line to Point Distance",
    "section": "",
    "text": "This transform calculates the minimum distance from a set of points to a line for each moment in time, which is useful for quantifying the spatial relationship between different tracked objects.\n\n\nThis operation computes the shortest Euclidean distance between a line (composed of connected segments) and one or more points at each corresponding timestamp. If multiple points are present at a single timestamp, the transform identifies the minimum distance among all points to the line. The calculation finds the closest point on any segment of the line to each of the provided points and returns the smallest distance found.\nIf the line and points are associated with different spatial scales (i.e., different image sizes), the points will be automatically scaled to the line’s coordinate system before the distance is computed. The output is an analog time series where the value at each time point is the calculated minimum distance.\n\n\n\n\nThis transformation is valuable for analyzing the interaction between different elements in behavioral or physiological experiments:\n\nWhisker Tracking: In studies of rodent behavior, one might track the position of a whisker (as a line) and the location of an object (as a point). This transform can precisely measure the distance from the whisker to the object at each frame of a video, helping to identify moments of contact or near-contact.\nLimb and Body Coordination: When tracking the movement of an animal’s limb (represented as a line) relative to a target or another body part (represented as a point), this transform can quantify their spatial relationship over time. This is useful for studies of motor control and coordination.\nNeural Prosthetics and Brain-Computer Interfaces: In experiments where an animal controls a cursor or robotic arm, this transform could be used to measure the distance from the effector (the line) to a target (the point), providing a continuous measure of performance.",
    "crumbs": [
      "Data Transformations",
      "Calculate Line to Point Distance"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_line_to_point_distance.html#overview",
    "href": "user_guide/data_transformations/calculate_line_to_point_distance.html#overview",
    "title": "Calculate Line to Point Distance",
    "section": "",
    "text": "This transform calculates the minimum distance from a set of points to a line for each moment in time, which is useful for quantifying the spatial relationship between different tracked objects.\n\n\nThis operation computes the shortest Euclidean distance between a line (composed of connected segments) and one or more points at each corresponding timestamp. If multiple points are present at a single timestamp, the transform identifies the minimum distance among all points to the line. The calculation finds the closest point on any segment of the line to each of the provided points and returns the smallest distance found.\nIf the line and points are associated with different spatial scales (i.e., different image sizes), the points will be automatically scaled to the line’s coordinate system before the distance is computed. The output is an analog time series where the value at each time point is the calculated minimum distance.\n\n\n\n\nThis transformation is valuable for analyzing the interaction between different elements in behavioral or physiological experiments:\n\nWhisker Tracking: In studies of rodent behavior, one might track the position of a whisker (as a line) and the location of an object (as a point). This transform can precisely measure the distance from the whisker to the object at each frame of a video, helping to identify moments of contact or near-contact.\nLimb and Body Coordination: When tracking the movement of an animal’s limb (represented as a line) relative to a target or another body part (represented as a point), this transform can quantify their spatial relationship over time. This is useful for studies of motor control and coordination.\nNeural Prosthetics and Brain-Computer Interfaces: In experiments where an animal controls a cursor or robotic arm, this transform could be used to measure the distance from the effector (the line) to a target (the point), providing a continuous measure of performance.",
    "crumbs": [
      "Data Transformations",
      "Calculate Line to Point Distance"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_line_to_point_distance.html#parameters",
    "href": "user_guide/data_transformations/calculate_line_to_point_distance.html#parameters",
    "title": "Calculate Line to Point Distance",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameter:\n\npoint_data: The key of the PointData object in the DataManager. This object contains the set of points from which the minimum distance to the line will be calculated at each timestamp.",
    "crumbs": [
      "Data Transformations",
      "Calculate Line to Point Distance"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/calculate_line_to_point_distance.html#example-configuration",
    "href": "user_guide/data_transformations/calculate_line_to_point_distance.html#example-configuration",
    "title": "Calculate Line to Point Distance",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example calculates the distance between a line stored with the key test_line and a set of points stored with the key test_points.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Line to Point Distance Pipeline\",\n            \"description\": \"Test line to point minimum distance calculation\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Calculate Line to Point Distance\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_line\",\n                \"output_key\": \"line_point_distances\",\n                \"parameters\": {\n                    \"point_data\": \"test_points\"\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Calculate Line to Point Distance"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/digital_interval_group.html",
    "href": "user_guide/data_transformations/digital_interval_group.html",
    "title": "Digital Interval Group",
    "section": "",
    "text": "This transform groups nearby intervals in a digital interval series based on a specified maximum spacing, which is useful for consolidating fragmented temporal events.\n\n\nThis transform merges digital intervals that are close to each other. If the gap between the end of one interval and the start of the next is less than or equal to the max_spacing value, the two intervals are combined into a single, larger interval. This process is repeated until no more intervals can be grouped.\nThis is particularly useful for cleaning up data where a single continuous event might be recorded as multiple, closely-spaced but separate intervals due to noise or transient interruptions in signal detection.\n\n\n\n\nIn neuroscience, this transform can be applied to refine event data:\n\nBout Analysis: When analyzing behaviors that occur in bouts (e.g., sniffing, grooming, vocalizations), this transform can merge closely spaced, individual instances into a single, continuous behavioral bout. For example, a series of short sniffs could be grouped into one “sniffing bout”.",
    "crumbs": [
      "Data Transformations",
      "Digital Interval Group"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/digital_interval_group.html#overview",
    "href": "user_guide/data_transformations/digital_interval_group.html#overview",
    "title": "Digital Interval Group",
    "section": "",
    "text": "This transform groups nearby intervals in a digital interval series based on a specified maximum spacing, which is useful for consolidating fragmented temporal events.\n\n\nThis transform merges digital intervals that are close to each other. If the gap between the end of one interval and the start of the next is less than or equal to the max_spacing value, the two intervals are combined into a single, larger interval. This process is repeated until no more intervals can be grouped.\nThis is particularly useful for cleaning up data where a single continuous event might be recorded as multiple, closely-spaced but separate intervals due to noise or transient interruptions in signal detection.\n\n\n\n\nIn neuroscience, this transform can be applied to refine event data:\n\nBout Analysis: When analyzing behaviors that occur in bouts (e.g., sniffing, grooming, vocalizations), this transform can merge closely spaced, individual instances into a single, continuous behavioral bout. For example, a series of short sniffs could be grouped into one “sniffing bout”.",
    "crumbs": [
      "Data Transformations",
      "Digital Interval Group"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/digital_interval_group.html#parameters",
    "href": "user_guide/data_transformations/digital_interval_group.html#parameters",
    "title": "Digital Interval Group",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameter:\n\nmax_spacing: The maximum allowed gap between two consecutive intervals for them to be merged. The time unit should be consistent with the data’s time representation (e.g., seconds or frames). If the gap is greater than this value, the intervals remain separate.",
    "crumbs": [
      "Data Transformations",
      "Digital Interval Group"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/digital_interval_group.html#example-configuration",
    "href": "user_guide/data_transformations/digital_interval_group.html#example-configuration",
    "title": "Digital Interval Group",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example groups intervals that are separated by 3.0 time units or less.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Interval Grouping Pipeline\",\n            \"description\": \"Test interval grouping on digital interval series\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Group Intervals\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_intervals\",\n                \"output_key\": \"grouped_intervals\",\n                \"parameters\": {\n                    \"max_spacing\": 3.0\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Digital Interval Group"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_point_from_line.html",
    "href": "user_guide/data_transformations/extract_point_from_line.html",
    "title": "Extract Point from Line",
    "section": "",
    "text": "This transform extracts a specific point from a line at a given fractional position. This is useful for isolating a single point of interest along a tracked object, such as the tip, base, or midpoint of a whisker or limb.\n\n\nThis operation pinpoints and extracts the coordinates of a single point along a line for each time step. The position of the point is determined by a fractional value from 0.0 (the start of the line) to 1.0 (the end of the line). Two different methods are available for extracting the point:\n\nDirect Method: This method calculates the cumulative length of the line’s segments and finds the point that lies at the specified fractional distance along this length. If use_interpolation is enabled, it will linearly interpolate between two adjacent vertices of the line to find the exact position. If disabled, it will select the nearest vertex.\nParametric Method: This method fits a polynomial to the line’s x and y coordinates separately as a function of the parametric distance along the line. It then evaluates the polynomial at the specified fractional position to determine the point’s coordinates. This can be useful for smoothing out irregularities in the line.\n\nThe output is a PointData object containing the extracted point for each time step.\n\n\n\n\nThis transformation is particularly useful for detailed analysis of movement and morphology:\n\nWhisker Analysis: Researchers can extract the tip (position 1.0), base (position 0.0), or any other consistent point along a tracked whisker. This allows for precise analysis of whisker contact, bending, or movement relative to other objects or whiskers.\nLimb Tracking: In studies of locomotion or reaching, this transform can isolate specific points on a limb, such as the endpoint (e.g., a hand or paw) or a joint (e.g., an elbow), to analyze trajectories and kinematics.\nMorphological Measurement: For organisms that change shape, like larvae or worms, this transform can be used to track specific points on the body midline to quantify bending or undulation.",
    "crumbs": [
      "Data Transformations",
      "Extract Point from Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_point_from_line.html#overview",
    "href": "user_guide/data_transformations/extract_point_from_line.html#overview",
    "title": "Extract Point from Line",
    "section": "",
    "text": "This transform extracts a specific point from a line at a given fractional position. This is useful for isolating a single point of interest along a tracked object, such as the tip, base, or midpoint of a whisker or limb.\n\n\nThis operation pinpoints and extracts the coordinates of a single point along a line for each time step. The position of the point is determined by a fractional value from 0.0 (the start of the line) to 1.0 (the end of the line). Two different methods are available for extracting the point:\n\nDirect Method: This method calculates the cumulative length of the line’s segments and finds the point that lies at the specified fractional distance along this length. If use_interpolation is enabled, it will linearly interpolate between two adjacent vertices of the line to find the exact position. If disabled, it will select the nearest vertex.\nParametric Method: This method fits a polynomial to the line’s x and y coordinates separately as a function of the parametric distance along the line. It then evaluates the polynomial at the specified fractional position to determine the point’s coordinates. This can be useful for smoothing out irregularities in the line.\n\nThe output is a PointData object containing the extracted point for each time step.\n\n\n\n\nThis transformation is particularly useful for detailed analysis of movement and morphology:\n\nWhisker Analysis: Researchers can extract the tip (position 1.0), base (position 0.0), or any other consistent point along a tracked whisker. This allows for precise analysis of whisker contact, bending, or movement relative to other objects or whiskers.\nLimb Tracking: In studies of locomotion or reaching, this transform can isolate specific points on a limb, such as the endpoint (e.g., a hand or paw) or a joint (e.g., an elbow), to analyze trajectories and kinematics.\nMorphological Measurement: For organisms that change shape, like larvae or worms, this transform can be used to track specific points on the body midline to quantify bending or undulation.",
    "crumbs": [
      "Data Transformations",
      "Extract Point from Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_point_from_line.html#parameters",
    "href": "user_guide/data_transformations/extract_point_from_line.html#parameters",
    "title": "Extract Point from Line",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nposition (float): The fractional distance along the line where the point should be extracted. Must be between 0.0 (start) and 1.0 (end).\nmethod (string): The method to use for point extraction. Can be either \"Direct\" or \"Parametric\".\npolynomial_order (integer, optional): The order of the polynomial to fit to the line if using the Parametric method. Defaults to 3.\nuse_interpolation (boolean, optional): Whether to use linear interpolation to find the exact point when using the Direct method. If false, the nearest vertex on the line is returned. Defaults to true.",
    "crumbs": [
      "Data Transformations",
      "Extract Point from Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/extract_point_from_line.html#example-configuration",
    "href": "user_guide/data_transformations/extract_point_from_line.html#example-configuration",
    "title": "Extract Point from Line",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example extracts the point at the 75% position along a line stored with the key whisker_1.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Whisker Tip Extraction Pipeline\",\n            \"description\": \"Extracts the tip of a whisker from line data.\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Extract Point from Line\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"whisker_1\",\n                \"output_key\": \"whisker_1_tip\",\n                \"parameters\": {\n                    \"position\": 0.75,\n                    \"method\": \"Direct\",\n                    \"use_interpolation\": true\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Extract Point from Line"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_angle.html",
    "href": "user_guide/data_transformations/line_angle.html",
    "title": "Line Angle",
    "section": "",
    "text": "This transform calculates the angle of a line at a specific position along its length for each point in time.\n\n\nThe Line Angle transform provides a way to quantify the orientation of a line in a 2D space. This can be particularly useful for analyzing the movement or orientation of objects that are tracked as lines. The angle is measured relative to a customizable reference vector, which defaults to the positive x-axis (0 degrees).\nThere are two methods for calculating the angle:\n\nDirect Points: This method approximates the tangent at a given position by calculating the angle of the vector between the point at that position and the point immediately preceding it. This is a simple and fast method suitable for getting a local orientation.\nPolynomial Fit: This method fits a polynomial of a given order to the line’s points (parameterized by their cumulative distance). The angle is then calculated from the derivative of the polynomial at the specified position. This method can provide a more accurate estimate of the tangent angle at a specific point along a curved line.\n\n\n\n\n\nIn neuroscience, this transform can be used to analyze a variety of data:\n\nWhisker Tracking: The angle of a tracked whisker relative to the animal’s head can be calculated to study sensory input and motor control.\nLimb Tracking: The angle of a limb segment (e.g., the forearm) can be calculated to analyze reaching movements or other motor behaviors.\nTongue Tracking: The angle of the tongue during licking or other oral movements can be quantified.",
    "crumbs": [
      "Data Transformations",
      "Line Angle"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_angle.html#overview",
    "href": "user_guide/data_transformations/line_angle.html#overview",
    "title": "Line Angle",
    "section": "",
    "text": "This transform calculates the angle of a line at a specific position along its length for each point in time.\n\n\nThe Line Angle transform provides a way to quantify the orientation of a line in a 2D space. This can be particularly useful for analyzing the movement or orientation of objects that are tracked as lines. The angle is measured relative to a customizable reference vector, which defaults to the positive x-axis (0 degrees).\nThere are two methods for calculating the angle:\n\nDirect Points: This method approximates the tangent at a given position by calculating the angle of the vector between the point at that position and the point immediately preceding it. This is a simple and fast method suitable for getting a local orientation.\nPolynomial Fit: This method fits a polynomial of a given order to the line’s points (parameterized by their cumulative distance). The angle is then calculated from the derivative of the polynomial at the specified position. This method can provide a more accurate estimate of the tangent angle at a specific point along a curved line.\n\n\n\n\n\nIn neuroscience, this transform can be used to analyze a variety of data:\n\nWhisker Tracking: The angle of a tracked whisker relative to the animal’s head can be calculated to study sensory input and motor control.\nLimb Tracking: The angle of a limb segment (e.g., the forearm) can be calculated to analyze reaching movements or other motor behaviors.\nTongue Tracking: The angle of the tongue during licking or other oral movements can be quantified.",
    "crumbs": [
      "Data Transformations",
      "Line Angle"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_angle.html#parameters",
    "href": "user_guide/data_transformations/line_angle.html#parameters",
    "title": "Line Angle",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nposition: A value between 0.0 and 1.0 that specifies the point along the line at which to calculate the angle. For example, 0.5 would be the midpoint of the line.\nmethod: The calculation method to use.\n\nDirect Points: (Default) Calculates the angle based on the vector from the start of the line to the point at position.\nPolynomial Fit: Fits a polynomial to the line and calculates the angle from the derivative.\n\npolynomial_order: The order of the polynomial to fit to the line when using the Polynomial Fit method. A higher order can capture more complex curves but requires more points and can be prone to overfitting. A typical value is 2 or 3.\nreference_x: The x-component of the reference vector. Defaults to 1.0.\nreference_y: The y-component of the reference vector. Defaults to 0.0. The default vector (1.0, 0.0) corresponds to the positive x-axis, meaning angles are measured with 0 degrees pointing to the right.",
    "crumbs": [
      "Data Transformations",
      "Line Angle"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_angle.html#example-configuration",
    "href": "user_guide/data_transformations/line_angle.html#example-configuration",
    "title": "Line Angle",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example calculates the angle at the midpoint of a line using the “Direct Points” method.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Line Angle Pipeline\",\n            \"description\": \"Test line angle calculation on line data\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Calculate Line Angle\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_line\",\n                \"output_key\": \"line_angles\",\n                \"parameters\": {\n                    \"position\": 0.5,\n                    \"method\": \"Direct Points\",\n                    \"polynomial_order\": 3,\n                    \"reference_x\": 1.0,\n                    \"reference_y\": 0.0\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Line Angle"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_curvature.html",
    "href": "user_guide/data_transformations/line_curvature.html",
    "title": "Line Curvature",
    "section": "",
    "text": "This transform calculates the curvature of a 2D line at a specified point. This is useful for analyzing the shape and tortuosity of lines, such as animal trajectories or neuronal processes.\n\n\nThe Line Curvature transform measures how much a line bends at a given point. It does this by fitting a parametric polynomial to the line’s (x, y) coordinates and then calculating the curvature of the fitted polynomial at a user-specified position along the line’s length.\nThe transform takes a LineData object (representing one or more lines over time) as input and produces an AnalogTimeSeries as output, where each value represents the calculated curvature at the corresponding time point.\nThis method allows for a smooth and robust estimation of curvature even when the underlying line data is noisy or unevenly sampled.\n\n\n\n\nIn neuroscience, analyzing the geometry of paths and structures is crucial for understanding behavior and neural architecture:\n\nAnimal Trajectory Analysis: The curvature of an animal’s path (e.g., a rat in a maze) can reveal changes in exploratory strategy, searching behavior, or the effects of neurological manipulations. High curvature might indicate turning or searching, while low curvature suggests straight-line travel.\nNeuronal Arborization: When analyzing the structure of neurons from microscope images, the curvature of dendrites or axons can be quantified to characterize their branching patterns and complexity. This is important for studying neuronal development, plasticity, and disease.\nAnalysis of Saccadic Eye Movements: The trajectory of saccadic eye movements is not perfectly straight. Analyzing the curvature of these paths can provide insights into the underlying motor control mechanisms.",
    "crumbs": [
      "Data Transformations",
      "Line Curvature"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_curvature.html#overview",
    "href": "user_guide/data_transformations/line_curvature.html#overview",
    "title": "Line Curvature",
    "section": "",
    "text": "This transform calculates the curvature of a 2D line at a specified point. This is useful for analyzing the shape and tortuosity of lines, such as animal trajectories or neuronal processes.\n\n\nThe Line Curvature transform measures how much a line bends at a given point. It does this by fitting a parametric polynomial to the line’s (x, y) coordinates and then calculating the curvature of the fitted polynomial at a user-specified position along the line’s length.\nThe transform takes a LineData object (representing one or more lines over time) as input and produces an AnalogTimeSeries as output, where each value represents the calculated curvature at the corresponding time point.\nThis method allows for a smooth and robust estimation of curvature even when the underlying line data is noisy or unevenly sampled.\n\n\n\n\nIn neuroscience, analyzing the geometry of paths and structures is crucial for understanding behavior and neural architecture:\n\nAnimal Trajectory Analysis: The curvature of an animal’s path (e.g., a rat in a maze) can reveal changes in exploratory strategy, searching behavior, or the effects of neurological manipulations. High curvature might indicate turning or searching, while low curvature suggests straight-line travel.\nNeuronal Arborization: When analyzing the structure of neurons from microscope images, the curvature of dendrites or axons can be quantified to characterize their branching patterns and complexity. This is important for studying neuronal development, plasticity, and disease.\nAnalysis of Saccadic Eye Movements: The trajectory of saccadic eye movements is not perfectly straight. Analyzing the curvature of these paths can provide insights into the underlying motor control mechanisms.",
    "crumbs": [
      "Data Transformations",
      "Line Curvature"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_curvature.html#parameters",
    "href": "user_guide/data_transformations/line_curvature.html#parameters",
    "title": "Line Curvature",
    "section": "Parameters",
    "text": "Parameters\nThis transform has the following parameters:\n\nposition: A fractional value between 0.0 and 1.0 indicating the point along the line’s total length at which to calculate the curvature. 0.0 is the start of the line, 0.5 is the midpoint, and 1.0 is the end.\nmethod: The algorithm to use for calculating curvature. Currently, only one option is available:\n\nPolynomialFit: Fits a parametric polynomial to the line data and calculates the analytical curvature.\n\npolynomial_order: The order of the polynomial to fit to the line data. A higher order can capture more complex curves but may be more sensitive to noise. A minimum of 2 is required for curvature calculation.\nfitting_window_percentage: The percentage (from 0.0 to 1.0) of the total line length to use for the polynomial fit, centered on the position parameter. A smaller window focuses the curvature calculation on the local shape, while a larger window provides a more global measure.",
    "crumbs": [
      "Data Transformations",
      "Line Curvature"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/line_curvature.html#example-configuration",
    "href": "user_guide/data_transformations/line_curvature.html#example-configuration",
    "title": "Line Curvature",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to run this transformation. This example calculates the curvature at the midpoint (0.5) of a line, using a 3rd-order polynomial fitted to 10% of the line’s data.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Line Curvature Pipeline\",\n            \"description\": \"Test line curvature calculation on a curved line\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Calculate Line Curvature\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"my_line_data\",\n                \"output_key\": \"calculated_curvature\",\n                \"parameters\": {\n                    \"position\": 0.5,\n                    \"method\": \"PolynomialFit\",\n                    \"polynomial_order\": 3,\n                    \"fitting_window_percentage\": 0.1\n                }\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Line Curvature"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_area.html",
    "href": "user_guide/data_transformations/mask_area.html",
    "title": "Calculate Mask Area",
    "section": "",
    "text": "This transform calculates the total area of all detected masks at each point in time, providing a measure of the overall size of masked regions.\n\n\nThis transformation processes a series of masks and, for each timestamp, calculates the total area covered by all masks at that specific time. The area is determined by summing the number of pixels (or voxels) that constitute each mask.\nThe result is an analog time series where the value at each point in time represents the total mask area for that moment. This is particularly useful for quantifying how the size of a region of interest (ROI) changes over time. Since this operation simply counts pixels within existing masks, it does not require any configuration parameters.\nThis transform takes a mask series as input and produces an analog time series as output.\n\n\n\n\nIn neuroscience research, quantifying the area of specific regions is a common requirement for analyzing imaging data:\n\nCell Swelling or Shrinking: When studying cellular dynamics, this transform can measure changes in the cross-sectional area of a cell over time, which might occur in response to a stimulus or pathological condition.\nPupilometry: In vision and cognitive neuroscience, tracking the area of the pupil provides insights into arousal, attention, and cognitive load. This transform can be applied to masks of the pupil from eye-tracking videos.\nLesion Sizing: In studies of brain injury or disease, this can be used to quantify the size of a lesion or a plaque from histological or in-vivo imaging data, tracking its progression over time.\nCalcium Imaging ROI Activity: For analyzing calcium imaging data, this transform can measure the area of a region of interest (ROI) that shows significant activity (e.g., above a certain brightness threshold), indicating the spatial extent of neural firing.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Area"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_area.html#overview",
    "href": "user_guide/data_transformations/mask_area.html#overview",
    "title": "Calculate Mask Area",
    "section": "",
    "text": "This transform calculates the total area of all detected masks at each point in time, providing a measure of the overall size of masked regions.\n\n\nThis transformation processes a series of masks and, for each timestamp, calculates the total area covered by all masks at that specific time. The area is determined by summing the number of pixels (or voxels) that constitute each mask.\nThe result is an analog time series where the value at each point in time represents the total mask area for that moment. This is particularly useful for quantifying how the size of a region of interest (ROI) changes over time. Since this operation simply counts pixels within existing masks, it does not require any configuration parameters.\nThis transform takes a mask series as input and produces an analog time series as output.\n\n\n\n\nIn neuroscience research, quantifying the area of specific regions is a common requirement for analyzing imaging data:\n\nCell Swelling or Shrinking: When studying cellular dynamics, this transform can measure changes in the cross-sectional area of a cell over time, which might occur in response to a stimulus or pathological condition.\nPupilometry: In vision and cognitive neuroscience, tracking the area of the pupil provides insights into arousal, attention, and cognitive load. This transform can be applied to masks of the pupil from eye-tracking videos.\nLesion Sizing: In studies of brain injury or disease, this can be used to quantify the size of a lesion or a plaque from histological or in-vivo imaging data, tracking its progression over time.\nCalcium Imaging ROI Activity: For analyzing calcium imaging data, this transform can measure the area of a region of interest (ROI) that shows significant activity (e.g., above a certain brightness threshold), indicating the spatial extent of neural firing.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Area"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_area.html#parameters",
    "href": "user_guide/data_transformations/mask_area.html#parameters",
    "title": "Calculate Mask Area",
    "section": "Parameters",
    "text": "Parameters\nThis transform does not have any parameters that need to be configured. It directly calculates the area from the input mask data.",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Area"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/mask_area.html#example-configuration",
    "href": "user_guide/data_transformations/mask_area.html#example-configuration",
    "title": "Calculate Mask Area",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to load mask data and run this transformation.\n[\n  {\n    \"transformations\": {\n      \"metadata\": {\n        \"name\": \"Mask Area Calculation Pipeline\",\n        \"description\": \"Test mask area calculation on mask data\",\n        \"version\": \"1.0\"\n      },\n      \"steps\": [\n        {\n          \"step_id\": \"1\",\n          \"transform_name\": \"Calculate Area\",\n          \"phase\": \"analysis\",\n          \"input_key\": \"test_mask_data\",\n          \"output_key\": \"calculated_areas\",\n          \"parameters\": {}\n        }\n      ]\n    }\n  }\n]",
    "crumbs": [
      "Data Transformations",
      "Calculate Mask Area"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/overview.html",
    "href": "user_guide/data_transformations/overview.html",
    "title": "Data Transformations",
    "section": "",
    "text": "Data transformations are operations we perform on one kind of data that result in another kind of data.\n\n\n\nFigure: 1) Feature select from available loaded data 2) Tranformation select Dialog box; 3) Output name of new data created by transformation; 4) Execution Button; 5) Parameter window for selected transformation",
    "crumbs": [
      "Data Transformations",
      "Data Transformations"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/skeletonize_mask.html",
    "href": "user_guide/data_transformations/skeletonize_mask.html",
    "title": "Skeletonize Mask",
    "section": "",
    "text": "This transform reduces a binary mask to a single-pixel-wide representation, preserving the essential structure of the shape.\n\n\nThis [Data Transform Operation] uses an algorithm to find the “skeleton” of a mask. The skeleton is a thinned version of the original shape that is equidistant from its boundaries. This process is useful for simplifying complex shapes into a more straightforward representation, like converting a blob into a line or a set of lines. This operation is parameter-free.\nThe skeletonization process is applied to each time frame of the mask data independently.\n\n\n\n\n\nDendritic and Axonal Tracing: In microscopy images of neurons, skeletonization can simplify the complex structures of dendrites and axons into simple lines. This allows for easier analysis of branching patterns, length measurements, and tracing of neural pathways.\nVessel Analysis: When analyzing blood vessels in the brain (angiography), skeletonization can reduce the vessels to their centerlines, which simplifies the measurement of vessel length, tortuosity, and branching angles.\nAnimal Tracking: When tracking the shape of an animal’s body or tail from video, skeletonization can provide a simplified representation that is easier to analyze for postural changes or movements over time.",
    "crumbs": [
      "Data Transformations",
      "Skeletonize Mask"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/skeletonize_mask.html#overview",
    "href": "user_guide/data_transformations/skeletonize_mask.html#overview",
    "title": "Skeletonize Mask",
    "section": "",
    "text": "This transform reduces a binary mask to a single-pixel-wide representation, preserving the essential structure of the shape.\n\n\nThis [Data Transform Operation] uses an algorithm to find the “skeleton” of a mask. The skeleton is a thinned version of the original shape that is equidistant from its boundaries. This process is useful for simplifying complex shapes into a more straightforward representation, like converting a blob into a line or a set of lines. This operation is parameter-free.\nThe skeletonization process is applied to each time frame of the mask data independently.\n\n\n\n\n\nDendritic and Axonal Tracing: In microscopy images of neurons, skeletonization can simplify the complex structures of dendrites and axons into simple lines. This allows for easier analysis of branching patterns, length measurements, and tracing of neural pathways.\nVessel Analysis: When analyzing blood vessels in the brain (angiography), skeletonization can reduce the vessels to their centerlines, which simplifies the measurement of vessel length, tortuosity, and branching angles.\nAnimal Tracking: When tracking the shape of an animal’s body or tail from video, skeletonization can provide a simplified representation that is easier to analyze for postural changes or movements over time.",
    "crumbs": [
      "Data Transformations",
      "Skeletonize Mask"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/skeletonize_mask.html#parameters",
    "href": "user_guide/data_transformations/skeletonize_mask.html#parameters",
    "title": "Skeletonize Mask",
    "section": "Parameters",
    "text": "Parameters\nThis transform does not have any parameters. The underlying skeletonization algorithm is applied with default settings.",
    "crumbs": [
      "Data Transformations",
      "Skeletonize Mask"
    ]
  },
  {
    "objectID": "user_guide/data_transformations/skeletonize_mask.html#example-configuration",
    "href": "user_guide/data_transformations/skeletonize_mask.html#example-configuration",
    "title": "Skeletonize Mask",
    "section": "Example Configuration",
    "text": "Example Configuration\nHere is a complete example of a JSON configuration file that could be used to load mask data and then apply the skeletonization transform.\n[\n{\n    \"transformations\": {\n        \"metadata\": {\n            \"name\": \"Mask Skeletonization Pipeline\",\n            \"description\": \"Test mask skeletonization on rectangular mask\",\n            \"version\": \"1.0\"\n        },\n        \"steps\": [\n            {\n                \"step_id\": \"1\",\n                \"transform_name\": \"Skeletonize Mask\",\n                \"phase\": \"analysis\",\n                \"input_key\": \"test_mask\",\n                \"output_key\": \"skeletonized_mask\",\n                \"parameters\": {}\n            }\n        ]\n    }\n}\n]",
    "crumbs": [
      "Data Transformations",
      "Skeletonize Mask"
    ]
  },
  {
    "objectID": "user_guide/machine_learning/ML_intro.html",
    "href": "user_guide/machine_learning/ML_intro.html",
    "title": "Overview",
    "section": "",
    "text": "The Machine Learning Widget allows us to fit models of relationships between multiple features of our data and then make predictions. This can be useful for semi-automated annotation of datasets, where the user labels some training data, and then predicts the remaining unlabeled frames. Then annotations can then be easily compared with the video data.",
    "crumbs": [
      "Machine Learning",
      "Overview"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/overview.html",
    "href": "user_guide/media_viewer/overview.html",
    "title": "Media Viewer Overview",
    "section": "",
    "text": "The Media Viewer Widget provides a unified display for neuroscience videos or images.\nIt is the central place where behavioral, microscopic, and event data can be inspected visually, and in addition, you can also process video or images as well as manipulate or modify extracted features such as mask or keypoint data.\nIn essence, the widget is designed to:",
    "crumbs": [
      "Media Viewer",
      "Media Viewer Overview"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/overview.html#main-components",
    "href": "user_guide/media_viewer/overview.html#main-components",
    "title": "Media Viewer Overview",
    "section": "Main Components",
    "text": "Main Components\n\nMedia Window\nThis is the main display area. All visual data and overlays will appear here.\nFeature Table\nA table showing all available data streams. Each entry lists its name, type, and visibility.\nVisibility can be toggled on or off.\nSubwidgets\nSpecialized panels for interacting with particular types of data. Only the relevant panel is active at a time, when selected.",
    "crumbs": [
      "Media Viewer",
      "Media Viewer Overview"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/overview.html#supported-data-types",
    "href": "user_guide/media_viewer/overview.html#supported-data-types",
    "title": "Media Viewer Overview",
    "section": "Supported Data Types",
    "text": "Supported Data Types\nThe widget supports multiple kinds of data. Each type has its own controls and visualization style.\n\n\n\n\n\n\n\nData Type\nDescription\n\n\n\n\nKeypoints (Points)\nIndividual locations, e.g. positions of body parts.\n\n\nLines\nLinear features, e.g. traces of movement or signals.\n\n\nMasks\nRegions of interest in an image or video frame.\n\n\nDigital Intervals\nTime periods when a signal or event is active.\n\n\nTensors\nMulti-dimensional data, often from machine learning.\n\n\nMedia (Video/Images)\nRaw recordings or still frames.\n\n\nText Overlays\nLabels, annotations, or other text displayed on media.",
    "crumbs": [
      "Media Viewer",
      "Media Viewer Overview"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/overview.html#feature-table",
    "href": "user_guide/media_viewer/overview.html#feature-table",
    "title": "Media Viewer Overview",
    "section": "Feature Table",
    "text": "Feature Table\nThe feature table lists all available data sources.\n\nFeature — the name of the data stream.\n\nType — the category of the data (e.g. Mask, Point, Line).\n\nEnabled — whether the data is visible in the media window.\n\nEnabling a feature makes it visible in the media window; disabling it hides it.",
    "crumbs": [
      "Media Viewer",
      "Media Viewer Overview"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/overview.html#subwidgets",
    "href": "user_guide/media_viewer/overview.html#subwidgets",
    "title": "Media Viewer Overview",
    "section": "Subwidgets",
    "text": "Subwidgets\nEach data type provides its own subwidget.\n\nPoint Widget — shows and manages keypoints.\n\nLine Widget — displays traces or line data.\n\nMask Widget — handles binary masks or segmented regions.\n\nInterval Widget — manages time-based event data.\n\nTensor Widget — provides tools for visualizing complex arrays.\n\nText Widget — adds and edits text overlays.\n\nProcessing Widget — applies filters or transformations to media.",
    "crumbs": [
      "Media Viewer",
      "Media Viewer Overview"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/process.html",
    "href": "user_guide/media_viewer/process.html",
    "title": "Media Processing",
    "section": "",
    "text": "The Media Processing Widget provides a collection of tools to adjust, enhance, and transform media data (images or video) within the viewer.\nEach processing option can be enabled by checking the Active box, and its effect will be applied directly in the media viewer.\nThis guide introduces each option, its background, and how to use each effectively.\nWhen media is selected in the feature table within the media widget, you will be presented with the following actions:",
    "crumbs": [
      "Media Viewer",
      "Media Processing"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/process.html#linear-transform",
    "href": "user_guide/media_viewer/process.html#linear-transform",
    "title": "Media Processing",
    "section": "Linear Transform",
    "text": "Linear Transform\nA linear transform modifies pixel intensity values by applying a simple linear function:\n\\[\nI_{out}(x, y) = \\alpha \\cdot I_{in}(x, y) + \\beta\n\\]\n\nAlpha (Contrast): Controls contrast. Values &gt;1 increase contrast, values between 0 and 1 decrease contrast.\n\nBeta (Brightness): Adjusts brightness by shifting all pixel values up or down.\n\nMinimum / Maximum: Sets intensity clipping or scaling boundaries. Pixels outside this range are clamped. This is often used for rescaling pixel intensities into a normalized display range.\n\nUse case: Brightening dark images, increasing contrast, or standardizing pixel intensity ranges.",
    "crumbs": [
      "Media Viewer",
      "Media Processing"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/process.html#gamma-correction",
    "href": "user_guide/media_viewer/process.html#gamma-correction",
    "title": "Media Processing",
    "section": "Gamma Correction",
    "text": "Gamma Correction\nGamma correction is a nonlinear intensity adjustment:\n\\[\nI_{out}(x, y) = I_{in}(x, y)^\\gamma\n\\]\n\nGamma: The power-law exponent.\n\nγ &lt; 1: Brightens images (boosts darker regions).\n\nγ &gt; 1: Darkens images (suppresses bright regions).\n\n\nThis compensates for human visual perception, which is more sensitive to relative changes in darker tones.\nUse case: Making hidden details in dark or bright regions more visible.",
    "crumbs": [
      "Media Viewer",
      "Media Processing"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/process.html#image-sharpening",
    "href": "user_guide/media_viewer/process.html#image-sharpening",
    "title": "Media Processing",
    "section": "Image Sharpening",
    "text": "Image Sharpening\nSharpening highlights edges by enhancing high-frequency details. This method uses an unsharp mask:\n\\[\nI_{out} = I_{in} + \\lambda \\cdot (I_{in} - G_\\sigma(I_{in}))\n\\]\nWhere \\(G_\\sigma\\) is a Gaussian blur applied with standard deviation σ.\n\nSigma: Controls the blur radius used before sharpening. Small values sharpen fine details; larger values enhance broader structures.\n\nUse case: Highlighting boundaries, edges, and fine structures in images.",
    "crumbs": [
      "Media Viewer",
      "Media Processing"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/process.html#clahe-contrast-limited-adaptive-histogram-equalization",
    "href": "user_guide/media_viewer/process.html#clahe-contrast-limited-adaptive-histogram-equalization",
    "title": "Media Processing",
    "section": "CLAHE (Contrast-Limited Adaptive Histogram Equalization)",
    "text": "CLAHE (Contrast-Limited Adaptive Histogram Equalization)\nCLAHE improves local contrast by redistributing pixel intensities based on local histograms. Unlike global histogram equalization, CLAHE works in small tiles (grid regions).\n\nClip Limit: Prevents noise amplification by limiting the contrast enhancement.\n\nGrid Size: Defines the size of the local regions used for histogram equalization.\n\nUse case: Bringing out local details in images with varying illumination, e.g., microscopy or medical images.",
    "crumbs": [
      "Media Viewer",
      "Media Processing"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/process.html#bilateral-filter",
    "href": "user_guide/media_viewer/process.html#bilateral-filter",
    "title": "Media Processing",
    "section": "Bilateral Filter",
    "text": "Bilateral Filter\nThe bilateral filter smooths images while preserving edges. It considers both spatial proximity and intensity similarity:\n\\[\nI_{filtered}(x) = \\frac{1}{W_p} \\sum_{x_i \\epsilon \\Omega} I(X_i) \\cdot f_r(||I(x_i)-I(x)||) \\cdot g_s(||x_i - x||)\n\\]\n\nDiameter: Size of the filter kernel (neighborhood).\n\nColor Sigma (σr): How much intensity differences are preserved; large values allow smoothing across intensity changes, small values preserve edges.\n\nSpace Sigma (σs): How far pixels influence each other in space.\n\nUse case: Noise reduction while keeping edges sharp (useful for video or structural microscopy data).",
    "crumbs": [
      "Media Viewer",
      "Media Processing"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/process.html#median-filter",
    "href": "user_guide/media_viewer/process.html#median-filter",
    "title": "Media Processing",
    "section": "Median Filter",
    "text": "Median Filter\nThe median filter replaces each pixel with the median value of its neighborhood.\n\nKernel Size: Defines the neighborhood size. Larger kernels smooth more but may lose detail.\n\nUse case: Removing salt-and-pepper noise or impulse noise.",
    "crumbs": [
      "Media Viewer",
      "Media Processing"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/process.html#magic-eraser",
    "href": "user_guide/media_viewer/process.html#magic-eraser",
    "title": "Media Processing",
    "section": "Magic Eraser",
    "text": "Magic Eraser\nThe magic eraser provides an interactive editing tool for masking unwanted regions.\n\nBrush Size: Size of the drawing tool when marking regions.\n\nMedian Filter Size: Strength of smoothing applied to the mask.\n\nStart Drawing Button: Enables drawing mode inside the media viewer.\n\nClear Mask Button: Resets the mask.\n\nUse case: Manually erasing artifacts, covering distracting elements, or excluding regions from processing.",
    "crumbs": [
      "Media Viewer",
      "Media Processing"
    ]
  },
  {
    "objectID": "user_guide/media_viewer/process.html#colormap",
    "href": "user_guide/media_viewer/process.html#colormap",
    "title": "Media Processing",
    "section": "Colormap",
    "text": "Colormap\nA colormap maps grayscale intensity values to colors using a lookup table (LUT). This improves visibility and interpretation of data.\n\nLookup Table (LUT): Defines the mapping (e.g., Jet, Hot, Cool). Each intensity is assigned a unique color.\n\nAlpha (Blend): Controls transparency of the colormap overlay (0 = fully transparent, 1 = fully opaque).\n\nNormalize Checkbox: Rescales intensity values into a normalized range before applying the colormap, ensuring full use of the LUT range.\n\nUse case: Enhancing interpretability of microscopy, behavior, or heatmap-like data.",
    "crumbs": [
      "Media Viewer",
      "Media Processing"
    ]
  }
]
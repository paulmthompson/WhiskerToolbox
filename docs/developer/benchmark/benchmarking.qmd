---
title: "Profiling and Performance"
format: html
---

This document describes the benchmarking infrastructure for Neuralyzer, including how to create benchmarks, run them, and analyze performance using various profiling tools.

## Quick Start

``` bash
# Build with benchmarks enabled (they're on by default)
cmake --preset linux-clang-release -DENABLE_BENCHMARK=ON
cmake --build --preset linux-clang-release

# Run all benchmarks
cd out/build/Clang/Release/benchmark
./benchmark_MaskArea

# Run specific benchmarks with filtering
./benchmark_MaskArea --benchmark_filter=Pipeline

# Run with different output formats
./benchmark_MaskArea --benchmark_format=json > results.json
./benchmark_MaskArea --benchmark_format=csv > results.csv
```

## Architecture

### CMake Infrastructure

The benchmarking system is built on `cmake/BenchmarkUtils.cmake`, which provides:

-   `add_selective_benchmark()` - Create individual benchmark executables
-   `configure_benchmark_for_profiling()` - Add profiling tool support
-   `print_benchmark_summary()` - Display configuration summary

Each benchmark can be individually enabled/disabled via CMake options:

``` bash
# Disable a specific benchmark
cmake -DBENCHMARK_MASK_AREA=OFF ..

# Only build specific benchmarks
cmake -DBENCHMARK_MASK_AREA=ON -DBENCHMARK_OTHER=OFF ..
```

### Test Data Fixtures

`benchmark/fixtures/BenchmarkFixtures.hpp` provides fixtures for generating realistic test data:

-   `MaskDataFixture` - Generate MaskData with configurable properties
-   `LineDataFixture` - Generate LineData for curve/line benchmarks
-   `PointDataFixture` - Generate PointData for point-based benchmarks

Presets are available for common scenarios: - `Presets::SmallMaskData()` - Quick iteration (10 frames) - `Presets::MediumMaskData()` - Realistic testing (100 frames) - `Presets::LargeMaskData()` - Stress testing (1000 frames) - `Presets::SparseMaskData()` - Few masks, large time gaps - `Presets::DenseMaskData()` - Many small masks per frame

Example usage:

``` cpp
#include "fixtures/BenchmarkFixtures.hpp"

BENCHMARK_F(MyBenchmark, TestCase)(benchmark::State& state) {
    auto fixture = MaskDataFixture(Presets::MediumMaskData());
    auto mask_data = fixture.generate();
    
    for (auto _ : state) {
        // Benchmark code here
    }
}
```

## Creating New Benchmarks

### 1. Create Benchmark Source File

Create `benchmark/MyFeature.benchmark.cpp`:

``` cpp
#include "fixtures/BenchmarkFixtures.hpp"
#include <benchmark/benchmark.h>

// Simple function benchmark
static void BM_MyFunction(benchmark::State& state) {
    // Setup
    auto data = setupTestData();
    
    for (auto _ : state) {
        auto result = myFunction(data);
        benchmark::DoNotOptimize(result);
    }
}
BENCHMARK(BM_MyFunction);

// Fixture-based benchmark
class MyFeatureBenchmark : public benchmark::Fixture {
public:
    void SetUp(benchmark::State const& state) override {
        // Generate test data once
        test_data_ = generateData();
    }
    
protected:
    std::shared_ptr<DataType> test_data_;
};

BENCHMARK_F(MyFeatureBenchmark, TestCase)(benchmark::State& state) {
    for (auto _ : state) {
        auto result = process(test_data_);
        benchmark::DoNotOptimize(result);
    }
}

BENCHMARK_MAIN();
```

### 2. Register in CMake

Add to `benchmark/CMakeLists.txt`:

``` cmake
add_selective_benchmark(
    NAME MyFeature
    SOURCES 
        MyFeature.benchmark.cpp
    LINK_LIBRARIES 
        DataManager
        MyFeatureLib
    DEFAULT ON
)

# Optional: Enable profiling support
if(TARGET benchmark_MyFeature)
    configure_benchmark_for_profiling(
        TARGET benchmark_MyFeature
        ENABLE_PERF ON
        ENABLE_HEAPTRACK ON
        GENERATE_ASM ON
    )
endif()
```

### 3. Build and Run

``` bash
cmake --build --preset linux-clang-release
./out/build/Clang/Release/benchmark/benchmark_MyFeature
```

## Google Benchmark Features

### Parameterized Benchmarks

Test with different input sizes:

``` cpp
BENCHMARK(BM_MyFunction)
    ->Arg(100)
    ->Arg(1000)
    ->Arg(10000)
    ->Unit(benchmark::kMillisecond);

// Or use ranges
BENCHMARK(BM_MyFunction)
    ->Range(8, 8<<10)  // 8 to 8192, powers of 2
    ->RangeMultiplier(2);
```

### Fixtures with Parameters

``` cpp
BENCHMARK_F(MyBenchmark, TestCase)(benchmark::State& state) {
    size_t size = state.range(0);
    // Use size parameter
}
BENCHMARK_REGISTER_F(MyBenchmark, TestCase)
    ->DenseRange(0, 4)  // Parameters 0, 1, 2, 3, 4
    ->Unit(benchmark::kMicrosecond);
```

### Custom Counters

Track additional metrics:

``` cpp
for (auto _ : state) {
    auto result = myFunction(data);
    state.counters["items_processed"] = data.size();
    state.counters["bytes_processed"] = data.size() * sizeof(Item);
}

state.SetItemsProcessed(state.iterations() * data.size());
state.SetBytesProcessed(state.iterations() * data.size() * sizeof(Item));
```

## Performance Analysis Tools

### 1. Perf (CPU Profiling)

Profile CPU usage and generate call graphs:

``` bash
# Record profile data
perf record -g ./benchmark_MaskArea --benchmark_filter=Pipeline

# View interactive report
perf report

# Generate flamegraph
perf script | stackcollapse-perf.pl | flamegraph.pl > flame.svg

# View specific functions
perf annotate functionName

# Show hot spots
perf top
```

Key perf options: - `-g` - Enable call-graph (stack trace) recording - `-e cycles` - Profile CPU cycles - `-e cache-misses` - Profile cache misses - `--call-graph dwarf` - Use DWARF for more accurate call graphs

### 2. Heaptrack (Memory Profiling)

Analyze memory allocation patterns:

``` bash
# Run with heaptrack
heaptrack ./benchmark_MaskArea

# View results in GUI
heaptrack_gui heaptrack.benchmark_MaskArea.*.gz

# View results in terminal
heaptrack_print heaptrack.benchmark_MaskArea.*.gz
```

What heaptrack shows: - Total memory allocated - Peak memory usage - Number of allocations - Call stacks for allocations - Memory leaks - Temporary allocations

### 3. Valgrind Cachegrind (Cache Analysis)

Analyze cache behavior:

``` bash
# Run cachegrind
valgrind --tool=cachegrind ./benchmark_MaskArea --benchmark_filter=CacheBehavior

# Visualize results
cg_annotate cachegrind.out.<pid>

# Interactive visualization
kcachegrind cachegrind.out.<pid>
```

Metrics provided: - L1/L2/L3 cache hits/misses - Instruction cache behavior - Data cache behavior - Branch prediction statistics

### 4. Assembly Inspection

View generated assembly for optimization:

``` bash
# Method 1: objdump
objdump -d -C -S ./benchmark_MaskArea | less

# Search for specific function
objdump -d -C -S ./benchmark_MaskArea | grep -A 50 "calculateMaskArea"

# Method 2: During compilation (if GENERATE_ASM=ON)
# Assembly files (.s) generated alongside object files
find . -name "*.s" | xargs less
```

What to look for: - Loop vectorization (SIMD instructions) - Unnecessary branches - Memory access patterns - Inlining decisions - Register usage

### 5. Time Command (Quick Stats)

Get quick overview of resource usage:

``` bash
/usr/bin/time -v ./benchmark_MaskArea

# Key metrics:
# - Maximum resident set size (peak memory)
# - Page faults (major = disk I/O, minor = memory)
# - Context switches
# - CPU percentage
```

### 6. Clang Build Time Analysis

If built with `-DENABLE_TIME_TRACE=ON`:

``` bash
# View compilation time breakdown
ClangBuildAnalyzer --all build_trace build_results.bin
ClangBuildAnalyzer --analyze build_results.bin
```

## AnalogTimeSeries Loading Performance

We benchmarked the loading of binary data into `AnalogTimeSeries` to identify performance bottlenecks.
The benchmark compares:
1.  **Best Case**: Reading raw bytes into `std::vector<uint8>` (simulating disk I/O limit).
2.  **Single Channel**: Loading `int16` data into one `AnalogTimeSeries` (single channel), using an optimized chunked reader with direct conversion.
3.  **Multi Channel**: Loading `int16` data into 32 `AnalogTimeSeries` (multi-channel interleaved).

### Results (10MB Data)

| Scenario | Time | Speed | Slowdown vs Best Case |
| :--- | :--- | :--- | :--- |
| **Best Case (Raw Read)** | ~2.5 ms | ~4.0 GB/s | 1.0x |
| **Multi Channel (32 ch)** | ~18 ms | ~550 MB/s | ~7.2x |
| **Single Channel (Optimized)** | ~62.5 ms | ~160 MB/s | ~25x |

### Analysis

*   **Optimization Impact**: The optimized single-channel loader (using chunked reads and direct `int16`->`float` conversion) improved performance by approximately **12%** compared to the baseline implementation (~70ms).
*   **Direct Conversion**: The optimized loader avoids creating an intermediate `std::vector<int16>`, reducing peak memory usage and memory bandwidth requirements.
*   **Multi-Channel Efficiency**: Multi-channel loading remains significantly faster (~3.5x faster than single-channel). This is likely due to:
    *   **Cache Locality**: Processing 32 smaller vectors (per channel) may fit better in CPU caches than processing one very large vector.
    *   **Vector Initialization**: Initializing one massive 20MB float vector (single channel) vs 32 smaller ones might have different performance characteristics depending on the allocator and OS page fault handling.

### Future Improvements

*   **Parallelization**: The `int16` to `float` conversion is a prime candidate for parallelization using OpenMP or `std::execution::par`, which could bridge the gap between single-channel loading and raw I/O.
*   **Vector Initialization**: Using `std::make_unique_for_overwrite` (or equivalent techniques) to avoid zero-initialization of the target `std::vector<float>` could save significant time, as writing 0s to 20MB of memory is not negligible.

---
title: "DigitalEventSeries View Storage Performance Analysis"
format: html
---

This document describes the performance analysis of using `DigitalEventSeries` views vs. raw vectors for raster plot generation, a common operation in neural data visualization.

## Background

A raster plot displays events (e.g., neural spikes) aligned to trial events (e.g., stimulus onsets). For each alignment event, we need to:

1. Extract events within a time window around the alignment point
2. Transform the data for GPU rendering (populate vertex buffers)

We compared three approaches:

1. **Baseline (Raw Vectors)**: Use `std::vector<std::vector<TimeFrameIndex>>` for windowed events
2. **View-Based (Manual)**: Use `std::vector<std::shared_ptr<DigitalEventSeries>>` with view storage, creating views in a manual loop
3. **Gather-Based**: Use `GatherResult<DigitalEventSeries>` via the `gather()` utility function

## Benchmark Configuration

```
Raster events:     100,000 (total events to filter)
Alignment events:    1,000 (number of trials/windows)
Window size:         1,000 (±500 around each alignment)
Expected events/window: ~100 (based on density)
```

## Results Summary

### Full Pipeline Performance

| Approach | Time (µs) | Relative |
|----------|----------|----------|
| Baseline (Raw Vectors) | 368 | 1.0x |
| View-Based (Manual) | 1,229 | 3.3x slower |
| Gather-Based | 1,229 | 3.3x slower |

The `gather()` utility matches the performance of manual view creation while providing a cleaner API.

### Phase Breakdown

| Phase | Baseline (µs) | View-Based (µs) | Gather (µs) | Ratio (View/Baseline) |
|-------|---------------|-----------------|-------------|----------------------|
| Window/View Creation | 182 | 1,096 | 984 | 5.4x |
| Buffer Population | 175 | 215 | 215 | 1.2x |
| Iteration Only | 17.5 | 29.9 | 27.2 | 1.6x |

**Notable**: `gather()` is ~10% faster than manual view creation due to better reservation and single-pass interval iteration.

### Transform Helper Performance

The `GatherResult::transform()` method enables efficient per-trial computations:

| Operation | Time (µs) |
|-----------|-----------|
| `gathered.transform([](v) { return v->size(); })` | 4.17 |

This is very fast for computing statistics like spike counts across 1,000 trials.

## Architecture: Zero-Copy View Storage

### How It Works

The `DigitalEventStorageWrapper` uses a type-erased `shared_ptr` design that enables true zero-copy view creation:

```cpp
class DigitalEventStorageWrapper {
    std::shared_ptr<StorageConcept> _impl;  // Shared ownership of storage
};
```

When creating a view, we use the **aliasing constructor** of `shared_ptr`:

```cpp
std::shared_ptr<OwningDigitalEventStorage const> getSharedOwningStorage() const {
    if (auto owning_model = std::dynamic_pointer_cast<StorageModel<OwningDigitalEventStorage> const>(_impl)) {
        // Aliasing constructor: shares ownership with _impl but points to inner storage
        return std::shared_ptr<OwningDigitalEventStorage const>(owning_model, &owning_model->_storage);
    }
    return nullptr;
}
```

This allows:

- **Zero data copying**: The returned `shared_ptr` points directly into the existing storage
- **Correct lifetime management**: The source storage stays alive as long as any view exists
- **Shared references**: Multiple views from the same source share one underlying storage

### View Storage Structure

Each view consists of:

1. A `shared_ptr` to the source's owning storage (shared among all views)
2. A vector of indices specifying which events are visible
3. Time range bounds for filtering

```
Source (100K events)
    └── _storage (DigitalEventStorageWrapper)
            └── _impl (shared_ptr<StorageConcept>)
                    └── OwningDigitalEventStorage with actual data

View 1 (window around alignment 0)      ──┐
View 2 (window around alignment 1)      ──┼── All share same source via shared_ptr
...                                       │
View 1000 (window around alignment 999) ──┘
```

## Performance Characteristics

### View Creation Overhead (5.6x)

Creating 1,000 views takes ~1ms. This overhead comes from:

| Factor | Cost |
|--------|------|
| `shared_ptr` allocation per view | ~400ns each |
| `ViewDigitalEventStorage` setup | Index vector allocation |
| `filterByTimeRange()` | Binary search + index copy |
| Type erasure wrapper construction | `StorageModel` allocation |

### Iteration Overhead (1.6x)

Once views are created, iterating through them is reasonably efficient:

| Factor | Baseline | View-Based | Impact |
|--------|----------|------------|--------|
| **Access Pattern** | `events[i]` | `source->getEvent(indices[i])` | Double indirection |
| **Data Layout** | Contiguous | Index + data arrays | Reduced cache locality |
| **Virtual Dispatch** | None | Through type erasure | Minimal (~1-2ns/call) |

The 1.6x iteration overhead is acceptable for most use cases.

## Scalability Analysis

| Alignments | Baseline (µs) | View-Based (µs) | Gather (µs) | Ratio (View) | Ratio (Gather) |
|------------|---------------|-----------------|-------------|--------------|----------------|
| 100 | 31 | 110 | 120 | 3.5x | 3.9x |
| 500 | 186 | 575 | 588 | 3.1x | 3.2x |
| 1,000 | 367 | 1,204 | 1,169 | 3.3x | 3.2x |
| 2,000 | 720 | 2,530 | 2,491 | 3.5x | 3.5x |
| 5,000 | 1,926 | 7,105 | 6,573 | 3.7x | 3.4x |

Both view-based approaches scale linearly with alignment count. At higher alignment counts (5000), `gather()` is ~8% faster than manual view creation.

## Historical Note: The Copy Bug

An earlier implementation had a critical bug causing ~5000x slowdown:

```cpp
// BUG: This copied the entire storage for every view!
auto view_storage = ViewDigitalEventStorage{
    std::make_shared<OwningDigitalEventStorage const>(*src_owning)  // COPIES!
};
```

With 1,000 views of 100K events, this meant 100 million unnecessary event copies. The fix was changing `DigitalEventStorageWrapper` from `unique_ptr` to `shared_ptr` with aliasing constructor support.

## When to Use Each Approach

### Raw Vectors

**Best for:**

- High-frequency rendering loops (60+ FPS animation)
- Simple iteration without entity tracking
- When data is already filtered at load time
- Performance-critical hot paths

### gather() with GatherResult

**Best for:**

- Trial-aligned analysis (raster plots, PSTHs, etc.)
- When you need source tracking and interval metadata
- Applying functions across all trials via `transform()`
- Clean, self-documenting code with no performance penalty vs. manual views

```cpp
// Example: Raster plot with gather()
auto spikes = dm->getData<DigitalEventSeries>("spikes");
auto trials = dm->getData<DigitalIntervalSeries>("trials");

auto raster = gather(spikes, trials);

// Compute spike counts per trial
auto counts = raster.transform([](auto const& trial) {
    return trial->size();
});

// Access interval metadata
for (size_t i = 0; i < raster.size(); ++i) {
    Interval interval = raster.intervalAt(i);
    // Use raster[i] for the view...
}
```

### Manual View Creation

**Best for:**

- Custom filtering logic not supported by gather()
- When you need fine-grained control over view creation
- Integration with existing code that expects `std::vector<shared_ptr<T>>`

## Alternative Approaches

For specific use cases, consider:

1. **Span-based views**: For contiguous time ranges, return `std::span<TimeFrameIndex>` directly
2. **Index pairs**: For windowed queries, return `(start_idx, end_idx)` into source
3. **Materialized views**: For repeated iteration, call `materialize()` once and reuse

## Running the Benchmark

```bash
# Build
cmake --preset linux-clang-release -DENABLE_BENCHMARK=ON
cmake --build --preset linux-clang-release --target benchmark_RasterPlotViews

# Run all tests
./out/build/Clang/Release/benchmark/benchmark_RasterPlotViews

# Run specific tests
./benchmark_RasterPlotViews --benchmark_filter="FullPipeline"
./benchmark_RasterPlotViews --benchmark_filter="IterationOnly"
./benchmark_RasterPlotViews --benchmark_filter="Gather"
./benchmark_RasterPlotViews --benchmark_filter="ScaleAlignments"

# Cache analysis
perf stat -e cache-references,cache-misses,L1-dcache-load-misses \
    ./benchmark_RasterPlotViews --benchmark_filter="IterationOnly"
```

## Conclusion

The view-based approaches using `DigitalEventSeries` achieve true zero-copy storage sharing via `shared_ptr` with aliasing constructor. The overall overhead is ~3.3x compared to raw vectors, primarily from view object allocation (~1ms for 1000 views) and index indirection during iteration (~1.6x).

**Key findings:**

1. **`gather()` matches or slightly beats manual view creation** - The ~10% improvement at scale comes from better reservation and single-pass interval iteration.

2. **Both view approaches are ~3.3x slower than raw vectors** - This is the expected cost of abstraction (shared_ptr allocations, type erasure, indirection).

3. **`gather()` provides cleaner API with no performance penalty** - You get ergonomic benefits (source tracking, `transform()`, `intervalAt()`) for free.

4. **The `transform()` helper is very fast** - 4µs to compute statistics across 1000 trials.

For most interactive visualization and analysis use cases, the view-based overhead is acceptable. The `gather()` utility is recommended for trial-aligned operations as it provides the cleanest API. For performance-critical rendering loops requiring maximum throughput, raw vectors remain the better choice.

## Time Normalization with Transform Pipeline

Building on the gather/view infrastructure, we evaluated the performance of applying value projections (time normalization) to trial-aligned data. This is essential for raster plots and other trial-aligned visualizations where events need to be normalized relative to an alignment point (t=0 at trial start).

### Normalization Benchmark Setup

Configuration remains the same:
- 100,000 total events
- 1,000 alignment events (trials)
- Window size: ±500 around each alignment

Three normalization approaches tested:

1. **Baseline + Normalization**: Extract events and normalize times in a single pass (manual)
2. **Gather + Value Projection**: Use `GatherResult` with per-trial value projections via `NormalizeTimeValue` transform
3. **Isolated Normalization**: Just the normalization step without buffer population

### Full Pipeline Results (With Normalization)

| Approach | Time (µs) | Relative to Baseline |
|----------|----------|---------------------|
| Baseline (No Normalization) | 367 | 1.0x |
| Baseline + Normalization | 598 | 1.6x |
| Gather (No Normalization) | 1,223 | 3.3x |
| Gather + Normalization | 1,424 | 3.9x |

**Key Insight**: Adding normalization costs only ~63% more for baseline (231µs), but ~17% more for gather (201µs). The gather approach absorbs normalization overhead more efficiently due to better amortization across the view creation cost.

### Isolated Normalization Phase Analysis

Measuring just the normalization step (without buffer population):

| Approach | Time (µs) | Description |
|----------|----------|-------------|
| Baseline Normalization | 429 | Extract windows + normalize (1000 trials × ~100 events) |
| Gather + Projection | 1,484 | Create gather result + build projections + iterate with projection |
| Difference | +1,055 | Gather overhead for normalization |
| Overhead Factor | 3.5x | Gather normalization is 3.5x slower |

### Phase Breakdown: Where Does the Overhead Come From?

Breaking down the Gather + Normalization pipeline:

| Phase | Time (µs) | Notes |
|-------|----------|-------|
| Create gather result | ~240 | View creation (1000 views) |
| Build projections | ~50 | Create projection factory for each trial |
| Apply projections | ~1,194 | Iterate views + project each event time |
| **Total** | **1,484** | |

For comparison, Baseline Normalization breakdown:

| Phase | Time (µs) | Notes |
|-------|----------|-------|
| Extract + normalize | 429 | Binary search + memory copy + subtract |
| **Total** | **429** | |

### Context-Aware Transform Infrastructure

The gather approach uses the transform pipeline infrastructure:

```cpp
// Pattern: Context-aware value projection for each trial
auto projection_factory = [](TrialContext const& ctx) -> ValueProjectionFn<EventWithId, float> {
    NormalizeTimeParams params;
    params.setContext(ctx);  // Inject trial alignment time
    return [params](EventWithId const& event) -> float {
        return normalizeTimeValue(event.time(), params);
    };
};

// Build projections per trial
for (size_t i = 0; i < gathered.size(); ++i) {
    auto ctx = gathered.buildContext(i);  // Get alignment info
    projections[i] = projection_factory(ctx);
}
```

### Why Is Gather Slower for Normalization?

Several factors contribute to the 3.5x overhead:

1. **View Creation Overhead**: Creating 1000 views takes ~240µs, amortized across the operation
2. **Lambda Captures**: Each trial's projection captures its context parameters (NormalizeTimeParams)
3. **Virtual Dispatch**: Type-erased storage adds indirection on each event access
4. **Memory Layout**: Events accessed via index indirection rather than contiguous array
5. **Cache Locality**: Jumping between indices and data arrays reduces cache efficiency

### Performance Sweet Spot

Despite the overhead, gather with projections has advantages in certain scenarios:

| Scenario | Winner | Reason |
|----------|--------|--------|
| **Single pass, raw raster** | Baseline | No abstraction overhead needed |
| **Multiple analyses on same gather** | Gather | Amortize view creation across multiple operations |
| **Trial sorting by metric** | Gather | Use `reduce()` with 4µs overhead per operation |
| **Complex filtering** | Gather | View-based filtering is composable and clean |
| **Interactive UI updates** | Gather | Cached views make incremental updates cheap |

Example: If you need to compute spike counts, first-spike latencies, AND draw raster:

```cpp
auto gathered = gather(spikes, trials);  // 1,223µs - one-time cost

// Operation 1: Count spikes
auto counts = gathered.transform([](auto v) { return v->size(); });  // 4µs
// Operation 2: Compute latencies  
auto latencies = gathered.reduce(latency_reducer);  // ~50µs
// Operation 3: Draw raster with normalization
auto projections = gathered.project(normalize_factory);  // 50µs
// ... then iterate with projections  // ~1,194µs

// Total: 1,223 + 4 + 50 + 50 + 1,194 = 2,521µs vs. 3× separate baseline runs
```

### Conclusion on Normalization

1. **Baseline normalization is simple and fast** - Direct extraction + subtraction takes 429µs

2. **Gather + projection adds abstraction overhead** - 3.5x slower in isolation, but provides composability

3. **The cost is reasonable for trial-aligned analysis** - Additional 201µs for gather+normalization vs. gather alone is ~2.5% of the gather cost

4. **Transform pipeline infrastructure is ready** - Context-aware parameters enable flexible trial-based transformations

5. **Use baseline for performance-critical tight loops** - Use gather for analysis pipelines where composability matters more than raw speed

For raster plot generation where you're only doing visualization, stick with baseline or raw vectors. For interactive analysis dashboards where you compute multiple metrics per trial, gather+transforms provide better code organization and reusability with acceptable performance cost.
